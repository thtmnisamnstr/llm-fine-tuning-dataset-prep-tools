[
    {
        "instruction": "Write a detailed guide on updating an old personal website using Gatsby with the Novela theme. Include steps from setting up prerequisites like Homebrew, XCode Command Line Tools, Git, Node.js, and creating a starter site with the Novela theme. Document the process of setting up a GitHub repository for version control and changing the default branch name from \"Master\" to \"Main\". Use a casual tone and provide Mac-specific commands where applicable. Mention that you will follow up next week with more advanced customization tips.",
        "input": "",
        "output": "My [personal website](https://thtmnisamnstr.com) sucks.\n\nI haven't touched it since 2017, and it looks dated. It doesn't have any of the writing I've been doing for work included either. It's just not a good showcase of me or my work. So I'm going to update it.\n\nWe started using [Gatsby](https://www.gatsbyjs.org) for a few of the company websites where I work, [New Relic](https://newrelic.com). So I'll use Gatsby for my site too. I'm choosing the [Novela theme](https://github.com/narative/gatsby-theme-novela) by narative, because it's popular, and I'm basic.\n\nI figured I'd document it and share it here. Hopefully somebody finds it useful.\n\n* **[GitHub Repo for this website](https://github.com/thtmnisamnstr/thtmnisamnstr-dotcom)**\n* **[My personal website](https://thtmnisamnstr.com)**\n\n*FYI, I'm using a Mac, so there may be some Mac-specific commands.*\n\n## Install prerequisites\n* Install [Homebrew](https://brew.sh/)\n  * `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"`\n* Install XCode Command Line Tools\n  * `xcode-select --install`\n* Install [Git](https://git-scm.com/)\n  * `brew install git`\n  * `git config --global user.name \"Your Name\"`\n  * `git config -- global user.email \"your@email.com\"`\n* Install [Node.js](https://nodejs.org/)\n  * `brew install node`\n\n## Install Gatsby and create your starter site\n* Install the [Gatsby CLI](https://www.gatsbyjs.org/docs/gatsby-cli/)\n  * `npm install -g gatsby-cli`\n* cd to the directory you want your site's root directory to be in (e.g. I have a development sandbox folder that I put all of my dev projects in).\n* Create a starter site (using the Novela theme by narative)\n  * `gatsby new [site-root-directory] https://github.com/narative/gatsby-starter-novela`\n\n*The last of your terminal output should be a confirmation like below:*\n\n```\nYour new Gatsby site has been successfully bootstrapped. Start developing it by running:\n  cd thtmnisamnstr-dotcom\n  gatsby develop\n```\n\n* Build and run your starter site\n  * `cd [site-root-directory]`\n  * `gatsby develop`\n\n*The last of your terminal output should be a confirmation like below:*\n\n```\nYou can now view site in the browser.\n  http://localhost:8000/\n\nView GraphiQL, an in-browser IDE, to explore your site's data and schema⠀\n  http://localhost:8000/___graphql⠀\n\nNote that the development build is not optimized.\nTo create a production build, use gatsby build\n\nsuccess Building development bundle - 14.604s\n```\n\n* Open your browser, and go to http://localhost:8000/\n\n## Create your GitHub repo\n* Login to [GitHub](https://github.com). Make an account if you don’t already have one.\n* Create a new repo\n\n* Name it. Don’t initialize with a readme, add a .gitignore, nor a license. Click Create Repository.\n\n* In Terminal...\n  * `cd [site-root-directory]`\n  * `git remote add origin https://github.com/[your-github-username]/[site-root-directory].git`\n  * `git push -u origin master`\n* In your repo on GitHub...\n  * Create New File\n\n  * Type “license\", and click the Choose License Template button\n\n  * Choose MIT License. Enter your full name. Click Review & Submit.\n\n  * Scroll to the bottom, and click Commit Changes\n\n**Note:** *You may have to merge a pull request if “Commit directly to the master branch.” wasn’t selectable in the previous screen.*\n\n* In Terminal...\n  * `git pull`\n\n**Boom!**\nYou just pulled down the MIT license file from your GitHub repo.\n\n## Rename your repo's default branch from Master to Main\n#### *...because nobody likes outdated terms, like Master, that are unnecessary references to slavery.*\n* In Terminal...\n  * `git branch -m master main`\n  * `git push -u origin main`\n\n*Your terminal output should be something like below:*\n\n```\nTotal 0 (delta 0), reused 0 (delta 0), pack-reused 0\nremote: \nremote: Create a pull request for 'main' on GitHub by visiting:\nremote:      https://github.com/thtmnisamnstr/thtmnisamnstr-dotcom/pull/new/main\nremote: \nTo https://github.com/thtmnisamnstr/thtmnisamnstr-dotcom.git\n * [new branch]      main -> main\nBranch 'main' set up to track remote branch 'main' from 'origin'.\n```\n\n* In your GitHub repo...\n  * Click Settings on your repo's navigation bar\n\n  * Click on Branches in the Settings nav\n\n  * Click the default branch dropdown (currently Master) and select Main\n\n  * Click the Update button\n\n  * A warning modal will pop-up. Click the \"I understand, update the default branch\" button\n\n**Boom!**\nNow your repo's default branch is named Main.\n\n**That's it for today.**\n*I'll post a more heavy duty tutorial next week on how to customize your new site, tailor it to you, build a resume page, and make your first post.*"
    },
    {
        "instruction": "Create a detailed announcement for the launch of New Relic's new pricing structure and perpetual free tier. The announcement should address common customer feedback about complexity and cost, highlight the benefits of the free tier including data ingest and user access levels, simplify packaging and pricing details with clear examples, and provide a step-by-step guide on how to sign up for the free tier. The tone should be straightforward and informative, emphasizing ease-of-use and value for developers.",
        "input": "",
        "output": "*[Click here to skip the exposition and jump straight to the sign up how to.](#how-to-sign-up)*\n\nNew Relic is a company built by developers for developers, but the feedback that customers have given us is that observability is complicated and New Relic is not making it easier to collect complete telemetry. *Also, we're expensive.*\n\n**Today, that changes.**\nWe're making big changes today. We are making it easier and more economical to use New Relic. [Read more here](https://blog.newrelic.com/product-news/new-relic-one-observability-made-simple/).\n* We are introducing a perpetual free tier.\n  * 100 GB/month of data ingest included.\n  * 1 full access user (Telemetry Data Platform, Full-Stack Observability, and Applied Intelligence)\n  * Unlimited free basic users (Telemetry Data Platform data exploration, dashboarding, and New Relic One application building. No Full-Stack Observability or Applied Intelligence access)\n* We simplified [our packaging and pricing](https://blog.newrelic.com/product-news/simple-pricing/).\n  * [Telemetry Data Platform](https://newrelic.com/platform/telemetry-data-platform) - consumption-based pricing, $0.25/GB of data ingest *which is a great deal*. Super cool if you believe in open source like me, because [we are supporting more and more open sources of telemetry data](https://blog.newrelic.com/product-news/introducing-telemetry-data-platform/).\n  * [Full-Stack Observability](https://newrelic.com/platform/full-stack-observability) - All of the tools developers love from New Relic (APM, Infrastructure, Browser, Mobile, etc.), all in one UI, now priced per-seat instead of per-product\n  * [Applied Intelligence](https://newrelic.com/platform/applied-intelligence) - AIOps capabilities like proactive anomaly detection (to catch problems before they occur), incident intelligence (to reduce unnecessary alerts and prioritize issues easily), and incident correlation (to reduce unnecessary alerts, understand the potential impact of an issue quickly, and route an issue to the right people automatically)\n\n[Sign-up today](https://newrelic.com/signup). It's free, it's easy, and it makes it easier to build better software.\n\n# How to sign up for New Relic's perpetual free tier\n* Go to the [New Relic sign up page](https://newrelic.com/signup).\n* Enter your Name and Email *-- That's it. No credit card. No phone number. No rights to your first-born son.*\n* Click the Sign Up button.  \n\n* New Relic will send you an email verification.\n\n* Open the verification email and click the Verify Email button.\n\n* This will take you back to New Relic to create a password. Enter your password and click the Save Password button.\n\n* Select where your data should be stored (United States or Europe) and click the Save button.\n\n**That's it.** You now have a free New Relic account w/ 100GB of data ingest/month.\n\nYou can start instrumenting your software and infrastructure by going to [New Relic One](https://one.newrelic.com), signing in if you need to, and clicking on the Add Your Data button...\n\n... or by going to [New Relic Open Source](https://opensource.newrelic.com), downloading and building one of our open source agents or integrations, and instrumenting from there.\n\nLearn more about what you can do with New Relic, including how to instrument your applications and how to use New Relic to make your software better, on our [Docs site](https://docs.newrelic.com), or dig in and learn how to include more advanced instrumentation in your applications on our [Developers site](https://developer.newrelic.com)."
    },
    {
        "instruction": "Write a blog post about deploying a Gatsby site with the Novela theme to Firebase, including step-by-step instructions. Start by mentioning that your personal website needs an update and you chose [Gatsby](https://www.gatsbyjs.org) along with the [Novela theme](https://github.com/narative/gatsby-theme-novela). Include links to your GitHub repo and live site.\n\nThe tone should be casual and informative, aimed at beginners who might not be familiar with Firebase deployment. Mention that you're using a Mac, so there may be some Mac-specific commands.\n\nCover the following points:\n1. Briefly explain what deploying means in this context.\n2. Log into Firebase and create a new project.\n3. Install Firebase CLI and initialize your site directory.\n4. Configure Firebase Hosting for your Gatsby site.\n5. Commit changes to your GitHub repo, mentioning that you're committing directly to the main branch.\n6. Build and deploy your site using `gatsby build` and `firebase deploy`.\n7. Conclude with a note about seeing your live site on Firebase.\n\nEnd by hinting at an upcoming tutorial on automating deployments with GitHub Actions and sharing some personal work-related updates, mentioning open-source projects and recent product launches you've been involved in.",
        "input": "",
        "output": "Quick recap. My personal website sucks. I'm updating it using [Gatsby](https://www.gatsbyjs.org) and the [Novela theme](https://github.com/narative/gatsby-theme-novela) by narative, because it's popular, and I'm basic.\n\n* **[GitHub Repo for this website](https://github.com/thtmnisamnstr/thtmnisamnstr-dotcom)**\n* **[My personal website](https://thtmnisamnstr.com)**\n\n*FYI, I'm using a Mac, so there may be some Mac-specific commands.*\n  \n\nAlright, part 3, deploying is the fun part. That’s when your site actually ends up on the internet. Like... people can visit it and stuff. It’s not just a project on your computer anymore. So let’s get into it..\n\n## Initialize Firebase on your site and deploy to Firebase\n* Login to [Firebase](https://firebase.google.com) and create a project for you site. Make an account if you don’t already have one.\n  * Click Go to Console\n\n  * Click the Add Project tile\n\n  * Give your project a name and click Continue\n\n  * Enable Google Analytics for your site (or not, your choice) and click Continue\n\n  * Select your Google Analytics location, configure your Google Analytics settings, accept both terms checkboxes, and click Create Project\n\n* Install the Firebase CLI and initialize your site's directory\n  * In terminal, `npm install -g firebase-tools`\n  * `firebase logout` *(you may not have to do this step, but I did)*\n  * `firebase login`\n\n  * Type 'y' to let Firebase collect usage and error reporting info. Type 'n' not to *(I go with 'n')* and press Enter\n  * This will open up a Google sign-in page\n\n  * Select your Google account that you use for Firebase\n  * You will be asked to authorize Firebase CLI to access your Google account. Click the Allow button.\n\n  * You should get a login successful screen *(you can close this window)*\n\n  * In Terminal, the output should be something like `✔  Success! Logged in as [your.email]@gmail.com`\n  * `cd` to your site's root directory\n  * `firebase init`\n\n  * Press the down arrow until the \"Hosting: ...\" option is highlighted, press the space bar to select it, and press Enter\n\n  * Use the up and down arrows to highlight \"Use an existing project\" and press Enter\n\n  * Select the Firebase project you created previously and press Enter\n  * For public directory, type 'public' and press Enter\n  * For whether to configure as a single-page app, type 'n' and press Enter\n\n  * If asked to overwrite anything, type 'n' and press Enter\n  * Your terminal output should be:\n\n```\ni  Skipping write of public/index.html\n\ni  Writing configuration info to firebase.json...\ni  Writing project information to .firebaserc...\n\n✔  Firebase initialization complete!\n```\n\n* Commit your changes to your repo\n  * `git add --all`\n  * `git commit -a -m \"Initialized Firebase\"`\n  * `git push -u origin main`\n\n*That's right we committed right to main. Didn't fork and do a PR. You don't have to. I usually do, but didn't this time. It's a good habit to fork and PR though.*\n\n* Deploy your site to Firebase (when your site is ready to deploy)\n  * `gatsby clean`\n  * `gatsby build`\n  * `firebase deploy --only hosting`\n  * Your terminal output should be like the following...\n\n```\n=== Deploying to 'thtmnisamnstr'...\n\ni  deploying hosting\ni  hosting[thtmnisamnstr]: beginning deploy...\ni  hosting[thtmnisamnstr]: found 135 files in public\n✔  hosting[thtmnisamnstr]: file upload complete\ni  hosting[thtmnisamnstr]: finalizing version...\n✔  hosting[thtmnisamnstr]: version finalized\ni  hosting[thtmnisamnstr]: releasing new version...\n✔  hosting[thtmnisamnstr]: release complete\n\n✔  Deploy complete!\n\nProject Console: https://console.firebase.google.com/project/thtmnisamnstr/overview\nHosting URL: https://thtmnisamnstr.web.app\n```\n\n**Boom!** You're site is deployed to Firebase. You should be able to go to the Hosting URL in your most recent terminal output to see your site. I already had my Firebase configured for [thatmanisamnstr.com](https://thtmnisamnstr.com). So when I go there, it's up.\n\n**That's it for today.**\n*I'll post a tutorial next week on how to automate deployments with GitHub Actions. That was supposed to be part of this post, but I’ve been crazy busy at work (I ran go-to-market for [open sourcing our instrumentation](https://blog.newrelic.com/product-news/introducing-open-source-agents-and-projects/), and I ended up doing a bunch of work [for our recent launch](https://blog.newrelic.com/product-news/new-relic-one-observability-made-simple/). [Our new free tier](https://dev.to/newrelic/how-to-sign-up-for-new-relic-s-new-perpetual-free-tier-1ebn) is dope btw.).*"
    },
    {
        "instruction": "Please provide an overview of how New Relic's Telemetry Data Platform can integrate with Grafana to enhance metric visualization and management without disrupting existing Prometheus-based dashboards. Include steps for setting up the integration, benefits of using this setup, and any additional features available for Grafana Enterprise users.",
        "input": "",
        "output": "Grafana is great -- and it makes data look really good.\n\nWe’ve talked with plenty of you who use Grafana to visualize and alert on disparate data sets, and the general consensus is you love it. It’s easy to get up and running; it connects to a wide range of telemetry data stores; and it has plenty of [official and community-built dashboards](https://grafana.com/grafana/dashboards) to get you started. Grafana is also open source, which is a bonus for us all.\n\nYou’ve spent years building dashboards in Grafana. You’ve come to rely on them, and you have no desire to lose them.\n\nAnd we don’t want you to.\n\nTo that end, we’ve teamed up with [Grafana Labs](https://grafana.com) so you can use our [Telemetry Data Platform](https://newrelic.com/platform/telemetry-data-platform) as a data source for Prometheus metrics and surface them in your existing dashboards. Now you can keep Prometheus and Grafana and seamlessly tap into the additional reliability, scale, and security provided by New Relic.\n\n## A platform link from Prometheus to Grafana\nPrometheus’ built-in remote write capability forwards metrics from your existing Prometheus servers to the Telemetry Data Platform, [a fully managed, elastic, time-series platform](https://blog.newrelic.com/product-news/introducing-telemetry-data-platform/). The Telemetry Data Platform gives you fully connected, global views of the relationships between all your data sources—from Prometheus metrics to application logs and distributed traces.\n\nSetting up the Prometheus remote write integration and configuring the Telemetry Data Platform as a Prometheus data source in Grafana won’t interrupt your current experience. Your existing Grafana dashboards won’t change—they’ll just pull Prometheus data from New Relic. You can even build new ones with PromQL based data stored in New Relic.\n\n## How to configure the Telemetry Data Platform as a Prometheus data store in Grafana\nIn this two-step example, we’ll show you how to replace your Prometheus data store with the Telemetry Data Platform and add it as a Prometheus data source in Grafana. In the end, you’ll be able to send Prometheus data to New Relic, and Grafana will populate your existing Prometheus-specific dashboards with that data.\n\n**Note:** This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. You’ll also need to [sign up for New Relic](https://newrelic.com/signup/).\n\n### Step 1: Get data flowing into New Relic with the Prometheus remote write integration\nNavigate to [Instrument Everything – US](http://one.newrelic.com/launcher/nr1-core.settings?pane=eyJuZXJkbGV0SWQiOiJ0dWNzb24ucGxnLWluc3RydW1lbnQtZXZlcnl0aGluZyJ9)  or [Instrument Everything – EU](http://one.eu.newrelic.com/launcher/nr1-core.settings?pane=eyJuZXJkbGV0SWQiOiJ0dWNzb24ucGxnLWluc3RydW1lbnQtZXZlcnl0aGluZyJ9), click the **Prometheus** tile, and complete the following steps:\n1. Enter a name for the Prometheus server you’re connecting to and generate your `remote write` URL.\n\n2. Copy and add the generated `remote write` snippet (with URL) to your Prometheus configuration file (`prometheus.yml`). A good place to add this information is under `global_config`, at the same indentation level as the `global` section.\n    ```\n    # my global config\n    global:\n    scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. \n    evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n    # scrape_timeout is set to the global default (10s).\n\n    remote_write:\n    - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=[prometheus-demo-1 key]&prometheus_server=prometheus-demo-1\n    ```\n    If you don’t have a configuration file, check out the [Prometheus configuration docs](https://prometheus.io/docs/introduction/first_steps/#configuring-prometheus), or refer to [this example configuration file](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus.yml).\n3. Restart your Prometheus server.\n4. Click **See your data**.You should get a `“Data received successfully...”` message, but it may take a few minutes to see your data in New Relic.\n\nBoom! Now your Prometheus data is flowing into the Telemetry Data Platform.\n\n*For more information on how to set up the Prometheus remote write integration, check out our [docs](https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/set-your-prometheus-remote-write-integration).*\n\n### Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic\nWhen you switch to the Telemetry Data Platform as your Prometheus data store in Grafana, you can keep your Prometheus dashboards as they are—no need to lose or rebuild them. You just need to add the Telemetry Data Platform as a Prometheus data source.\n1. Generate a New Relic Query key. From the New Relic One home screen, click the **More > Insights**, and in the left-hand nav of Insights, click **Manage data > API Keys**.\n2. Click the **+** icon next to **Query Keys**, and in the **Notes** field, enter a short description of what you’re using the key for, click **Save your notes**.\n\n3. Click **Show** and copy your new API key, as you’ll need it later.\n4. From the Grafana home page, click **Configuration > Data Sources**. You should see your existing Prometheus data source. Click it to edit.\n5. Update the **URL** field of the data source based on your New Relic data region:\n    * US: [https://prometheus-api.newrelic.com](https://prometheus-api.newrelic.com/)\n    * EU: [https://prometheus-api.eu.newrelic.com](https://prometheus-api.eu.newrelic.com/)\n6. Set the **Access** field to **Server**.\n7. Under **Custom Headers**, select **Add Header**. Set the Header value to **X-Query-Key** and add the API key you copied above.\n8. Click **Save & Test**.\n\nYou should get a `“Data source is working”` message, and if you click **Back**, the URL in your Prometheus data source should reflect that it’s pointed at the Telemetry Data Platform.\n9. Navigate to your Prometheus dashboard. You should see Prometheus data from New Relic populating your charts.\n\nBoom! Now your Prometheus data is being stored in New Relic and being used in Grafana for visualization. Easy. Not a single dashboard to rebuild.\n\n*For more information on how to configure New Relic as a Prometheus data source for Grafana, check out our [docs](https://docs.newrelic.com/docs/integrations/grafana-integrations/set-configure/configure-new-relic-prometheus-data-source-grafana).*\n\n## Grafana Enterprise users: query all Telemetry Data Platform data from Grafana!\nWhy stop at Prometheus data? With the Telemetry Data Platform, you can ingest metrics, events, logs, and traces from any source, including Telegraf, OpenTelemetry, Logstash, and Kubernetes. Your Grafana dashboards can reflect it all.\n\nIf you use [Grafana Enterprise](https://grafana.com/products/enterprise/), be sure to install the [New Relic data source plugin](https://grafana.com/grafana/plugins/grafana-newrelic-datasource). With this plugin, you can use the [New Relic query language (NRQL)](https://docs.newrelic.com/docs/query-data/nrql-new-relic-query-language/getting-started/nrql-syntax-clauses-functions) to query metrics, logs, and traces from other data sources you’ve stored in the Telemetry Data Platform. Use those NRQL results to build telemetry dashboards in Grafana Enterprise.\n\nCheck out the [Grafana Labs blog post about the plugin](https://grafana.com/blog/2020/07/22/introducing-the-new-and-improved-new-relic-plugin-for-grafana/) for more information.\n\n## Grafana Labs and New Relic—better together\nGiven the rate of innovation and diversity of technology, making real, impactful observability tooling will require collaboration in areas where competition has traditionally existed. We’re excited about this collaboration with Grafana Labs, and we’re excited to [bring observability to everyone](https://blog.newrelic.com/product-news/new-relic-one-observability-made-simple/). When you’re ready to get started, [sign up for a Grafana Enterprise trial](http://newrelic.com/grafana-enterprise-trial), and get those dashboards populated."
    },
    {
        "instruction": "Create a detailed guide for automating the build and deployment of a Gatsby website using GitHub Actions. The website is built with the Novela theme by narative, and it's hosted on Firebase. Include steps for setting up the necessary environment in `.gitignore`, configuring GitHub Actions to clean, build, and deploy the site, and how to test if the automation works. Mention any issues encountered with building on Ubuntu and how they were addressed. Assume a basic understanding of Gatsby, Firebase, and GitHub.",
        "input": "",
        "output": "Quick recap. My personal website sucks. I'm updating it using [Gatsby](https://www.gatsbyjs.org) and the [Novela theme](https://github.com/narative/gatsby-theme-novela) by narative, because it's popular, and I'm basic.\n\n* **[GitHub Repo for this website](https://github.com/thtmnisamnstr/thtmnisamnstr-dotcom)**\n* **[My personal website](https://thtmnisamnstr.com)**\n\n*FYI, I'm using a Mac, so there may be some Mac-specific commands.*\n  \n\nAlright, part 4, automating deploys with GitHub Actions. GitHub Actions seems to be all the rage right now. I've used a couple of different enterprise-grade CI/CD tools, primarily Jenkins. So I'm familiar with the benefits and may have implemented it once or twice years ago. I haven't used GitHub Actions though. So this was a learning process. There was a lot of experimentation and testing (and failing) while doing this. At the end though, this is really good, easy to implement, CI that is free for most GitHub non-enterprise users. GitHub Actions are awesome.\n  \n\n**Post-Publish Update:**  I ended up commenting out the Gatsby ‘clean’ and ‘build’ steps of my action. The Gatsby CLI action that I used worked fine, but it looks like Ubuntu is the only free choice for the containers the actions run in. While Ubuntu is great, some of the components of my Gatsby site look to be incompatible on Ubuntu and the site builds incorrectly vs. Mac. So I am building on my Mac, PR-ing my repo, and automating the deploy only with GitHub Actions.\n  \n\n## Automate deployments with GitHub Actions\n### Add your Firebase token to your GitHub repo\n* In Terminal, `firebase login:ci`\n* This will open up a Google sign-in page\n\n* Select your Google account that you use for Firebase\n* You will be asked to authorize Firebase CLI to access your Google account. Click the Allow button.\n\n* You should get a login successful screen *(you can close this window)*\n\n* In Terminal, the output should be something like...\n\n```\n✔  Success! Use this token to login on a CI server:\n\n[onebigassstringthatisyourtoken]\n\nExample: firebase deploy --token \"$FIREBASE_TOKEN\"\n```\n\n* Copy your token and/or don't close your terminal so you can copy it later\n* Login to GitHub and go to your site's repo and click on the Settings navigation tab\n\n* Click Secrets in the left navigation menu\n\n* Click the New Secret button\n\n* For Name, enter 'FIREBASE_TOKEN'\n* For Value, enter the Firebase token you copied earlier from Terminal\n* Click the Add Secret button\n\nYou should see the new secret in your GitHub repo. Here's mine.\n\n### Add GitHub Actions to automate deployment\n* In your site's repo in GitHub, click on Actions navigation tab\n\n* Click the Setup this Workflow button under the Simple Workflow section\n\n* Change your Actions YAML name and click the Start Commit button *(I named mine `build-deploy.yml`)*\n\n* Add a commit subject and message and click the Commit New File button\n* In Terminal, cd to site's root directory\n* `git checkout main`\n* `git pull` *(your `build-deploy.yml` file should pull down to your local machine)*\n* Open './.github/workflows/build-deploy.yml'\n* Go the the [GitHub Action for Gatsby CLA](https://github.com/marketplace/actions/github-action-for-gatsby-cli) and click the Use Latest Version button\n\n* Copy the text and paste it at the top of your `build-deploy.yml` file\n\n* Go the the [GitHub Action for Firebase](https://github.com/marketplace/actions/github-action-for-firebase) and click the Use Latest Version button\n\n* Copy the text and paste it at the top of your `build-deploy.yml` file below the Gatsby action you pasted earlier\n\n* In your `build-deploy.yml` file, add your build and deploy and save. Here's mine.\n\n```\n# This is a basic workflow to help you get started with Actions\n\nname: Build and Deploy\n# Controls when the action will run. Triggers the workflow on push or pull request\n# events but only for the main branch\non:\n  # Gavin, 20200802: Removing on-push, because I only want PRs to trigger this job\n  push:\n    branches: [ main ]\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a two jobs, \"build\" and \"deploy\"\n  build:\n    name: Build and Deploy\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout Repo\n      uses: actions/checkout@master\n    - name: Clean Gatsby Site\n      uses: jzweifel/gatsby-cli-github-action@master\n      with:\n        gatsby-arg: clean\n    - name: Build Gatsby Site\n      uses: jzweifel/gatsby-cli-github-action@master\n      with:\n        gatsby-arg: build\n    # Gavin, 202008014: Moved this up from its own Deploy job into this job ()\n    - name: Deploy to Firebase\n      uses: w9jds/firebase-action@master\n      with:\n        args: deploy --only hosting\n      env:\n        FIREBASE_TOKEN: ${{ secrets.FIREBASE_TOKEN }}\n```\n\n## Loosen your .gitignore restrictions and push your changes\n* Open './.gitignore'\n* In your `.gitignore` file, unignore `node_modules` and `public` (for building and deploying) and save. Here's mine.\n\n```\n# Logs\nlogs\n*.log\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n# Runtime data\npids\n*.pid\n*.seed\n*.pid.lock\n\n# Directory for instrumented libs generated by jscoverage/JSCover\nlib-cov\n\n# Coverage directory used by tools like istanbul\ncoverage\n\n# nyc test coverage\n.nyc_output\n\n# Grunt intermediate storage (http://gruntjs.com/creating-plugins#storing-task-files)\n.grunt\n\n# Bower dependency directory (https://bower.io/)\nbower_components\n\n# node-waf configuration\n.lock-wscript\n\n# Compiled binary addons (http://nodejs.org/api/addons.html)\nbuild/Release\n\n# Dependency directories\njspm_packages/\n# Gavin, 20200719: Including the Novela theme directory under node_modules\n# Gavin, 20200802: Including all under directory under node_modules\n#node_modules/*\n#!node_modules/@narative/\n#node_modules/@narative/*\n#!node_modules/@narative/gatsby-theme-novela/\n\n# Typescript v1 declaration files\ntypings/\n\n# Optional npm cache directory\n.npm\n\n# Optional eslint cache\n.eslintcache\n\n# Optional REPL history\n.node_repl_history\n\n# Output of 'npm pack'\n*.tgz\n\n# dotenv environment variables file\n.env\n\n# gatsby files\n.cache/\n\n# Gavin, 20200802: Including public\n#public\n\n# Mac files\n.DS_Store\n\n# Yarn\nyarn-error.log\n.pnp/\n.pnp.js\n# Yarn Integrity file\n.yarn-integrity\n\n.netlify/\n```\n\n* In Terminal, `gatsby clean`\n* `git add --all`\n* `git commit -a -m \"Added GitHub actions to automate build and deploy. Also added the 'node_modules' folder and the 'public' folder\"`\n* `git push -u origin main`\n\n*So... Great. How can you tell if this works?*\nWhen you pushed you your main repo to origin (GitHub), the action should have automatically run. Your Action will run when you push to your main branch, merge a PR into your main branch, or manually re-run your Action. I'll describe how to re-run an Action manually because everything for it will apply to the other automated runs.\n* In your GitHub repo, click on Actions.\n\n* You can see your Actions on the left and your Action running on the right, if it's running *(You can also see my failures on the right in the screenshot)*. Click on the Action you just created.\n\n* Click on the most recent run of your Action. *It might be a failure. It doesn't matter, just click it.*\n\n* Click Re-run jobs > Re-run all jobs.\n\n* Your Action will queue, then run. Its status will change to In Progress and the spinners will start spinning.\n\n* If you click on the Action in the left nav panel, you can see it run (or troubleshoot failures).\n\n**Boom!** Now your Gatsby site is automatically building and deploying when you merge a pull request to your `main` branch. Professional level CI on a personal site, easy, and free. Is it overkill? Maybe. Am I going to continue overkilling my personal site and doing write-ups like this? Probably."
    },
    {
        "instruction": "Create a detailed guide on transforming a cheap Atom 2-in-1 laptop into a Linux-powered device for under $50, focusing on technical steps and tips. Include sections on hardware modifications, software installation, performance improvements, and troubleshooting common issues. Use a conversational tone to make the instructions accessible and engaging for readers who are following along step-by-step. Mention any necessary tools or commands used throughout the process. Ensure the content is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in this project.",
        "input": "",
        "output": "It all started with a goal. When I wake up late at night and can't go back to sleep (which happens way too often tbh), I wanted something I could write on. Something compact and light but with a keyboard. Basically I wanted a barebones, ultra-compact laptop that was cheap. You may have at some point in the distant past seen something that fits this description called a [netbook](https://en.wikipedia.org/wiki/Netbook). Well, those don't really exist anymore. So I ventured to make something like that.\n\n## Finding the laptop\n### Requirements\n* It had to be cheap. The maximum I'd be willing to spend was $200. $100 or less would be even better.\n* It had to be small. I didn't want anything with a screen over 11\", nothing under 8\". The more compact the better, but under 8\" would be difficult to use IMO.\n* It had to have an [x86](https://en.wikipedia.org/wiki/X86) processor. I wanted to run a non-mobile OS, probably Linux, because I really don't like any version of Windows that I've used since Windows 7 (really since XP).\n* I had to reasonably believe I could get it up and running with my browser of choice, [Chromium](https://www.chromium.org/Home), and a lightweight text editor with integrated terminal, preferably [VS Code](https://code.visualstudio.com).\n\nThe search for a laptop that fit these requirements was kinda long. I looked on all the major laptop vendors' sites (e.g. HP, Dell, Acer, Asus, Lenovo, etc.). Then I scoured the big box stores' sites (e.g. Best Buy, Target, Costco, Walmart, etc.). Then I searched Amazon, but decided that anything I bought for cheap off Amazon would be a bad idea. Then I searched refurbished laptops across all the manufacturers and stores. I considered several and almost pulled the trigger on one that was close to $200 until I came across a refurbished, economy 2-in-1 tablet/laptop. There were a whole lot of sacrifices, but it satisfied my requirements. It wasn't much, but it was cheap, tiny, and had potential.\n\n### The laptop\n**[Ematic EWT826BK](https://ematic.us/product/ematic-8-hd-windows-10-tablet-32gb-with-wifi-intel-atom-z3735g-processor-docking-keyboard-ewt826bk/)**\n\\[[refurbished from Walmart.com](https://www.walmart.com/ip/Ematic-EWT826BK-8-32GB-Windows-Quad-Core-Tablet-Black-Refurbished/454553155)\\]\n* **Price:** $57.66 ($49.00 purchase price + $4.66 sales tax + $4.00 merchandise fee) -- also fuck whatever a merchandise fee is. I almost didn't buy this laptop because of this stupid $4 fee.\n* **CPU:** 1.3 GHz Intel Atom Quad-core\n* **RAM:** 1GB DDR2\n* **Storage:** 32GB SSD\n* **OS:** Windows 8.1\n* **Other Specs:** 8\" touchscreen @ 1280x800, 802.11b/g/n wifi, Bluetooth v4.0, 1xMicro USB 2.0 port, 1xmicroSD card slot, back and front 2MP webcams, 1x3.5mm headphone jack, keyboard case included.\n\n*I didn't have a banana, so I used a quarter for scale instead.*\n\nI bought this 2-in-1 refurbished from walmart.com. It was dirt cheap, super small due to the keyboard case and 2-in-1 form factor, and ran an x86 Intel Atom processor. Another big plus was that it came with  Windows 8.1 installed. I never had any intention of keeping or even using Windows, but having it installed meant that a full-featured, non-mobile OS install was possible.\n\nThe reviews on it weren't great. The positive reviews were pretty much all around the size of the device. The much more numerous 1-star reviews were primarily around the device being slow and first-run Windows updates taking up 80% of the disk space and leaving the device near useless due to slow application load times.\n\nI didn't let the bad reviews deter me. I'm pretty well-versed in OS's, and was pretty confident a lot of the speed and performance issues were caused by Windows. If there's one thing I know about Windows it's that it will absolutely update itself into a state of inoperability on hardware that barely clears the minimum specs. That's what this was. I could fix it with Linux.\n\n## Choosing the right Linux distro\n### [Lubuntu](https://lubuntu.me)\nChoosing the right distro was extremely important. This is a low-end device. I needed lightweight. My first inclination was Lubuntu, the lightest weight, official [Ubuntu](https://ubuntu.com)-based distro. It being Ubuntu-based is a big benefit. It meant I wouldn't have to deal with a lot of issues installing Linux comes with (e.g. abandoned distros, lack of hardware support, difficult install process, etc.). So I decided to give it a try.\n\nI got Lubuntu installed and running without too much trouble. Restarting and launching the live USB via the 32-bit UEFI that came with this 64-bit machine, was a bit of a trick (I'll cover the install process in the next section) The touchscreen didn't work though. This device came with a keyboard case but didn't have a mouse. I figured I'd need the touchscreen (I was wrong). So I tried out a few different distros in hopes of finding one that worked decently on this device and supported its touchscreen.\n\n### [KDE Neon](https://neon.kde.org)\nIn my research, I'd read that the KDE desktop environment was much less resource intensive than it traditionally has been. I also read that KDE Neon was a better experience than Kubuntu and was similarly based on Ubuntu.\n\nI got the live USB to boot, but the install failed at the requirements check and didn't allow the install to start (which is weird, because most Linux installs I've run usually let you continue).\n\n### [Kubuntu](https://kubuntu.org)\nSince KDE Neon didn't work, I installed Kubuntu. It is the official Ubuntu distro that includes the KDE desktop environment. So very similar to KDE Neon.\n\nKubuntu installed fairly easily. Similar to Lubuntu. After install, the touchscreen still didn't work and it was sluggish. It was sluggish to the point that launching Firefox and browsing would frequently lock the device up. Successful install, beautiful UI, but still a downgrade from Lubuntu for this device.\n\n### [Ubuntu Touch](https://ubuntu-touch.io)\nSince Ubuntu Touch is the mobile and tablet version of Ubuntu (not official but kinda official) I thought it might work. Nope. The device wasn't supported.\n\n### [Peppermint OS](https://peppermintos.com)\nPeppermint OS is a distro based off of [Linux Mint](https://linuxmint.com). It is meant to be web-centric and lightweight, kinda like Chrome OS but more legit Linux,\n\nIt was tricky getting it to boot to the live USB, but managed it with the process I'll detail later in this post. The install was slow but had no issues until after completion, when it presented an error dialog about [GRUB](https://www.gnu.org/software/grub/) not installing correctly. After that error, it only gave me options to reboot. So I did, and it almost bricked the device. GRUB didn't install fully. So I was presented with the GRUB rescue prompt, but without any of the standard tools that come with it. So I couldn't really do anything from here. I spent hours trying different things, and nothing worked from this prompt. Recovering from this was a major effort.\n\n### [Fedora Workstation](https://getfedora.org/en/workstation/)\nAfter I figured out how to get out of the GRUB rescue prompt and to the UEFI setup screen, I gave Fedora a shot. Fedora uses [GNOME 3](https://www.gnome.org/gnome-3/) as its desktop environment. GNOME is not lightweight, but it is supposed to support a lot of touchscreens. So I wanted to try it out.\n\nInstallation was easy. The live USB supported 32-bit UEFI out of the box, so that made live simple. Install went without a hitch. After install, the touchscreen still didn't work, and it was ultra slow.\n\nThis is when I gave up on the touchscreen. For what I wanted this for, I didn't really need a touchscreen anyway. I did some research and learned about [i3](https://i3wm.org), a tiling window manager that was lightweight, keyboard-centric, and pretty popular. So I decided a lightweight distro with i3 was the way to go.\n\n### [ArchLabs Linux](https://archlabslinux.com)\nArchLabs Linux is a distro based on [Arch Linux](https://www.archlinux.org). I'm really curious about Arch. It's lightweight, bare bones, very manual, but very configurable. ArchLabs came with i3 by default. So I thought it might be a good fit.\n\nI couldn't get the live USB to boot. I tried several different approaches, but no luck.\n\n### Which distro I finally chose (Lubuntu + i3)\nIn the end, I went back to Lubuntu. It was easy to install and performed the best of all of the distros I tried. No touchscreen support, but I decided installing i3 and setting it to the default window manager would solve that issue.\n\nAfter I installed Lubuntu (again), I...\n* Used the default package manager, Muon, to remove every program that I saw no use for.\n* Fully updated my system via Muon.\n* Installed the [Snap Store](https://snapcraft.io/store), because I wanted Chromium and VS Code, and Snap makes installing both easy. Snap comes pre-installed on all official Ubuntu distros. Installing the store just gave me a pretty, easily searchable UI for it.\n* Installed Chromium and VS Code via the Snap Store.\n* Installed i3\n* Set my default window manager to i3 in the Session Settings section of `lxqt-config`.\n* Restarted.\n\nAnd it worked. It worked well. Chromium ran well. I installed the [Vimium](https://vimium.github.io) extension in Chromium to make navigating the web via keyboard easier. VS Code ran fine. I cloned my personal website repo from GitHub, and it downloaded fast. Now it's exactly what I was looking for. An ultra-compact, ultra-cheap laptop that ran a non-mobile OS that I could use Chromium and VS Code on to write and occasionally do some light coding with.\n\n## Why all of this was So difficult (32-bit UEFI on a 64-bit machine)\n\n*Pretty much all of the info below was gleaned from a post by Intel titled* ***[Why Cheap Systems Run 32-bit UEFI on x64 Systems](https://software.intel.com/content/www/us/en/develop/blogs/why-cheap-systems-run-32-bit-uefi-on-x64-systems.html).***\n\nThe big obstacle to installing Linux in this device and many other small, inexpensive 2-in-1's that run an Atom processor is that they run 32-bit UEFI for firmware with a 64-bit processor. The reason these devices are built like this is simple, cost vs. performance.\n\nDevices with less than 4GB of RAM don't benefit from having a 64-bit processor, and almost none of these 2-in-1 devices have that much RAM. Windows 8.1 (the OS pre-installed on my device) began offering a cheaper, 32-bit version targeted at devices like this even, which was a benefit because it performed the same but took up less disk space and was less expensive than the 64-bit version. A device's firmware and it's OS architecture need to be the same. So inexpensive 32-bit Windows requires a 32-bit UEFI. The selection of 32-bit Linux distros is limited in comparison with 64-bit distros. The Atom processor is x86 though, meaning it can operate in 16-, 32-, and 64-bit modes. So you can run a 64-bit Linux distro on the device, but installing it via the 32-bit UEFI is the challenge as most distros don't support this mixed architecture.\n\n### How I installed 64-bit Linux with a 32-bit UEFI\n\n* Download the Linux distro ISO you want to install.\n* Make an editable, bootable live USB from the ISO. Partition scheme should be GPT. File system should be FAT32.\n  * For Windows, use [Rufus](https://rufus.ie).\n  * For Mac, use [UNetbootin](https://unetbootin.github.io). A lot of how-to's recommend [balenaEtcher](https://www.balena.io/etcher/), but it never left me with an  editable, bootable USB.\n* Download a [bootia32.efi](https://github.com/jfwells/linux-asus-t100ta/blob/master/boot/bootia32.efi) file. This file will let your 64-bit OS live USB boot on a 32-bit UEFI. I used the jfwells one, because it is the most referenced in forums.\n* Copy bootia32.efi to the `/EFI/BOOT/` directory on your live USB.\n* Restart in UEFI settings (which is where you can launch your live USB from).\n  * If Windows is still installed, hold the Shift key and restart using the Windows UI. This boots into Advanced Startup Options. From here select Troubleshoot > Advanced Options > UEFI Firmware Settings.\n  * If Linux is installed, open a terminal, enter ‘systemctl reboot --firmware-setup’.\n  * If no OS is installed, plug in an external USB keyboard via USB-to-Micro USB adapter, hold the Esc key, and restart (a hard restart is fine). The included keyboard doesn’t have an Esc key, so an external is required. After landing on the UEFI boot menu, remove the keyboard from the adapter and insert the live USB.\n\nYou can boot from your USB via the UEFI boot menu. It's pretty straightforward after that. Most distros, at least the primarily Ubuntu-based ones I tried, had no issue starting their live USB from here. ArchLabs wouldn't, and Fedora had booting to 32-bit UEFI out-of-the-box, but all of the others I tried booted via this method.\n\n## How's it work?\n\nIt works fine. It's not fast, but it's not super slow either. It hasn't crashed since I started it. I haven't had to hard reboot it a single time. I wrote about 1/3 this blog on it, another 1/3 on my iPhone, and the last 1/3 between my personal MacBook and my work MacBook.\n \n\n*Here's a screenshot of me writing this post in Chromium.*\n\n*Here's my website in VSCode.*"
    },
    {
        "instruction": "Write a detailed guide on getting started with Kafka Connect for New Relic, focusing on technical steps and tips. Include sections on setting up Kafka Connect, configuring connectors for New Relic, and monitoring data flow. Use a conversational tone to make the instructions accessible and engaging for readers who are following along step-by-step. Mention any necessary plugins or configurations, such as those related to Kafka Connect and New Relic. Ensure the content is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in this setup. Include terminal commands and configuration examples where applicable.",
        "input": "",
        "output": "It’s 3:00 am and PagerDuty keeps firing you alerts about your application being down. You need to figure out what the issue is, if it’s impacting users, and resolve it quickly so your users aren’t interrupted and you can get back to sleep. A lot of people rely on [New Relic](https://newrelic.com) to help with this.\n\nTraditionally, the telemetry data that is stored in New Relic’s [Telemetry Data Platform](https://newrelic.com/platform/telemetry-data-platform) has come from New Relic’s own instrumentation. More and more, that isn’t the case, with open source tools and alternative instrumentation sending data to the Telemetry Data Platform. Now, it’s easier than ever to build these observability pipelines with the New Relic connector for Kafka Connect, available both on the [Confluent Hub](https://www.confluent.io/hub/newrelic/newrelic-kafka-connector) and open source on [GitHub](https://github.com/newrelic/kafka-connect-newrelic).\n\nThe below tutorial details how to set up the connector for both events and metrics and provides simplified examples of sending data in both data types to New Relic through the connector.\nNew Relic without Kafka Connect\n\n## New Relic Connector for Kafka Connect\nThe New Relic connector for Kafka Connect allows you to ingest data from Apache Kafka® topics into the New Relic platform without writing a single line of code. Because we’re working with Kafka Connect, the connector is purely driven by configuration, which you apply using the Kafka Connect REST API. Things like retries, error handling, and dead letter queues are handled for you by Kafka Connect and can be customized as required. Achieving this with a Kafka consumer would require many hours of development time and testing.\n\nCurrently, the connector supports the [Metric](https://docs.newrelic.com/docs/telemetry-data-platform/get-data/apis/introduction-metric-api) and [Event](https://docs.newrelic.com/docs/telemetry-data-platform/ingest-manage-data/ingest-apis/use-event-api-report-custom-events) telemetry data types. We are working on releasing code for the remaining data types (Log and Trace) soon.\n\nTo get started, head over to [Confluent Hub](https://www.confluent.io/hub/newrelic/newrelic-kafka-connector) and follow the instructions to install the connector into your local Kafka cluster. After that, you can use the [Kafka Connect REST API](https://docs.confluent.io/current/connect/references/restapi.html?&_ga=2.148221751.1248744156.1603040520-1928848811.1603040520#post--connectors) to launch a connector. This will require an [API key from New Relic](https://docs.newrelic.com/docs/telemetry-data-platform/ingest-manage-data/ingest-apis/use-event-api-report-custom-events#) to start sending data.\n\nThe connector takes care of all the error handling for you—if messages do not follow the recommended format, you can pipe them to a dead letter queue (DLQ) and also decide whether you want to abort processing if a bad message is read, or continue regardless. This [blog post](https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/) explains these options in more detail.\n\nRuntime errors like connectivity issues, the wrong API key, or too large of payload are handled in different ways:\n* The wrong API key will shut down the connector task immediately.\n* If there is a network error, the connector will retry up to a user-configured limit.\n* If the payload is too large, the connector splits and retries until it sends everything over. From a user perspective, you don’t need to worry about this.\n\nLet’s look now in more detail at how to set up an Event or Metric connector.\n\n## Events\nThe [Event API](https://docs.newrelic.com/docs/telemetry-data-platform/ingest-manage-data/ingest-apis/use-event-api-report-custom-events) is supported by New Relic. To create an Event connector, you can use the sample JSON to configure the connector in Kafka Connect:\n\n```\n{\n \"name\": \"events-connector\",\n \"config\": {\n \"connector.class\": \"com.newrelic.telemetry.events.TelemetryEventsSinkConnector\",\n \"value.converter\": \"com.newrelic.telemetry.events.EventsConverter\",\n \"topics\": \"nrevents\",\n \"api.key\": \"\"\n }\n}\n```\n\nThe above JSON will create a connector with the name `events-connector`. It will start sinking messages from a topic called `nrevents`. A sample Event message looks something like this (note that the timestamp field in the payload needs to be the Unix epoch in milliseconds):\n\n```\n[\n  {\n    \"eventType\":\"Purchase\",\n    \"account\":3,\n    \"Amount\":259.54,\n    \"timestamp\":\n  },\n  {\n    \"eventType\":\"Purchase\",\n    \"account\":5,\n    \"amount\":12309,\n    \"product\":\"Item\",\n    \"timestamp\":\t\n  }\n]\n```\n\nThe above message creates two events in New Relic of the type `Purchase`. **Make sure you use the current timestamp.** You can list them in New Relic One’s [Query Builder](https://docs.newrelic.com/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder) by running a query like `SELECT * from Purchase`.\n\nYour messages in the topic should adhere to this format. For an event, `eventType` and `timestamp` are required fields.The `EventsConverter` is responsible for validating these messages and will flag them as errors if they do not comply with the required format.\n\n## Metrics\nThe details of the Metric API can be found in the [documentation](https://docs.newrelic.com/docs/telemetry-data-platform/get-data/apis/introduction-metric-api). Metrics are broadly classified into the three following categories:\n1. **Gauge:** Examples include temperature, CPU usage, and memory.\n2. **Count:** Examples include cache hits per reporting interval and the number of threads created per reporting interval.\n3. **Summary:** Examples include transaction count/durations and queue count/ durations.\n\nThis is the sample JSON for creating a Metric connector:\n\n```\n{\n \"name\": \"metrics-connector\",\n \"config\": {\n \"connector.class\": \"com.newrelic.telemetry.events.TelemetryMetricsSinkConnector\",\n \"value.converter\":\"com.newrelic.telemetry.metrics.MetricsConverter\",\n \"value.converter.schemas.enable\": false,\n \"topics\": \"nrmetrics\",\n \"api.key\": \"\"\n }\n}\n```\n\nThe above configuration will create a connector named `metrics-connector`, which will ingest metrics from the `nrmetrics` topic. As mentioned above, you will need to replace the `` with the API key that you got from New Relic. This sample message creates three different metrics of the type `Count`, `Gauge`, and `Summary`. **Make sure you use the current timestamp.**\n\n```\n[\n  {\n    \"metrics\": [\n      {\n        \"name\": \"cache.misses\",\n        \"type\": \"count\",\n        \"value\": 15,\n        \"timestamp\": ,\n        \"interval.ms\": 10000,\n        \"attributes\": {\n          \"cache.name\": \"myCache\",\n          \"host.name\": \"dev.server.com\"\n        }\n      },\n      { \n        \"name\": \"temperature\", \n        \"type\": \"gauge\", \n        \"value\": 15, \n        \"timestamp\": , \n        \"attributes\": { \n          \"city\": \"Portland\", \n          \"state\": \"Oregon\" \n        } \n      },\n      {\n        \"name\": \"service.response.duration\",\n        \"type\": \"summary\",\n        \"value\": {\n          \"count\": 5, \n          \"sum\": 0.004382655, \n          \"min\": 0.0005093, \n          \"max\": 0.001708826\n        },\n        \"interval.ms\": 10000, \n        \"timestamp\": ,\n        \"attributes\": {\n          \"host.name\": \"dev.server.com\",\n          \"app.name\": \"foo\"\n        }\n      }\n    ]\n  }\n]\n```\n\nAfter you add the message to the Kafka topic, head over to New Relic One’s Query Builder and run a query like `SELECT * from Metric`. You should see the three metrics above created.\n\nThe required fields in a Metric are explained in the [documentation](https://docs.newrelic.com/docs/telemetry-data-platform/get-data/apis/report-metrics-metric-api#json-payload-keys). Also, note that you can control how many times you want the connector tasks to retry in the case of network issues using these two extra fields in your JSON config: `nr.max.retries` (maximum number of retries, default 5) and `nr.retry.interval.ms` (interval between retries in milliseconds, default 1,000).\n\n## Next steps\nInterested in learning more about Kafka Connect? [Download the Kafka Connect New Relic Connector](https://www.confluent.io/hub/newrelic/newrelic-kafka-connector) to get started! You can also [check out Confluent Cloud](https://confluent.cloud/login?_ga=2.41256098.1248744156.1603040520-1928848811.1603040520), a fully managed event streaming service based on Apache Kafka. Use the promo code **CL60BLOG** to get an additional $60 of free Confluent Cloud.[\\*](https://www.confluent.io/confluent-cloud-promo-disclaimer/)\n\nIf you’re not already a New Relic user, there’s no better time to get started. You can [send 100 GB per month](https://newrelic.com/signup/) to the Telemetry Data Platform and enjoy complete access to all of New Relic—free forever."
    },
    {
        "instruction": "Write a detailed article on how New Relic adapted its code and culture to go open source. Include sections on the decision-making process, internal changes, community engagement, and benefits of going open source. Use a conversational tone to make the content accessible and engaging for readers who are interested in this transition. Mention any specific initiatives or tools used during the adaptation process. Ensure the article is comprehensive yet easy to follow, suitable for both New Relic employees and external stakeholders.",
        "input": "",
        "output": "On April 30 of 2020, New Relic’s Chief Product Officer, [Bill Staples](https://www.linkedin.com/in/williamstaples/), announced internally that we were about to make a major change. Even though we had invested hundreds of thousands of engineering hours towards developing the most comprehensive instrumentation available through our agents, integrations, and SDKs, [we were now going to open source it](https://blog.newrelic.com/product-news/introducing-open-source-agents-and-projects/). All of it. And we needed to do it in only three months.\n\n## The future of New Relic is open instrumentation\nOur Instrumentation team’s mission is simple: “Enable every engineer in the world to gather complete telemetry about their digital systems.” It’s straightforward in language and grand in scope. We share this same grand goal with [OpenTelemetry](http://opentelemetry.io/), a key project of the [Cloud Native Computing Foundation (CNCF)](https://www.cncf.io/). In fact, we’ve been a key contributor to the OpenTelemetry project for [over a year](https://blog.newrelic.com/product-news/observability-open-instrumentation-opentelemetry/), but seeing the growing support for it, especially from some of the top cloud providers, such as Microsoft and Google ([#2 and #3](https://opentelemetry.devstats.cncf.io/d/5/companies-table?orgId=1&var-period_name=Last%20year&var-metric=contributions) contributors respectively), helped us realize the benefit of moving away from vendor-specific instrumentation towards an industry-backed open standard. Supporting a consistent method of instrumentation will only lead to more instrumentation and more data and bring the team closer to accomplishing its mission.\n\nOpenTelemetry is the future of instrumentation and the future of New Relic.\n\nWe’re still developing and supporting our current agents, but, as these CNCF standards mature and their instrumentation becomes generally available, more of our instrumentation for applications, services, and infrastructure will implement the OpenTelemetry and [Prometheus](https://prometheus.io/) standards and be built downstream from their projects.\n\nIt bears repeating: All of our future instrumentation will be open source and based on these open standards when they are mature.\n\n## Lighting an open source fire\n\nThis wasn’t our first foray into open source. Our [Elixir](https://opensource.newrelic.com/projects/newrelic/elixir_agent) and [Roku](https://blog.newrelic.com/product-news/how-to-monitor-roku/) agents launched as open source projects, and the source code for our [Node.js](https://opensource.newrelic.com/projects/newrelic/node-newrelic) and [Ruby](https://opensource.newrelic.com/projects/newrelic/newrelic-ruby-agent) agents has long been available. But the vast majority of our instrumentation engineers weren’t focused on open source projects, nor were they working with OpenTelemetry and Prometheus. We knew that to change into a community-friendly, open source partner, we needed to transform the way that we create and maintain all of our instrumentation.\n\nThe Instrumentation team’s leadership, Ramon Guiu, VP of Product Management, and Greg Unrein, Group Vice President of Engineering, decided this wouldn’t be a gradual change for New Relic—no, we were going to move fast with determination. Greg and Ramon lit a fire under the engineering organization with a three-month deadline. By July 30th, when we would launch our [completely reimagined New Relic One](https://blog.newrelic.com/product-news/new-relic-one-observability-made-simple/), all of our most popular proprietary agents, SDKs, and integrations would live in the open source domain, with more following as they become open source ready.\n\nObviously, we didn’t complete this transformation overnight. A lot went into open sourcing our code. We had to do an enormous amount of code preparation. We had to make sure we weren’t using any third-party, non-open source libraries. We had to prep our code for public consumption, as code commenting and documentation standards had evolved over time. We had to redefine the way we comment and communicate via code. Our Instrumentation team took all of this on with an aggressive deadline. It wasn’t an impossible request, but it was a difficult one.\n\nWe also understand that making the leap from a closed, proprietary culture to an open source culture can make or break a business. It can be the difference between staying innovative or becoming irrelevant. We knew we’d have to change a lot to adopt an open source culture. We’d have to rethink the way we handle issues and feature requests. We’d have to re-establish the way we deal with customer escalations. Security within and around our products would have had to change, as would the way our engineers track their work and write their code. We’d even have to change the way we hire engineers.\n\nWe knew headed into this, that pretty much everything about the way we built, maintained, and supported our instrumentation had to change.\n\n## The Open by Default project\nTo ensure our success through this transformation, we spun up a team led by three Relics:\n\n* [Sharr Creeden](https://www.linkedin.com/in/sharr-creeden/), Director of Software Engineering on our Instrumentation team. Sharr is responsible for the engineering team that builds our instrumentation.\n* [Melissa Klein](https://www.linkedin.com/in/melissa-klein-66a9923/), Open Source Program Manager from our Open Source Program Office. Before joining New Relic, Melissa was the Open Source Program Manager at IBM for 15 years. Her experience is crucial.\n* [Ben Evans](https://blog.newrelic.com/author/benevans/), Principal Engineer and JVM Technologies Architect. Ben has been involved in the production of new Java standards for many years, and he’s been working with open source projects, frequently in leadership positions, for over 20 years. He’s the author of the O’Reilly book, *[Java In A Nutshell](https://www.oreilly.com/library/view/java-in-a/9781492037248/)*.\n\nThey named this project, “Open by Default.”\n\nThe Open by Default project had four guiding principles to shift our Instrumentation team’s focus from building proprietary software to creating open source software:\n\n1. **Open by default:**  Any new software the Instrumentation team builds will be open source by default. The amount of work the team does in non-open, source repos will, over time, decrease to a minimum. This will require a major shift in thinking that goes beyond code to include transitioning into workflows where project governance and design decisions will take place in the open.\n2. **Go where the engineers are:**  If a community already exists, New Relic should contribute to that community, not pull that community to our projects. Contributing code is not the hard part of open source software—community building and maintenance take time, patience, and commitment. Starting a successful new community is difficult, and it can take years to see returns on your investment. We won’t waste our time and resources on building communities that already exist.\n3. **Do the work:**  While we expect that New Relic will contribute in a leadership capacity to many important open source and open standards projects, we recognize that in many cases our contribution will come from showing up and doing the work—and being seen doing it—rather than angling for leadership roles before contributing.\n4. **Our contributions are visible:**  Our web properties should be top hits for engineers searching for docs and other materials relating to the open source projects we contribute to. We must invest resources, inside and outside of Engineering, to ensure that our audience comes to us for information about the open source projects we contribute to and does not read about our work, unacknowledged, on competitors’ websites. Customers and prospects should know that they can find open source at New Relic.\n\n## The Open by Default transformation guidebook\nIn addition to these guiding principles, the Open by Default team published a guidebook, a library of policy and practice proposals collaboratively built across the Instrumentation team. The guidebook defined the future state of how they would work in the open and align everyone on the team around those policies and practices.\n\nThe primary function of the guidebook is to communicate information efficiently and provide mechanisms for collaboration and feedback. The policies in the guidebook include enhancements to:\n\n* **Issue lifecycle management:** Historically, we defined the issue lifecycle process on a per-team basis, so different agents had slightly different processes. Open by Default standardized that process across teams. One notable change is teams moved from private instances of Jira to GitHub Issues, which made issue management more open source friendly.\n* **Feature request process:** Our existing feature request process had three entry points: 1) our product roadmap software, 2) account teams, and 3) support channels. The Open by Default team decided that the most open source-friendly way to handle our feature request process was to stay consistent with how we handle issues and use GitHub Issues. But instead of forcing all feature requests to come through GitHub Issues, the Open by Default team decided to make GitHub Issues the home for feature requests and the place where they would be managed, but to also sync issues with our roadmap software, so that customers and account teams familiar with it wouldn’t be disrupted.\n* **Public roadmaps:** Product roadmaps resided in the same product roadmap software as our feature request process, but, since we were moving a lot of processes to GitHub Issues, the Open by Default team wanted to stay consistent and move roadmaps to GitHub Projects. Doing this amidst all of the other work our engineering teams had to do and on the timeline, they had to work with wasn’t feasible. So they split the transition into two phases: 1) Including a ROADMAP.md file based off of a template at the top-level of each repository as an MVP, 2) move roadmaps to GitHub Projects in the future.\n* **Security process:** We have a robust [security reporting process](https://docs.newrelic.com/docs/security/security-privacy/information-security/report-security-vulnerabilities-hackerone) that we intend to keep, but, with the move to GitHub Issues, we saw an opportunity for security issues to come in through a publicly visible channel. The Open by Default team decided that security issues will not be deleted and will remain publicly visible in GitHub Issues and will be closed (but still searchable and visible) after they are resolved. For highly sensitive, embargo-level security issues that need to be removed from public view, there is a non-default workflow involving tracking in private JIRA.\n* **Customer escalation process:** We updated our existing customer escalation process to adapt to the new channels and audience we get with our move to GitHub Issues. We created templates for issues and feature requests to enable our Global Technical Support team, as well as self-directed users who submit their own tickets. Templates for common replies and routing were developed for engineers who triage issues.\n* **Engineering work tracking:** Since we moved our issue and feature request processes to GitHub Issues, and because OpenTelemetry uses GitHub Issues as well, it only made sense to transition work tracking to GitHub Issues too.\n* **Engineering management practices:** We updated the engineering career ladder and performance analysis process to better align with our engineers’ new work process. Planning and timeline expectations were also updated to better align with the open source workflow.\n* **Single-track development:** The Open by Default team decided that all of our open source products would engage in single-track development by default—no maintaining multiple source code repositories for the same project—except in cases involving embargo-level security issues or work that contains sensitive, confidential, or personally identifiable information.\n* **Hiring profile:** Before the Open by Default project, we hadn’t emphasized open source development, so we needed to update our hiring profile to be better suited for attracting and hiring open source engineers. The Open by Default team revised the attributes and traits we’d look for in a new hire and revised the code challenge to allow candidates to demonstrate an ability to work with open source processes.\n* **Moderation guidelines for the Explorers Hub:** The [Explorers Hub](https://discuss.newrelic.com/) is our forum where customers, especially developers, can learn “how do I” build on New Relic. Users can post questions and answers, give feedback on our open source projects, and contribute to other solutions. For our open source project categories on the Explorers Hub, a process was defined that detailed expectations for moderator response to new topics, communication norms for any questions that required engineering support, and the response cadence for engineering teams.\n\n## Were we successful?\nTo say the initial push to open source our instrumentation was successful would be an understatement. Our engineers not only delivered all of the planned projects by the July 30 deadline, but they delivered more than planned earlier than planned. All of our most popular instrumentation along with several other projects were ready for open source well in advance of July 30, so we pushed up [the announcement](https://blog.newrelic.com/product-news/introducing-open-source-agents-and-projects/) to July 22.\n\n***Check it out: Don’t miss [InfoQ’s Q&A with Sharr, Ben, and Greg](https://www.infoq.com/news/2020/08/newrelic-opensource/) about open sourcing our instrumentation.***\n\nTo measure the success of the Open by Default project beyond the initial launch and announcement, the team decided to look at metrics around issue resolution, which is indicative of our engineers’ adoption of the new processes, as well as community engagement. To satisfy this, at least initially, they incorporated metrics from the GitHub’s Open Source Guide on [Open Source Metrics](https://opensource.guide/metrics/). The plan going forward is to baseline and work toward improving every area of metrics covered in the guide: discovery, usage, retention, and maintainer activity.\n\nBy these measures, was our Open by Default project successful? We don’t know yet. We’re still setting the baselines. Having these metrics defined beforehand, however, helps us identify when something isn’t working with our policies and processes, so we can take steps to remediate the problem, whether that’s by revising our policies and processes, providing additional training to our engineers, or taking problems to project leaders to address with their teams.\n\n## If New Relic can do it, so can you\nThe world runs on software, and we truly believe that to build a more perfect internet and a better world, we need to build better software—together. That starts with each of us open sourcing what we can. If it isn’t a competitive advantage, you should open source it. It’s not simple. It can be difficult, but if we can do it, you can, too. Learn from our story, and do it better than we did. Then share your story, so it’s easier for the next company looking to make this bold move. In fact, this post was inspired by [Adobe’s story about adopting an open source culture](https://medium.com/adobetech/how-adobe-is-changing-its-culture-around-open-source-c9c7daa6dc2a). If we all commit to adopting such practices and sharing our learnings along the way, it’s not an exaggeration to say that releasing open source software can soon be a standard process for all of us, all of the time.\n\n***Visit [New Relic Open Source](https://opensource.newrelic.com/) to learn more about our open source projects and contributions and start contributing today.***"
    },
    {
        "instruction": "Create a detailed announcement for the launch of RudderStack Cloud Free, including:\n- A brief introduction to RudderStack and its capabilities.\n- Key features of RudderStack Cloud Free (no time limit, no credit card required).\n- Benefits of using RudderStack (warehouse-first approach, security, flexibility for developers, scalability, community support).\n- Step-by-step instructions on how to sign up for RudderStack Cloud Free and start using it, including adding a source and setting up an integration with Google Analytics.\n- Encouragement to join the community on Slack and follow social media channels.",
        "input": "",
        "output": "[RudderStack](https://rudderstack.com) is an [open-source](https://github.com/rudderlabs/rudder-server), warehouse-first customer data platform (CDP) that builds your CDP on your data warehouse for you. RudderStack makes it easy to collect, unify, transform, and store your customer data as well as route it securely to a wide range of common, popular marketing, sales, and product tools.\n\nToday, we launched [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial), a no time limit, no credit card required, completely free tier of [RudderStack Cloud](https://resources.rudderstack.com/rudderstack-cloud) (read the blog post announcing RudderStack Cloud Free [here](https://rudderstack.com/blog/start-building-a-better-cdp-for-free-with-rudderstack-cloud-free/)). You get the same great experience you get with RudderStack Cloud Pro, with the only limitation being a cap of 500,000 events per month (that’s roughly 10,000 monthly active users for most sites and apps).\n\n*[Click here to skip the exposition and jump straight to the how to.](#how-to-sign-up)*\n\n## Here’s what you get with RudderStack Cloud Free\n*   **Warehouse-first**: RudderStack is the warehouse-first CDP. RudderStack builds your CDP on your data warehouse, with support for cloud data warehouses like [Amazon Redshift](https://aws.amazon.com/redshift/), [Google BigQuery](https://cloud.google.com/bigquery), and [Snowflake](https://www.snowflake.com). This approach makes it more cost effective, secure, and flexible than other CDPs.\n*   **Secure by design**: RudderStack’s core architecture was built around the principles of privacy and security. That’s the reason it is warehouse-first. RudderStack is a modern CDP that can be hosted SaaS, on-prem, or in your own cloud, so you control your customer data.\n*   **Built for devs**: RudderStack is open source and built API-first, so it is easily customizable and fits into your existing development processes. Its multitude of SDKs, sources, and destinations make it easy to instrument, ingest, and route customer data from all of your digital touchpoints.\n*   **Speed and Scale**: RudderStack is extremely scalable. A single node can easily handle hundreds of thousands of events per day. You can also have a multi-node RudderStack setup, where you can easily scale to millions of events per day. All of this is configurable in a highly secure and available environment.\n*   **Community support**: We have built a rich community of core developers who, along with our engineers, provide daily support on [Slack](https://resources.rudderstack.com/join-rudderstack-slack).\n\n## How to sign up for and start using RudderStack Cloud Free\n*   Go to the [RudderStack signup page](https://app.rudderlabs.com/signup?type=freetrial).\n*   Enter your information in the required fields - your email address, name, organization name and the password needed to sign-in. Then, click on **Create New Account**.\n\n*   You should get a verification code sent to your email. Enter the code in the field and click on **Submit Verification Code**, as shown:\n\nThat’s it. You should now have full access to the RudderStack dashboard.\n\nNow, you can start instrumenting your website or app using RudderStack’s [11 SDKs](https://docs.rudderstack.com/rudderstack-sdk-integration-guides), configuring your customer data **[Sources](https://docs.rudderstack.com/sources)**, and setting up integrations with over 60 third-party **[Destinations](https://docs.rudderstack.com/destinations)**. \n\nThe following sections demonstrate how you can instrument your site with RudderStack’s JavaScript SDK to track and collect events and then route the collected event data to a common destination, [Google Analytics](https://analytics.google.com/).\n\n### Step 1: Adding a source\n*   Sign up for RudderStack Cloud Free by following the steps mentioned above. Once you’ve signed up, log into RudderStack Cloud Free to access your dashboard:\n\n*   Next, click on **Add Source**.\n\n*   Choose the source from which you want to collect the event data. \n \nFor this example, we will choose the **RudderStack JavaScript SDK** to track and collect events from your website. Select **JavaScript**, and click on on **Next**.\n\n*   Add a name for your source, and click on **Next**.\n\n*   You should now seen the following window, containing the source details:\n\nYour JavaScript source is now configured and ready to collect event data from your website.\n\n### Step 2: Adding a destination in RudderStack for routing your event data\n*   Once you have added a source, click on the **Add Destination** button, as shown:\n\n*   Choose the destination platform from the list of destinations and then click on **Next**. For this example, we want to route the collected website events to **Google Analytics**.\n\n*   Add a name for your destination and click on **Next**.\n\n*   The next step is to connect your event data source to this destination. Your previously configured JavaScript source should now appear automatically. Select the source and click on **Next**.\n\n*   Next, you need to specify the connection settings for your destination. For Google Analytics, you will need to enter the **Tracking ID**, which you can retrieve from your Google Analytics admin dashboard, as shown:\n\n*   Enter this tracking ID in the **Connections Settings** window, as shown. You can also configure the other settings as per your requirements, and then click on **Next**.\n\n*   RudderStack also lets you transform your event data before routing it to your destination. You can choose an existing transformation or create a new transformation function to do so. Select the appropriate option and then click on **Next**. \n\nThat’s it! Your destination is now configured successfully. The events from your JavaScript web source will start flowing to Google Analytics via RudderStack. You can also view the events coming from your source in real-time via the **Live Events** tab on the top right, as shown:\n\nYou can refer to our [documentation](https://docs.rudderstack.com/) for more information on how to configure and use RudderStack.\n\n## Start building a better CDP with RudderStack\n\nStart building a better, warehouse-first CDP that delivers complete, unified data to every part of your marketing and analytics stack. Sign up for [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial) today.\n\nJoin our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open source repos on [GitHub](https://github.com/rudderlabs), and follow us on social: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw)."
    },
    {
        "instruction": "Create a detailed announcement for the launch of RudderStack Cloud Free, highlighting its benefits and unique features compared to other CDPs. Emphasize its warehouse-first approach which avoids creating another customer data silo by not storing user data, focusing instead on integrating with existing cloud data warehouses like Amazon Redshift, Google BigQuery, and Snowflake. Mention that it is designed for developers with API-first functionality, 11 SDKs, and over 60 destination connections including compatibility with Segment’s API. Stress the open-source nature of RudderStack and provide links for signing up, joining Slack, checking out GitHub repositories, and following social media channels.",
        "input": "",
        "output": "Today, we launched [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial), a no time limit, no credit card required, completely free tier of RudderStack Cloud. The driving force behind this is simple: we want you to try RudderStack, and RudderStack Cloud Free makes it easier than ever to do that. You receive the same great experience you get with RudderStack Cloud Pro, with the only limitation being a cap of 500,000 events per month (that’s roughly 10,000 monthly active users for most sites and apps). We are confident that if you try RudderStack, you will find value in it and love it.\n\n## RudderStack’s Warehouse-First Approach is Better Than Other CDPs\nThe whole point of your customer data platform (CDP) is to eliminate the customer data silos that are invariably created through your company’s use of a variety of common, popular marketing, sales, and product technologies. Every CDP _claims_ to do this, and modern CDPs, like Segment, actually do this well, but they all have one glaring flaw in their approach. They create another customer data silo, because they store your data. That means you have a third-party data warehouse for your customer data in addition to your own data warehouse, where you store all of your historical data… including another copy of your customer data.\n\n> **RudderStack’s warehouse-first approach fixes this flaw.** \n\nRudderStack does not persist any of your customer data. RudderStack builds your CDP on your data warehouse, with support for cloud data warehouses like [Amazon Redshift](https://aws.amazon.com/redshift/), [Google BigQuery](https://cloud.google.com/bigquery), and [Snowflake](https://www.snowflake.com). No more paying your CDP vendor a premium to store your data. No more concerns about whether your CDP vendor is keeping your customer data private and secure. No more crossing your fingers and hoping the BI, ML, or AI tools you already use and love work with your CDP. No more reliance on your CDPs black box for complex functions like identity stitching.\n\n## RudderStack is Built for Developers\nThe team that owns your data warehouse and data infrastructure should own your customer data stack too. At pretty much every company, that team primarily consists of developers. So we built RudderStack to be easy to use for devs.\n\nRudderStack’s features are built API-first, so they can easily fit into your existing development processes. It offers [11 SDKs](https://docs.rudderstack.com/rudderstack-sdk-integration-guides) in addition to [source integrations](https://docs.rudderstack.com/sources) with popular cloud-based customer tools including [Looker](https://looker.com) and [Customer.io](https://customer.io), so you can instrument and start ingesting customer data from all of your digital touchpoints. RudderStack also offers connections to over [60 destinations](https://docs.rudderstack.com/destinations), so you can route your customer data to all of the systems that need it  – including popular event-streaming platforms like [Apache Kafka](https://kafka.apache.org), data warehouses like Snowflake, cloud tools like [Amplitude](https://amplitude.com), [AppsFlyer](https://www.appsflyer.com), and many more.  \n  \nRudderStack is [fully compatible](https://docs.rudderstack.com/how-to-guides/rudderstack-migration-guide) with Segment’s API too. So, if you have already instrumented your digital touchpoints with Segment, you don’t have to go through the toil of reinstrumenting with RudderStack. Just update the configuration on your Segment SDKs and you’re done.  \n  \nRudderStack is also open source (visit [RudderStack on GitHub](https://github.com/rudderlabs)). So if you ever need to augment or modify your RudderStack, you can, and then, hopefully, you’ll contribute that back to the project, so others benefit from your work too.\n\n## Start Building a Better CDP With RudderStack\nStart building a better, warehouse-first CDP that delivers complete, unified data to every part of your marketing and analytics stack. Sign up for [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial) today.  \n\nJoin our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open source repos on [GitHub](https://github.com/rudderlabs), and follow us on social: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw)."
    },
    {
        "instruction": "Create a detailed blog post about RudderStack's initiative to pay open source contributors. Include the following points:\n- Introduction to RudderStack and its commitment to open source software.\n- Explanation of the challenges faced by RudderStack in managing all enhancements, integration tests, and bug fixes internally.\n- The concept behind paying developers for contributions to specific GitHub issues labeled with `$$$ Bounty`.\n- A step-by-step guide on how interested contributors can apply to work on these bountied issues, including:\n  - Navigating the rudder-server repo to find available issues.\n  - Instructions for applying via a Google Form.\n  - Requirements for having GitHub Sponsors enabled and details on setting it up if needed.\n- The process of selecting applicants and paying contributors through GitHub Sponsors.\n- Highlight two specific bounty issues, including their titles, issue numbers, and the amount of the bounty.\n- Encourage readers to support this initiative by starring the rudder-server repo or applying for available bounties.",
        "input": "",
        "output": "RudderStack is built around open source software. The main component of RudderStack, [rudder-server](https://github.com/rudderlabs/rudder-server), as well as a significant amount of the rest of our software is open source. Our engineering team builds pretty much all of our open source software. We get some contribution but not as much as we'd like.\n\nWe also have enhancements (and integration tests and probably some bugs) that would be great to have but are not a top priority for our engineering team.\n\nWe want open source developers, people like you, to build some of these enhancements. And we are going to pay you to do it.\n\n## Paying for open source contributions... What???\nYep. We are going to pay open source developers to contribute to our repos. It's only fair. This is work that is valuable to RudderStack, that we want built, and that we don't have the capacity to build ourselves. There are tons of open source developers that are more than capable and qualified to do the work. We should pay them to do it.\n\n## How does it work?\nGo to the [rudder-server repo](https://github.com/rudderlabs/rudder-server), go to Issues, and filter the issues to ones with the label `$$$ Bounty`.\n\nIf the Issue has no Assignee, it is open for application.\n\n### How do I apply?\n* Apply to work on any unassigned Issue via the Google Form [here](https://forms.gle/Qifc1xF6Db3uBD7A8). You can apply as an individual or as a team.\n  * Provide the issue number, a link to the issue, GitHub usernames for you and all team members, a single contact email, an estimated date that you think you could complete work by, and a brief note on why we should select you/your team.\n  * **You and everybody on your team must have GitHub Sponsors enabled.** Details on how to enable GitHub Sponsors on your account are [here](https://docs.github.com/en/free-pro-team@latest/github/supporting-the-open-source-community-with-github-sponsors/setting-up-github-sponsors-for-your-user-account). If you are on the GitHub Sponsors waitlist and are selected, we will work with GitHub to get you approved quickly.\n* If we are interested in your application, we will reach out via email. If we aren't, we will do our best to reach out in some way as well.\n* Once a contributor is selected, they will be assigned the Issue in GitHub, and applications will be closed.\n* After completion and a PR has been approved (not necessarily merged) by RudderStack, the contributor will be paid via GitHub sponsors in the most expedient way possible (e.g. until one-time payments are implemented for GitHub Sponsors, if bounty = $2500 => 2-months of $1000 sponsorship and 1-month of $500 sponsorship).\n\n**\\*\\*\\* DO NOT SUBMIT A PULL REQUEST FOR ANY BUG OR ENHANCEMENT WITH AN ASSOCIATED BOUNTY UNLESS IT IS ASSIGNED TO YOU. PULL REQUESTS FROM UNASSIGNED CONTRIBUTORS WILL BE REJECTED. \\*\\*\\***\n\n## What enhancements can I get paid to work on?\nThe first two GitHub Sponsors bounties are:\n* [Issue \\#655: Jamstack instrumentation - Gatsby plugin + Netlify function](https://github.com/rudderlabs/rudder-server/issues/655).  \n**Bounty: $2000.**\n* [Issue \\#656: Roku SDK](https://github.com/rudderlabs/rudder-server/issues/656).  \n**Bounty: $2000.**\n\nWe will be rolling out more of these over time. The more interest and success we have, the more we will do. So, if you like this approach and support devs getting paid for their open source contributions, go star the [rudder-server repo](https://github.com/rudderlabs/rudder-server), or, even better, see what GitHub Sponsors bounties are available and apply."
    },
    {
        "instruction": "Provide an overview of how open source software impacts the tech industry and discuss RudderStack's involvement with GitHub Sponsors, particularly focusing on their initiative to compensate developers through enhancement bounties. Include details about the launch of GitHub Sponsors for companies at GitHub Universe 2020 and explain how this new channel aims to improve sustainability in open source development. Mention specific examples of enhancement bounties offered by RudderStack and detail their approach compared to other companies using GitHub Sponsors. Additionally, encourage potential contributors to follow RudderStack’s social media channels for updates on future opportunities.",
        "input": "",
        "output": "We all use **Open Source Software (OSS)** all the time, even when we don’t know it. The backbone of the Internet is built on OSS. OSS makes standing on the shoulders of coding giants as easy as forking a repo, which just isn’t possible with closed source software. Yet a vast majority of developers that build and architect OSS, unless in the small, incredibly lucky minority of developers fully employed by a company or organization to do open source work, don’t get compensated for their work. We are working to change that.\n\n## RudderStack Partners in Launch of GitHub Sponsors for Companies\n\nWe partnered with GitHub in their launch of [GitHub Sponsors](https://github.com/sponsors) for companies at [GitHub Universe 2020](https://githubuniverse.com/). Open source developers deserve to be compensated for their work, and GitHub Sponsors for companies gives a new, better channel for compensating them. A channel that we are excited to be using.\n\nGitHub Sponsors launched in 2019. It allowed individuals to support open source projects and developers. Millions of dollars have been sent to open source developers through GitHub Sponsors already, with some even making six-figures annually. This has helped to make open source more sustainable and has encouraged some developers to make open source their full-time job.\n\nGitHub Sponsors for companies gives organizations this same ability to invest in open source all within the GitHub ecosystem, making the compensation process easier for companies and whoever they sponsor. Due to the way that GitHub Sponsors works and its initial intent, individuals sponsoring open source projects and devs for work they have done, almost all of the companies that are currently participating in GitHub Sponsors are supporting open source projects and contributors to open source projects that the companies use or that their engineers vote to sponsor. We decided to take a different approach.\n\n## Enhancement Bounties\n\nSince RudderStack is open source, and we want more external contribution to our projects, we are using GitHub Sponsors to directly compensate developers for work that we want done via what we are calling Enhancement Bounties. Each bounty is defined in a GitHub Issue in our [rudder-server repo](https://github.com/rudderlabs/rudder-server) that includes the label “$$$ Bounty”. We already have two enhancement bounties, [one for Jamstack instrumentation](https://github.com/rudderlabs/rudder-server/issues/655) that is nearing completion and [one for a Roku SDK](https://github.com/rudderlabs/rudder-server/issues/656) that hasn’t been picked up by anyone yet. Each has a bounty set at $2,000. You can read our initial announcement of them in our [dev.to post](https://dev.to/rudderstack/devs-wanted-get-paid-to-contribute-to-rudderstack-s-open-source-software-bjp). The results of our first bounty have been good, with a Gatsby plugin for RudderStack built by [Chris Wray](https://github.com/cwray-tech) and already published to [npm](https://www.npmjs.com/package/gatsby-plugin-rudderstack) and [Gatsby’s plugin page](https://www.gatsbyjs.com/plugins/gatsby-plugin-rudderstack/) (GitHub repo [here](https://github.com/rudderlabs/gatsby-plugin-rudderstack)).\n\nWe will have new enhancement bounties coming soon with each announced in a post on [our dev.to](https://dev.to/rudderstack) as well as [our Twitter](https://twitter.com/RudderStack). Follow us on both for info about future enhancement bounty opportunities.\n\nStart building a better, warehouse-first CDP that delivers complete, unified data to every part of your marketing and analytics stack by signing up for [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial) today. Join our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open-source repos on [GitHub](https://github.com/rudderlabs), and follow us on [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw)."
    },
    {
        "instruction": "Write a detailed guide on redefining customer data analytics using an open-source stack. Include sections on selecting appropriate open-source tools, setting up the infrastructure, and implementing analytics workflows. Use a conversational tone to make the instructions accessible and engaging for readers who are following along step-by-step. Mention any specific tools or configurations used in the process, such as Apache Kafka, Elasticsearch, and Kibana. Ensure the content is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in this setup. Include code snippets and configuration examples where applicable.",
        "input": "",
        "output": "_Thank you Yakko Majuri from [PostHog](https://posthog.com/) for coming up with the idea for this article and for your feedback and contributions to it._\n\nA data analytics stack enables all of the teams across your organization to look at important metrics and make data-driven decisions. It integrates different technologies needed to efficiently collect, store, transform, and analyze your data and to derive critical insights from it. \n\nWhen it comes to using an analytics stack, businesses are often faced with two choices - buy one or more proprietary tools or build an open source analytics stack. While proprietary tools often offer best-in-class analytics and data management services, they also have some major downsides that include premium pricing plans, vendor lock-in, limited flexibility, and, often, no ability to host the solution on your own infrastructure. \n\nThese downsides as well as the high level of sophistication of today's open source tools are why we make the argument that you should build an open source analytics stack. In this post, we'll talk about why you should use open source software to build your analytics stack and the types of tools you need along with some popular open source examples of each.\n\n## Why an open source analytics stack?\n\nSetting up your analytics stack with proprietary software often allows you to get started quickly but comes at the significant cost of data ownership. This often makes these solutions unsuitable for large enterprises, who either build key parts of their own stack from scratch or require extensive due diligence, to ensure compliance with their own security protocols as well as regulations such as GDPR and CCPA, before incorporating a new proprietary tool into their processes.\n\nOpen source software, on the other hand, reduces the concerns regarding data privacy and ownership, since users can deploy the software in their own infrastructure. First, companies can ensure the software meets their requirements, as open source software gets rid of the “black box” effect, allowing anyone to inspect the code that they’re running. Second, not sending any data to third parties makes compliance with enterprise security protocols and data legislation easier. And, most importantly, having full access to the underlying data you’re processing makes it much simpler to perform custom analytics, run ETL between platforms, and even tweak the software to your specific use-cases.\n\nOpen source software also offers better flexibility in terms of the tools you use to build your stack. You aren’t tied to a handful of proprietary solutions that it fiscally makes sense to run multiple business processes through, inevitably delivering less than optimal performance across many, if not all, of those processes. Using open source tools allows you to, instead, use the best solution to solve each problem. This encourages teams to innovate and gives them the freedom to leverage better tools and features than what many proprietary solutions deliver.\n\nProprietary tools make you heavily dependent on vendors for updates, bug fixes, and more. Open source products help prevent this vendor lock-in, since users themselves can make or suggest changes to the product, without having to rely on vendors for updates and bug fixes. It’s also easier to migrate away from a tool when you’re in full control of its deployment and the data it handles.\n\nAdditionally, a lot of this proprietary software is ridiculously expensive. If you don’t have a six- or seven-figure budget for an analytics stack, open source tooling is really your best choice. Open source software, in addition to generally being cheaper to try out at the early stages, allows companies to more easily scale their infrastructure up and down as needed to cope with fluctuating volumes, optimizing for lower costs as a result. Moreover, support is not limited to the company who developed the software, as it can be sourced from the open source community for free.\n\nTo achieve the control, flexibility, and data-privacy of open source, you often have to build tools in-house. However, leveraging existing, high-quality, open source software means significantly reducing development and product costs, for what is likely a much higher quality and more easily maintained solution than most can build in-house.\n\n## What does an open source analytics stack look like?\n\nA great analytics stack should be able to:\n\n*   Integrate data (in different formats) sitting within multiple platforms.\n*   Ingest data into a storage system (a data warehouse).\n*   Clean and Transform data for different use cases.\n*   Use transformed data for analytics like visualization or machine learning.\n\nHere’s what an ideal open source analytics stack would look like:\n\nOur goal is to help you understand how building your entire data analytics stack with completely open source solutions can help your business scale with minimal cost and a high-level of security.\n\n## What tools make up an open source analytics stack?\n\nAlmost all data analytics systems follow the same basic approach for setting up their analytics stack:  data collection, data processing, and data analytics. The tools used to perform each of these activities form the analytics stack. An open source analytics stack is no different, just that it uses open source tools to obtain the same results or better than proprietary tools. \n\nLet's understand each of the processes in detail and how open source tools contribute to each process in the open source analytics stack.\n\n### Data Ingestion and Transformation\n\nThe primary step for collecting your data for analytics is to ingest it from all your sources including your in-house applications, SaaS tools, data from your IoT devices, and all other sources. Various tools are available to make this process a seamless experience. \n\n#### ETL vs ELT\n\nUntil recently, data ingestion followed a simple ETL (Extract, Transform, and Load) process in which data was collected from a source, realigned to fit the properties of a destination system or business requirements, and then loaded into that system. Creating in-house ETL tools would mean taking developers away from the user-facing products which puts the accuracy, availability, and consistency of the analytics environment at risk. While commercially packaged ETL solutions are available, an open source alternative is a great option. One such example is **Singer** ([website](https://www.singer.io/), [GitHub](https://github.com/singer-io)), an open source ETL tool used to program connectors for sending data between any custom sources and targets like web APIs and files. \n\nDue to the rise in cloud-based data warehouses, businesses can directly load all the raw data into the data warehouse without prior transformations. This process is known as ELT (Extract, Load, Transform) and gives data and analytics teams freedom to develop ad-hoc transformations based on their particular needs. ELT became popular as the cloud's processing power and scale became better suited to transforming data. **DBT** ([website](https://www.getdbt.com/), [GitHub](https://github.com/fishtown-analytics/dbt)) is a popular open source tool recommended for ELT and allows businesses to transform data in their warehouses more effectively.\n\n#### Real-time data streams\n\nWith the increase in real-time data streams and event streams, certain use cases emerged that require access to real-time data such as financial services risk reporting or detecting a credit card fraud. Real-time streams can be obtained using a stream processing framework like **Apache Kafka** ([website](https://kafka.apache.org/), [GitHub](https://github.com/apache/kafka)). The focus is to direct the stream of data from various sources into reliable queues where data can be automatically transformed, stored, analyzed, and reported concurrently.\n\n### Customer Data Platform (CDP)\n\nTalking about successful data ingestion tools, most businesses rely increasingly on different Customer Data Platforms (CDPs) that track, collect, and ingest data from multiple sources and systems into a single platform to get a unified customer view. **Apache Unomi** ([website](https://unomi.apache.org/), [GitHub](https://github.com/apache/unomi)) is a perfect example of an open source CDP that ingests data and collects it at one place. \n\nHowever, limitations to traditional CDPs, especially around connecting to best of breed customer tooling and exposing data for use across an organization have driven a new generation of non-CDPs. Solutions like **Snowplow’s** ([website](https://snowplowanalytics.com/), [GitHub](https://github.com/snowplow)) data delivery platform and **RudderStack’s** ([website](https://rudderstack.com/), [GitHub](https://github.com/rudderlabs/rudder-server)) smart customer data pipeline ingest data from a multitude of sources, apply in-stream transformations, and route data to your data warehouse, like Snowplow, or your warehouse plus your preferred customer tooling destinations for activation, like RudderStack. \n\n### Data Warehouses\n\nThis is the next important piece of the analytics stack. Data warehouses act like a common repository for companies to store data collected from different sources where it can be transformed or combined for different use cases. Data warehouses store both raw and transformed data and can be easily accessed by all employees within an organization. They are often similar to a traditional database, but many traditional databases were designed to store data based on and be accessed by specific domains like finance, human resources, and so on, which results in data silos and disconnected data. Over the years, as cloud data warehousing has taken roots, more and more companies are migrating from an on-premise to a modern, cloud data warehouse. \\\n\nMoreover, using open source warehouse tools can allow unlocking additional insights from your data in real-time and with lesser cost. **PostgreSQL** ([website](https://www.postgresql.org/), [repo](https://git.postgresql.org/)) is a popular example of an efficient and low-cost data warehousing solution. Another example is **ClickHouse** ([website](https://clickhouse.tech/), [GitHub](https://github.com/ClickHouse/ClickHouse)), an open source, analytics-focused DBMS that allows generating analytical reports from data in real-time using SQL.  \n\n### Data Consumers\n\nAfter your data is ingested and transformed, it is sent to different platforms to leverage cutting edge analytics and get more out of your data. There are various tools available for your different analytics needs. We have curated a few leading open source tools that will fit right into your analytics stack and provide a lot of valuable insight. \n\n**Matomo** ([website](https://matomo.org/), [GitHub](https://github.com/matomo-org/matomo)) is an open source web analytics tool and calls itself a Google Analytics alternative. Matomo gives you valuable insights into your website's visitors, marketing campaigns etc., making it easy to optimize your strategy and online experience of your visitors.\n\nThe self-hosted **PostHog** ([website](https://posthog.com/), [GitHub](https://github.com/PostHog/posthog)) is an excellent open source alternative for product analytics and can be easily integrated into your infrastructure. You can easily analyze how customers interact with your product, the user traffic, and ways to improve your user retention. \n\n**Countly** ([website](https://count.ly/), [GitHub](https://github.com/Countly)) is also an open source product analytics platform that is designed primarily for marketing organizations. It helps marketers track website information (website transactions, campaigns and sources that led visitors to the website, etc.). Countly also collects real-time mobile analytics metrics like active users, time spent in-app, customer location, etc., in a unified view on your dashboard.\n\n### Business Intelligence\n\nBusiness intelligence (BI) has become prevalent in nearly every organization to get a regular health check on their business operations. BI provides you with excellent ways to analyze your historical data, apply learnings to your current operations, and make better-informed business decisions for your future. Every business is different with different goals, so choosing a BI tool that exactly fits your use case is essential.\n\n \n\nWith self-service dashboards, business leaders can fully leverage BI tools to understand the impact of their decisions on the business. BI tools also provide ad-hoc analysis with customizable features such as data filters and data grouping to find interesting trends. They connect directly to your data warehouse and can be applied to any set of transformed data in your warehouse.\n\nOpen source BI platforms such as **Metabase** ([website](https://www.metabase.com/), [GitHub](https://github.com/metabase/metabase)) and **Apache SuperSet** ([website](https://superset.apache.org/), [GitHub](https://github.com/apache/incubator-superset)) are easy to deploy without IT involvement. Metabase lets you build dashboards from the data in your warehouse easily, with no SQL, or, if you have data engineering or science know-how, inside more powerful and flexible notebooks or with SQL itself. Similarly, Apache SuperSet helps businesses explore and visualize data from simple line charts to detailed geospatial charts.\n\n### ML and AI for Analytics\n\nIf implemented and trained well, machine learning (ML) and artificial intelligence (AI) can rapidly and accurately spot actionable trends in your data that would take even the best data science teams months or years to do manually. They are notoriously difficult to implement and train well though, generally requiring large, rich datasets for training with a predilection to pick up and emphasize any biases in the training data. That is one of the big reasons that these advanced analytics tools may not be implemented by many data companies, even less so full-fledged, but, if done well, they can add tremendous value to your data.\n\nML allows you to input transformed or modelled data into platforms such as **KNIME** ([website](https://www.knime.com/), [GitHub](https://github.com/knime)), deployed on open source tools like R, Python, and so on, to train, evaluate, and deploy models. These models integrate with your company's existing products to create customer-facing features like a recommendation engine. \n\n## Build your own open source analytics stack\n\nBuilding a completely open source analytics stack can be challenging. However, as your business evolves and the systems you use evolve, your data evolves too, and your needs change. You will have to look for new data tools to scale and grow. An open source analytics stack will let you test and choose the best data tools for your company without the cost and security concerns presented by many proprietary, hosted SaaS tools. As you can see in this article, the variety of high quality, open source data tools spans the whole analytics stack, frequently with multiple options at each layer. The number of these tools and coverage of the analytics stack has been increasing and will continue to increase in the future. So as time goes on, your open source analytics stack will only get better and better.\n\n## Try RudderStack Today\n\nStart building a smarter customer data pipeline. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack. Sign up for [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial) today.\n\nJoin our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on social: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw). Don’t miss out on any updates. [Subscribe](https://rudderstack.com/blog/) to our blogs today!"
    },
    {
        "instruction": "Write a detailed article on why you might not want a customer data platform (CDP). Include sections on the limitations of CDPs, alternative approaches to managing customer data, and best practices for effective data analytics without a CDP. Use a conversational tone to make the content accessible and engaging for readers who are interested in this topic. Mention any specific tools or strategies discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in customer data management.",
        "input": "",
        "output": "It’s been almost a year since Segment and a group of their partners declared that [CRM is not enough](https://segment.com/crm-is-not-enough/). We agree with their assessment that you have to go far beyond CRM to connect the disparate channels through which Sales, Support, Marketing, and Product engage your customers. \n\nWe also agree with the three market forces that drove their declaration: \n\n1) the demand for interoperability of customer tools; \n2) the resistance to vendor lock-in; and \n3) the preference for software that enables better customer experiences.\n\nWhat we don’t agree with is the claim that you need a customer data platform (CDP) in addition to your CRM.\n\nYou don’t actually want a CDP; and here’s why...\n\n## What is a CDP?\n\nCDPs promise to do almost everything, depending on the vendor and their inclination towards exaggeration. However, these exaggerations and the “art of the possible” sold as reality don’t define what CDPs actually do.\n\nThe most widely accepted definition for a CDP comes from the [CDP Institute](https://www.cdpinstitute.org/). “A Customer Data Platform is packaged software that creates a persistent, unified customer database that is accessible to other systems.” The three cornerstones of this definition are:\n\n*   **A CDP must be packaged software.** They are prebuilt systems that require less technical expertise to deploy and maintain than a standalone data warehouse project.\n*   **A CDP must create a persistent, unified customer database.** Customer data is ingested from multiple systems and linked to create a comprehensive view of each customer that is stored and can be analyzed over time.\n*   **A CDP must be accessible to other systems.** Customer data stored in a CDP needs to be accessible by other systems for analysis and activation.\n\nThe CDP Institute’s definition is primarily concerned with the technical aspects of a CDP. In use, CDPs are marketing tools and a less technical definition can tell us more about what a CDP is supposed to do. In this vein, [Gartner](https://www.gartner.com/smarterwithgartner/marketers-to-guide-customer-data-platforms/) gives a good definition of a CDP:\n\n_“A CDP is a marketing system that unifies a company’s customer data from marketing and other channels”_\n\nGartner also notes that CDPs overlap with many other related technologies, as we saw in the introduction. There is an overlap with CRMs and increasingly there’s a big overlap with data infrastructure technologies — like a data warehouse.\n\nHaving said that, what CDPs are _not_ is platforms. This is probably one of the reasons for the confusion around the term and the value of these products.\n\nBy [definition](https://en.wikipedia.org/wiki/Platform_as_a_service), a platform is a technology that allows customers to develop, run, and manage applications. \n\nBased on the above definition, a CDP is more of a piece of software — marketing software specifically — than a platform. And that’s totally fine, but using the term platform adds to the confusion around the purpose of these tools.  \n\n### What Problems do CDPs Solve?\n\nThe [primary function](https://www.gartner.com/smarterwithgartner/marketers-to-guide-customer-data-platforms/) of a CDP is to enable customer modeling and optimize the timing and targeting of messaging and offers. \n\nTo deliver that, it is important for a CDP to be able to consolidate customer data into one place, create customer profiles and finally create some kind of persistent concept of identity around the customers. \n\nTo do the above, the primary technical problem that CDPs solve is identity resolution. You’ve probably seen tons of names for this — they vary based on the CDP. Frequently it will involve the term “customer 360,” a term so overused and loaded that just seeing it raises red flags about the utility of the software for us.\n\nOnce you get past all of the naming shenanigans, identity resolution is a pretty straightforward idea that is incredibly difficult to execute. CDPs ingest data from multiple disparate customer tools, convert it into a standardized form, and build an identity graph from it. This identity graph lets your applications identify customers in real-time and activate based on customer attributes that are either collected by your CDP or produced from some form of analysis.\n\nThis all sounds simple, except that building an identity graph is quite difficult. There are numerous methods and algorithmic approaches to building an identity graph, with the effectiveness of each being highly dependent on its implementation and the context of the data it is applied to. CDPs obscure this difficulty, which is nice for users, but it also obscures the effectiveness of their identity resolution. Is a CDP’s identity resolution approach good? It’s difficult to know.\n\nNearly all of the other CDP use cases are derived from identity resolution. Segmentation, personalization, recommendations, campaign optimization, etc. — these all rely on identity resolution. Having a unified customer profile that your customer tools can access gives you the ability to solve a lot of problems. There are use cases that don’t necessarily involve identity resolution, like sessionization and customer journey mapping, but most CDPs don’t support them.\n\n## That all Sounds Really Great. So… Why Don’t I Want a CDP?\n\nIf a CDP resolves your customers’ identities, unifies your customer data, and helps solve a bunch of important, difficult customer experience problems, it seems like you’d want one. All of these promises CDPs make are why they have become a hot commodity.\n\nHowever...CDPs suffer from a variety of problems, many of which are familiar to anyone in the[ data infrastructure industry](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf). It makes sense, because at the end of the day a CDP has to solve many of the same problems a data warehouse is also traditionally trying to solve. Consolidating data, offering analytics, interoperating with other technologies — all of these are functions shared between the two. So, it’s not surprising that they also share some of the problems.\n\n### Product Cost\n\nA recurring complaint with CDPs is that they are too expensive. They are so expensive that companies frequently have to evaluate whether they should build their own solution, instead of buying a solution from a vendor. This isn’t a decision that should even be on the table. When was the last time you heard of somebody considering building their own CRM because there were no reasonable alternatives? How about marketing automation software? It doesn’t happen, because the difficulty and expense of building your own CRM or marketing automation solution would far outweigh the cost of paying a vendor for their solution. That isn’t the case for CDPs, very simply because CDPs are way too expensive.\n\n### Total Cost of Ownership\n\nApart from the cost of the product itself, CDP users have to double the storage costs — since data has to be replicated to the CDP. This might be included in the price of the CDP; but still, keeping multiple copies of effectively the same data adds unnecessary cost. Include also the cost of building and operating ETL processes to ingest the data into the CDP, and the total cost of ownership increases even more.\n\n### Data Lock-in\n\nData ownership defines who has control over data security, data access, and data privacy. Using a CDP lessens your ownership in a few ways, but most drastically in the area of data security. Since CDPs build and store your customer data warehouse in their infrastructure, you don’t have visibility and control over all accesses to your data — which means you can’t ensure data security. Third-party vendors are arguably the biggest risk to data security that companies have. If your vendor is storing your customer data and they have a security breach, you have a security breach too. You just can’t do anything about it. Your data is one of your biggest assets, so you should have total control and ownership over it.\n\n### Data Staleness and Reliability\n\nIs the data in the CDP accurate and up-to-date? How often can you sync the data into your CDP? Maintaining state accurately between different systems is a hard problem. By adding yet another data consumer (the CDP) to your overall data infrastructure, it just makes things even more complicated. So, you are either going to compromise the quality of your data — something that will cost you in terms of the effectiveness of your marketing campaigns — or you will invest money to ensure that your data infrastructure will reliably deliver the SLAs that your marketing teams need. In both cases, the total cost of ownership will increase.\n\n### Limited to Vendor-Defined Problems\n\nSome older, traditional CDPs provide tools to execute Marketing and Sales activities, like email campaigns and digital personalization. These solutions are limited to only solving the problems that they’ve built functionality for. If you step outside of their functional wheelhouse, effectively using your customer data will be a challenge. \n\nModern CDPs, like Segment, don’t provide tools to execute Marketing and Sales activities. Instead they provide connections to best-of-breed tools for execution. These solutions are less limited in function than traditional CDPs, but they are still limited to which insights are derived and can be activated on. You can’t just build a model on your customer data, you are limited to the models they provide and run on your data in a black box. So while a feature like Segment’s Personas provides useful, powerful audiences that are easy to use, you can’t go much beyond it. If instead of activating based off of an audience, you wanted to activate based off of sessionized data or customer journey maps, then you’re out of luck.\n\n### Lack of Advanced Analytics\n\nThe bread and butter of a CDP is to offer methods for segmenting your customers based on behavioral characteristics. From a data analytics perspective, these methodologies and queries are pretty simple. For this reason it’s also possible to offer very intuitive user interfaces to define user segments. We are living in an era of ML and AI though. The value of simple queries and intuitive interfaces, while very high for human users, is very low for these technologies. How can these new technologies be incorporated into your CDP? Again, this is not a problem unique to CDPs. Vendors like Snowflake and Google are augmenting their data warehouse solutions with ML algorithms that can be executed over your datasets and enrich your data inside the data warehouse. \n\n## A CDP Should be an Application Built on top of Your Existing Data Infrastructure.\n\nA CDP is not a platform, but the best way to implement one is by building it on top of your existing data infrastructure or data platform — as a data application. \n\nThe problems that CDPs have, plus several other issues CDP users run into frequently, are caused by CDPs trying to reimplement a complete data infrastructure to deliver their value. If a CDP didn’t store your customer data, your costs would be lower; you wouldn’t have to add the vendor’s security to your list of security concerns and you wouldn’t be limited to only solving the problems that the vendor defines.\n\nGiven the power of today’s modern cloud data warehouses, the solution is obvious. Your customer data warehouse and your identity graph should be built and stored on your data warehouse — not a CDP vendor’s. Then you can control your costs, improve your data security, and expand the problem set your data scientists can solve for.\n\n## Instead of a CDP, you Need Smarter Customer Data Pipelines.\n\nYou need a tool that will make it easy to ingest customer data, route data to downstream customer tools, and build your customer data warehouse and identity graph on your data warehouse. You don’t need a platform. You need a pipeline — a pipeline that is designed for customer data, and that can automatically build your customer data warehouse and identity graph.\n\n[RudderStack](https://rudderstack.com/) is the smart customer data pipeline. It is warehouse-first. It builds your customer data warehouse and your identity graph in your data warehouse, giving you complete control over your customer data. With RudderStack, you can easily build pipelines that connect your whole customer data stack and then make them smarter by ingesting and activating enriched data from your warehouse.\n\nThe warehouse-first approach eliminates the problems that CDP users frequently face. It solves the use cases associated with a CDP and enables many more that CDPs don’t address, all while being significantly more cost-effective, secure and accessible by other tools. Warehouse-first is a simple approach, but it’s a good one and one that we think will power the future of customer data tooling and analysis."
    },
    {
        "instruction": "Create a promotional blog post for RudderStack highlighting the benefits and features of their newly launched **Cloud Extract** and **Warehouse Actions**. The post should include:\n\n- An introduction to how RudderStack helps in using all customer data, answering difficult questions, and sending insights across the customer data stack.\n- A detailed section on **Event Stream**, emphasizing its role as a key feature for easy event data collection and routing.\n- An explanation of the challenges with non-event customer data locked into cloud applications like Salesforce and ZenDesk, which are difficult to combine with event data.\n- Introductions to Cloud Extract and Warehouse Actions:\n  - **Cloud Extract**: Focus on how it simplifies building ELT pipelines from cloud apps to warehouses, enabling easier integration of product, sales, marketing, support, and finance team data with customer event data.\n  - **Warehouse Actions**: Emphasize its role in activating insights from the warehouse across all customer tools.\n- Use cases for Cloud Extract and Warehouse Actions including audience segmentation, recommendation engines, personalization, and lead scoring/tagging. Provide examples of how these can be implemented using RudderStack features.\n- A call to action encouraging readers to try RudderStack, along with links to sign up for the service, join their Slack community, explore GitHub repositories, follow social media platforms, and subscribe to the blog.",
        "input": "",
        "output": "## Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack.\n\nRudderStack **Event Stream** is what we are known for and has always been top-notch. Our SDKs and plugins, like our [Gatsby plugin](https://www.gatsbyjs.com/plugins/gatsby-plugin-rudderstack/), make it easy to instrument your sites and apps, collect your event data, and route it to all of your customer tools. Your event data isn’t all of your customer data though. It’s only a subset. You still have non-event customer data locked into cloud applications, like [Salesforce](https://docs.rudderstack.com/sources/salesforce) and [ZenDesk](https://docs.rudderstack.com/sources/zendesk), that is near-impossible to combine and correlate with your event data, apply deep analysis to, and activate in your customer tools. It’s too difficult to use _all_ of your customer data.\n\nRudderStack has been warehouse-first since the beginning as well. We build your _customer_ data warehouse on your data warehouse for you. It’s a first-class citizen for us, because the most valuable work on your customer data happens in your warehouse. But it’s very manual and difficult to activate the insights from your warehouse in your customer tools. Like data in cloud apps, these insights get trapped in your data warehouse.\n\nToday, we’re solving both of these issues with the launch of two new, powerful features, **Cloud Extract** and **Warehouse Actions**. Cloud Extract makes it easy to build ELT pipelines from your cloud applications to your warehouse by providing out-of-the-box integrations for popular tools like Salesforce, ZenDesk, and many more (even Google Sheets). Warehouse Actions takes being warehouse-first to the next level by letting you route analysis-driven insights from your warehouse to all of your customer tools for activation.\n\nNow, with RudderStack, you can use all your customer data to answer more difficult questions and send those insights to your whole customer data stack.\n\n## Cloud Extract\n\nRudderStack Cloud Extract lets you go beyond event streaming and use all of your customer data. It makes it easy to build ELT pipelines from your cloud applications to your data warehouse, letting you aggregate all of your customer data in your warehouse, from customer event data generated by digital touchpoints to non-event customer data from different cloud applications. This enables you to easily integrate data from your product, sales, marketing, support, and finance teams’ cloud tools with your customer event data so that you can surface answers to more complicated, nuanced business questions.\n\n_Learn more about Cloud Extract including how to use it in our post [RudderStack Cloud Extract makes cloud-to-warehouse pipelines easy](https://rudderstack.com/blog/rudderstack-cloud-extract-makes-cloud-to-warehouse-pipelines-easy)._\n\n## Warehouse Actions\n\nOnce data is successfully ingested into your warehouse, your teams can then apply advanced queries or models to generate valuable insights that are frequently materialized as their own tables and stored in your data warehouse.\n\nRudderStack Warehouse Actions let you send this rich analysis from your warehouse to your entire customer data stack. Your data warehouse effectively becomes a data source for enriched audiences and events that RudderStack can ingest and route to all of the customer tool destinations we support.\n\n_Learn more about Warehouse Actions including how to use it in our post [RudderStack Warehouse Actions unlocks the data in your warehouse](https://rudderstack.com/blog/rudderstack-warehouse-actions-unlocks-the-data-in-your-warehouse)._\n\n## Solve more complex analytical problems\n\nCloud Extract and Warehouse Actions are both powerful features that when used along with Event Stream can help you solve difficult problems that aren’t easy to tackle with the existing tools on the market.\n\n### Audience Segmentation\n\nThis marketing strategy refers to creating subgroups (based on demographics and customer buying patterns) within your target audience that lead to customized experiences and stronger connections with your customers.\n\nNow you can perform your own segmentation on your warehouse using complex queries or tools like DBT, then use Warehouse Actions to send audiences and cohorts to all of your customer tool destinations.\n\n### Recommendation Engines\n\nA recommendation engine is a system that suggests related and relevant products, services, and other information based on the analysis of customer data. With a recommendation engine, companies can easily boost revenue, conversions, and other sales and engagement metrics. Good recommendations also improve customer satisfaction and, as a result, retention. \n\nNow you can build your own recommendation engine, using tools like Google BigQuery ML, on your data warehouse and use RudderStack Warehouse Actions to send the results to all of your customer tool destinations.\n\n### Personalization\n\nPersonalization customizes the customer experience on a cohort or even individual level, making interactions smoother and more powerful. This leads to increased customer satisfaction and the likelihood of repeat customer visits.\n\nJust like the recommendation engine, you can build your own personalization engine on your data warehouse and use Warehouse Actions to send the results to your user profile store in tools like [Redis](https://redis.io/), which drive website and app personalization.\n\n### Lead Scoring/Tagging\n\nLead scoring is a popular sales and marketing method for ranking leads to determine their sales-readiness or intent to buy. Leads are scored are largely based on behavior, which is represented as events (like browsing behavior, conversion, etc.), and profile, which is represented as customer traits. Many companies also group leads based on different categories using a tagging system.\n\nNow you can use your complete customer data set to perform lead scoring and automatically sync it back to your CRM. With Event Stream and Cloud Extract, you can:  \n*   Use Event Stream to collect behavioral event data from your websites and apps.\n*   Use Cloud Extract to combine event data in your warehouse with up-to-date lead data from your CRM.\n*   Build the lead scoring/tagging logic on your warehouse that uses all of your customer data (event and non-event).\n*   Use Warehouse Actions to send the results back to your CRM.\n\n## Try RudderStack today\n\nStart building a smarter customer data pipeline. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack. Sign up for [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial) today.\n\nJoin our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on social: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw)."
    },
    {
        "instruction": "Explain the concept of \"warehouse-first architecture\" in the context of cloud data warehouses (CDWs) and discuss its key benefits, including improved data control, increased flexibility without duplicated data, and lower costs. Provide examples of companies that have adopted this approach and mention how it is reshaping the software industry. Include insights from recent developments such as Snowflake's IPO and Databricks' funding round to illustrate the growing importance of CDWs. Mention RudderStack’s role in promoting warehouse-first architecture and invite readers to learn more about the topic in a follow-up article by MessageGears.",
        "input": "",
        "output": "Last year may well have been the year of the cloud data warehouse (CDW). Snowflake had the [biggest software IPO ever](https://www.cnn.com/2020/09/16/investing/snowflake-ipo/index.html) (at the time) and a [blistering growth rate](https://www.cnbc.com/2020/12/02/snowflake-snow-earnings-q3-2021.html) to go with it. It became a household name in IT and tech. This year looks to continue that trend, with [Databricks raising $1B in funding at a $28B valuation](https://www.cnbc.com/2020/12/02/snowflake-snow-earnings-q3-2021.html) already. CDWs have become a huge, growing business.\n\nA cohort of applications built on top of CDWs has risen from this huge, growing business. This trend was included as one of the most impactful ideas of 2020 in [Kleiner Perkins’ A 2020 Perspective](https://www.kleinerperkins.com/perspectives/a-2020-perspective/). Applications such as [Observe](https://www.observeinc.com/blog/welcome-to-observe/) use Snowflake to process all of their data and as their central data store, driving down the cost of storage to “[little more than the cost of Amazon S3](https://www.observeinc.com/blog/welcome-to-observe/).”\n\nIn part one of this two-part series, we’ll define warehouse-first architecture and its benefits. Next month our Friends at Message Gears will publish part two, diving deeper into warehouse-first applications.\n\n## What is the Warehouse-First Architecture?\n\nBuilding on top of Snowflake is certainly something we will see more of, but a new crop of modern cloud companies is taking it a step further and enabling _their customers_ to build on top of _their own_ CDWs and data lakes. \n\nAt Rudderstack, we take the warehouse-first approach, building your customer data lake on your data warehouse, but the architecture’s value applies across the data stack. For example, our friends at [Panther](https://runpanther.io/), who build SIEM tooling for cloud-focused security teams, use the warehouse-first approach in the security space. On the other end of the spectrum, [MessageGears](https://messagegears.com/) is a customer marketing platform that is warehouse-first, running on their customers’ CDWs. \n\nThis is the warehouse-first architecture: instead of providing (and charging) for storage infrastructure and running features in black boxes, warehouse-first tools are architected to build the data lake in the _user’s_ CDW. \n\n## What are the Benefits of Warehouse-First Software?\n\nWarehouse-first may seem like a minor difference in the way applications operate compared to the traditional model where SaaS providers store their customers’ data in their own (often proprietary) databases, but the implications are huge. \n\n### Improved Data Control\n\nThe data lake that warehouse-first applications build and operate on top of is stored in their customers’ data warehouses. So, if you use warehouse-first tools, you don’t have to rely on the vendor to protect your sensitive data. It’s in your data warehouse, and you have control of its security and privacy. \n\nAdditionally, you don’t have to deal with black boxes that use your data in ways you can’t access. Identity resolution is a great example: 3rd-party vendors make assumptions about how to resolve identities using a copy of your data that they store. If your use case doesn’t match their assumptions, you won’t get as much value out of the tool. With warehouse-first tools, though, you can see _and modify_ functionality around things like the identity graph because it lives on your owned CDW. \n\n### Increased Flexibility with no Duplicated Data\n\nMany customers want the flexibility to use the data from data-intensive applications for analysis and activation in other tools. This flexibility isn’t possible with traditional vendors because most don’t allow direct access to their data lake. So if you want to perform analysis on your data or enrich it and use it for activation in another tool, you have to export it to your data warehouse first. This data duplication is expensive, inefficient, and unnecessary.\n\nWarehouse-first applications give you this flexibility without requiring you to duplicate your data. The data lakes these applications build are already in your warehouse. So you can analyze your data, enrich it, and connect it to other tools without any data duplication.\n\nFrom a pipeline management standpoint, you don’t have to create and manage infrastructure whose only purpose is moving data from a proprietary vendor database to your CDW—it’s already there and ready to use. \n\n### Lower Costs\n\nSince warehouse-first applications don’t store their customer’s data, they can’t charge for it. This has resulted in significantly lower pricing compared to traditional vendors.\n\nYou have to pay a bill to your CDW; however, as your data is in your warehouse, you know how much you’re storing and its costs. This means you can make your own decisions on data retention and more accurately forecast expenses.\n\nThe resulting cost savings are substantial. At RudderStack, customers frequently tell us they save up to 66% when compared to Segment!\n\n## Choose Warehouse-First Applications\n\nData silos and inflated bills from vendors of data-intensive applications are an artifact of design decisions made before CDWs were a viable option for their data lakes. Engineering teams had to build their own data stores, but that isn’t the case anymore. The capabilities and low cost of CDWs make them the perfect foundation to build modern applications.\n\nThat’s why you should search for and choose warehouse-first applications. It’s a modern design decision based on modern technology that makes applications more secure, flexible, and cost-effective.\n\nCheck back next month for more on warehouse-first architecture from our friends at [MessageGears](https://messagegears.com/). They’ll take the baton and elaborate on the results you can drive by adopting warehouse-first applications.\n\n## Try RudderStack Today\n\nStart using a smarter customer data pipeline that builds your customer data lake on your data warehouse. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack. Sign up for [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial) today.\n\nJoin us on [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, explore our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on our socials: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw). Don't miss out on any updates. [Subscribe](https://rudderstack.com/blog/) to our blogs today!"
    },
    {
        "instruction": "Explain RudderStack's licensing model, detailing how the company balances open-source contributions with commercial interests through different licenses for various software components. Include explanations of AGPL-3.0 and MIT licenses used for open source parts and describe what features are available under the enterprise license. Clarify why certain decisions were made regarding licensing to ensure customers understand the structure and rationale behind RudderStack's licensing approach.",
        "input": "",
        "output": "Software licensing can be a tricky subject, especially when you are a commercial company building an open-source product like RudderStack.\n\nWe want to build RudderStack into a strong company, and we keep some of our features under our enterprise license to help with that. We also want to build the best product and distribute it in the most effective fashion, and that is under an open source license. We have to balance which parts of our software should be under our enterprise license and which should be under open source licenses.\n\nFrom a customer’s viewpoint, we see how this could be confusing. In this post, we’ll explain how our software is licensed.\n\n## Open Source\n\n### [AGPL-3.0](https://www.gnu.org/licenses/agpl-3.0.en.html)\n\nThe core of RudderStack - the components that make up our Event Stream feature - is open source. It is in our [rudder-server](https://github.com/rudderlabs/rudder-server) repository.\n\nWe are the only open source customer event streaming software available (at least that we’ve been able to find). We have more than 2,000 stars on GitHub and approximately 300 open source users.\n\nA simplified explanation of the AGPL-3.0 license is that you can use and modify rudder-server however you want, but if you distribute it in any way - including offering it as SaaS - you have to make all of the source code for the derivative software available under the AGPL-3.0 license.\n\nThe AGPL-3.0 license provides us the most protection against competition while still being open source. That’s the reason we have our core functionality, **rudder-server**, under it.\n\n### [The MIT License](https://opensource.org/licenses/MIT)\n\nA majority of RudderStack’s third-party destination integrations live in the [rudder-transformer](https://github.com/rudderlabs/rudder-transformer) repository. They are open source as well, licensed under the MIT License.\n\nAlmost all of our SDKs and instrumentation repos for tooling and utilities are open source too. We also have some of our data modeling repos - for use-cases like customer journey analysis and sessionization for data residing in your data warehouse - open sourced under the MIT license. You can find all these repos in our[ GitHub organization](https://github.com/rudderlabs).\n\nA simplified explanation of the MIT License is that you can do whatever you want with the source code - including distributing and commercializing it - as long as you include the original copyright and license in any copy of the derivative software that you distribute.\n\nThe MIT License provides us very little protection from competition, but it allows a lot of flexibility and freedom for the developers using our code. That’s the reason we have our non-core functionality - things that _would not_ make sense for us to distribute or commercialize on their own - under it.\n\n**Note: We have a few repositories that are forked from other repos and are under their licenses. Most are under the [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) license, but there are a few under various other licenses based on where they are forked from.**\n\n## Enterprise License\n\nAll of our Pro and Enterprise features are under our enterprise license. The source code for this software isn’t publicly available and is a closed source, but any of our Enterprise customers that request it are given access to these repositories. So, even though our enterprise license is a closed source license, the repositories are code available for all our Enterprise customers.\n\nFeatures licensed under our enterprise license include:\n\n*   Warehouse Actions\n*   Cloud Extract\n*   Transformations\n*   Event Replay\n*   SSO (Single Sign-On)\n\nA simplified explanation of our enterprise license is that you can only use the software under it as we specify. You can only use it in your RudderStack instance and only if you are an Enterprise customer. If you build anything on top of it or spin up your own instance, you cannot distribute or commercialize it and are not allowed to use it if you aren’t an Enterprise customer.\n\nOur enterprise license provides us the highest level of protection from competition, but we try our best to keep our software transparent and the source code easy to access by making it source available to Enterprise customers. That’s the reason we have our core, differentiated functionality - things that _would_ make a lot of sense to distribute or commercialize on their own - under it.\n\n## Licensing Can be Confusing. We Hope This Helped.\n\nHopefully, this post helps clarify RudderStack’s licensing. Open core products like ours frequently end up with complicated licensing that changes over time. We’re trying to avoid that by being intentional with how we license our software components and limiting the number of licenses we use, so customers and contributors don’t have to deal with that complexity. \n\nIf better licensing options are available in the future, we will consider changing to them. As of today, this licensing configuration is the best we have found for our software and our business, and we will hopefully stick with it for a long time. \n\n## Try RudderStack Today\n\nStart using a smarter customer data pipeline that builds your customer data lake on your data warehouse. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack. Sign up for [RudderStack Cloud Free](https://app.rudderlabs.com/signup?type=freetrial) today.\n\nJoin our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on social: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw). Don't miss out on any updates. [Subscribe](https://rudderstack.com/blog/) to our blogs today!"
    },
    {
        "instruction": "Write a detailed overview of RudderStack's participation in SXSW Pitch 2021, including:\n- A brief description of what SXSW Pitch is\n- The selection process and the total number of applicants and finalists\n- Categories for this year’s competition with emphasis on RudderStack's category\n- The format of the event (pre-recorded pitch video duration and live Q&A session)\n- Event dates, including the award ceremony date\n- Your specific involvement in the pitching day\n- Encourage support from both attendees and non-attendees of SXSW Online, providing instructions for those who wish to watch or support you on Twitter",
        "input": "",
        "output": "[SXSW Pitch](https://www.sxsw.com/awards/sxsw-pitch/) (formerly SXSW Accelerator) is the marquee event of [South by Southwest Online](https://www.sxsw.com/), where leading startups from around the world showcase some of the most impressive technology innovations to a panel of hand-picked judges and a virtual audience.\n\nOut of the more than 500 companies that applied to present at SXSW Pitch 2021, RudderStack was selected as one of the 40 finalists.\n\nThere are 8 categories this year with 5 finalists in each:  Artificial Intelligence, Robotics & Voice; **Enterprise & Smart Data** _(the category we are in)_; Entertainment,Gaming & Content; Future of Work; Health, Wearables & Wellbeing; Innovative World Technologies; Smart Cities, Transportation & Logistics; and Social & Culture.\n\nThe format this year is pretty simple. A 3-minute pre-recorded pitch video and a 5-minute live-recorded Q&A with the judges.\n\n## Join us at SXSW Pitch 2021\n\nSXSW Pitch is a two-day event, presented on Wednesday, March 17th, and Thursday, March 18th, during SXSW Online. The winners for each category will be announced during the SXSW Pitch Award Ceremony on Thursday, March 18th.\n\n**I am pitching RudderStack on day 2 of the competition.** If you're attending SXSW Online this year, come watch our pitch and throw some support our way. If not, register for the conference so you can watch us pitch and support us on Twitter ([@RudderStack](https://twitter.com/RudderStack))."
    },
    {
        "instruction": "Explain how customer data pipelines impact data privacy, distinguishing between data privacy and data security. Highlight the role of RudderStack in improving data privacy through specific processes such as data masking, attribute removal, and event filtering. Include examples for each process and mention the open-source repository for Transformation templates provided by RudderStack. End with a call to action for trying RudderStack and details on how to stay connected with their community and updates.",
        "input": "",
        "output": "Customer data pipelines play a critical role in the privacy of your customer data. They are one of the primary and most expansive collectors of your customers’ personally identifiable information (PII). They are also one of the most expansive sharers of customer data - with one of the primary use cases being event streaming to frequently large libraries of destination integrations.\n\nDue to their specialized role of collecting and sharing customer data, customer data pipelines can either help ensure your data privacy or wreak havoc on it.\n\nIn this post, we’ll explain how your customer data pipeline can help improve your data privacy and how to ensure your data privacy with [RudderStack](https://rudderstack.com/).\n\n## Data Privacy vs. Data Security\n\nTo remove one common vector of confusion before we launch into this post, we want to make sure the difference between data privacy and data security is clear.\n\n**Data privacy** - the focus of this post - is about what data is collected, stored, and for how long it is retained; and what customer data is shared. _What customer data are you collecting and storing, and how are you using that data?_\n\n**Data security** is about how collected data is protected - where data is stored, who has access, whether data is encrypted, etc. _How are you keeping the customer data you store safe?_\n\n## Your Customer Data Pipeline can Improve Your Data Privacy\n\nYour customer data pipeline can give you fine-grained control over what data you are sending to which tools and what data you are storing. This can help you avoid data privacy issues before they ever occur.\n\nThe three processes below are designed to help you ensure that your customer data stays private. You can implement all of them with a robust customer data pipeline tool like RudderStack.\n\n### Data Masking\n\nData masking is taking fields in your event data and obfuscating them. This is most frequently used to hide PII. Your customer data pipeline can mask your PII before it is ever sent to a destination or stored in your warehouse.\n\nFor example, if your event payload includes the following attributes...\n\n```\n\"globalUserId\": \"XYJ458907432AAC\",\n\"userId\": \"contactUser\",\n\"userFirstName\": \"Rudder\",\n\"userLastName\": \"Stack\",\n\"userEmail\": \"contact@rudderstack.com\",\n\"userSSN\": \"123-45-6789\",\n\"eventType\": \"newsletter-sign-up”\n```\n\nOne level of data masking would remove the directly identifiable PII, like SSN and email address.\n\n```\n\"globalUserId\": \"XYJ458907432AAC\",\n\"userId\": \"contactUser\",\n\"userFirstName\": \"Rudder\",\n\"userLastName\": \"Stack\",\n\"userEmail\": \"XXXXXXX@XXXXXXXXXXX.XXX\",\n\"userSSN\": \"XXX-XX-XXXX\",\n\"eventType\": \"newsletter-sign-up”\n```\n\nAnother, more stringent level of data masking would remove all unnecessary attributes. Since most of the attributes in this payload are identifiers in one way or another, only the global identifier and event type would be unmasked.\n\n```\n\"globalUserId\": \"XYJ458907432AAC\",\n\"userId\": \"XXXXXXXXXXX\",\n\"userFirstName\": \"XXXXXX\",\n\"userLastName\": \"XXXXX\",\n\"userEmail\": \"XXXXXXX@XXXXXXXXXXX.XXX\",\n\"userSSN\": \"XXX-XX-XXXX\",\n\"eventType\": \"newsletter-sign-up”\n```\n\n### Attribute Removal\n\nSimilar to data masking, attribute removal is selectively removing attributes from your event data. Not every application you send event data to needs all of the customer data you collect in your events. Attribute removal can be used to remove PII or to remove unnecessary customer data and reduce payload.\n\nUsing the same event example, if you wanted to activate that data by having it trigger an email send in your email/marketing automation tool, you would remove the unnecessary attributes for sending an email - userId and userSSN.\n\n```\n\"globalUserId\": \"XYJ458907432AAC\",\n\"userFirstName\": \"Rudder\",\n\"userLastName\": \"Stack\",\n\"userEmail\": \"contact@rudderstack.com\",\n\"eventType\": \"newsletter-sign-up”\n```\n\n### Event Filtering\n\nNot all of the tools you event stream to need every type of event. Event filtering is the process of removing events from an event stream based on filtering criteria. This ensures that only the events you want to activate on are ever shared with the tools that you activate in. So you don’t overshare your customer data with tools that only use a small portion of it.\n\nUsing the same event example, if you filtered to where `eventType = \"newsletter-sign-up\"`, the sample event would be included. If you filtered to where `eventType != \"newsletter-sign-up\"`, the sample event would be excluded.\n\n## RudderStack Transformations Keeps Your Customer Data Private\n\nRudderStack Transformations allows you to transform your event data in-flight - after collection, before delivery. Transformations are reusable functions - written in JavaScript - that can be applied to the data in your event streams prior to delivery to a destination tool or your data warehouse.\n\nWith RudderStack Transformations, you can implement all three of the data privacy processes detailed above, plus any other type of data transformation you can code in JavaScript. Transformations are applied on a destination-by-destination basis, so you can implement specific privacy processes for each tool you use and your data warehouse - only sharing the exact customer data you need to share. And they are reusable, so it’s easy to apply the same transformation to multiple destinations. Write it once and apply it everywhere.\n\nWe maintain an [open source repository](https://github.com/rudderlabs/sample-user-transformers) of Transformations templates that implement a wide variety of data transformations - from data masking, attribute removal, and event filtering to event enrichment. The JavaScript code for individual transformations is stored in this repo. You can copy it, edit it to work with your data, and paste it into RudderStack Transformations. \n\n*   [Data masking template](https://github.com/rudderlabs/sample-user-transformers/blob/master/PIIUserTransformation.js)\n*   [Attribute removal template](https://github.com/rudderlabs/sample-user-transformers/blob/master/CleanWarehouseEventsUserTransformation.js)\n*   [Event filtering template](https://github.com/rudderlabs/sample-user-transformers/blob/master/EmailDomainBasedFilteringNameSplitCampaignParameterUpdate.js)\n\n_If you want more details about how to use RudderStack Transformations, read our [step-by-step guide on adding custom Transformations](https://docs.rudderstack.com/adding-a-new-user-transformation-in-rudderstack)._\n\n_If you want more details about how to mask PII with RudderStack Transformations, read our blog post [Protect Personally Identifiable Information (PII) in Your Apps Using RudderStack](https://rudderstack.com/blog/protect-personally-identifiable-information-pii-using-rudderstack)._\n\n## Try RudderStack Today\n\nStart using a smarter customer data pipeline that builds your customer data lake on your data warehouse. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack. Sign up for RudderStack Cloud Free today.\n\nJoin our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on social: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw). Don't miss out on any updates. [Subscribe](https://rudderstack.com/blog/) to our blogs today!"
    },
    {
        "instruction": "Write a press release announcing the launch of RudderStack Pro self-serve plans, emphasizing the ease of purchasing with just a credit card and avoiding complex invoicing processes or mandatory sales calls. Include details on the pricing structure for different event volumes (25M, 50M, 100M events per month), payment options, how to upgrade from RudderStack Free to Pro, and key benefits such as access to a smarter customer data pipeline. Additionally, mention the recent product redesign and include links to relevant blog posts, pricing pages, demo requests, social media handles, and other resources for more engagement. Keep the tone informative yet inviting, suitable for potential customers and existing users alike.",
        "input": "",
        "output": "## Upgrade to RudderStack Pro with just a credit card. No more complicated invoicing processes or mandatory sales calls.\n\nToday, we are announcing the availability of our RudderStack Pro self-serve plans. Now, you can purchase RudderStack Pro, up to 100M events per month, with just a credit card. No complicated invoicing processes and no mandatory discussions with salespeople.\n\n_We also announced our product redesign along with our RudderStack Pro self-serve plans. Read about that in our blog post- [Good Products Deserve Good Design: Introducing the Redesigned RudderStack](https://rudderstack.com/blog/good-products-deserve-good-design-introducing-the-redesigned-rudderstack)._\n\n## Our Self-Serve Plans\n\nAll of our self-serve plans give you access to RudderStack Pro (you can learn more about how our Pro tier compares to our Free and Enterprise tiers on [our Pricing page](https://rudderstack.com/pricing/)). We offer three plans based on monthly event volume- monthly, pay-as-you-go, and reduced annual pricing.\n\n| **Event Volume**      | **Monthly Price** | **Annual Price**  \n_20% discount_ |\n| --------------------- | ----------------- | ------------------------------------ |\n| Up to 25M events/mth  | $900              | $9,000                               |\n| Up to 50M events/mth  | $1,200            | $12,000                              |\n| Up to 100M events/mth | $1,800            | $18,000                              |\n\nIf you want access to RudderStack Enterprise or your event volumes are greater than 100m events per month, contact our sales team - by either [requesting a demo](https://resources.rudderstack.com/request-a-demo) or sending us a message in our application like in the image below.\n\n## How to Upgrade from RudderStack Free to a RudderStack Pro Self-Serve Plan\n\n1. Login to [RudderStack Cloud](https://app.rudderstack.com/login). If you don’t have an account, [sign up for free](https://app.rudderlabs.com/signup?type=freetrial).\n2. Click on **Upgrade** in the left navigation menu:\n\n    \n\n3. Click on the **Upgrade Plan** button:\n\n    \n\n4. This takes you to our Billing page. Here you can add a payment method and update your billing information.\n\n    \n\n5. Click on the **Update Plan** button.\n6. Select the monthly or yearly plan for your desired monthly event volume and click the **Continue** button.\n\n    \n\n7. Click the **Confirm** button to finalize your new RudderStack Pro plan.\n\n    \n\n## Try RudderStack Today\n\nIf you haven’t already, sign up for [RudderStack Free](https://app.rudderlabs.com/signup?type=freetrial) today. Start using a smarter customer data pipeline that builds your customer data lake on your data warehouse. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack.\n\nJoin us on [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, explore our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on our socials: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw). Don’t miss out on any updates. [Subscribe](https://rudderstack.com/blog/) to our blogs today!"
    },
    {
        "instruction": "Draft a product update blog post for RudderStack that covers new features, enhancements, bug fixes, and integrations released in the last quarter. Include detailed sections on each major feature such as Transformations API, Warehouse Syncs, and Audit Logs with descriptions of their functionalities. Highlight any improvements to user experience or performance. Also, list out new Event Stream Sources and Cloud Extract Sources added for integration. Conclude with a call-to-action encouraging readers to sign up for RudderStack Free and engage with the community on social media platforms and Slack. Use an informative and engaging tone suitable for technical users and decision-makers in data management.",
        "input": "",
        "output": "Our engineering teams continually build and release new features. We tell you, our users, about various functionality we build in our launch blogs. Every feature isn’t big enough for its own launch, though. To keep you up-to-date on everything that doesn’t fit into a launch, we will be writing regular, recurring product roundups to cover the features, fixes, and integrations that our engineers build.\n\n## New Features, Enhancements, and Bug Fixes\n\n### Transformations API\n\nUsers love our Transformations. It is one of our most popular features. Its API wasn't accessible until now, though. We released improvements to the Transformations API and exposed it to users, programmatically interacting with the Transformations functionality.\n\nWith the Transformations API, you can:\n\n*   Programmatically add and remove transformations to the platform.\n\n    **Note:** New transformations are always checked for successful compilation and execution.\n\n*   Create an organization-wide sandbox where your team can store transformations before publishing them to the live environment.\n*   Define Libraries - collections of transformations that can be re-used in RudderStack.\n*   Version control your transformations - every change in a transformation creates a new version, and the whole lineage of a single transformation is available.\n\nAlso included with this release is a selection of useful and popular transformation libraries, exposed under the name RudderStack Libraries in the Transformations UI.\n\n### Warehouse Syncs\n\nWe released a new feature called **Warehouses Syncs**. It gives you observability for the pipelines that load data into your warehouse.\n\nWith Warehouse Syncs, you can:\n*   See a chronologically ordered list of all syncs to all your data warehouse destinations in RudderStack.\n*   Find important metadata, like the total time a sync took, if the sync was successful or not, what the source was, etc.\n*   Search syncs based on various criteria.\n*   Get detailed information about each sync by clicking on the sync in the UI, e.g., details on the table data synced to, error reports, etc.\n\n### Audit Logs\n\nSecurity and compliance around data and its infrastructure are becoming increasingly important for our users. That’s why we released the first version of **Audit Logs**.\n\nThe Audit Log is a log of all the actions every user has taken in RudderStack. Actions that are tracked include:\n*   CRUD operations for every object in RudderStack (e.g., Sources, Destinations, and connections).\n*   Invitations and user management events.\n*   Logins and password operations.\n\n### Other Features and Bug Fixes\n\nIn addition to the above features, we also released the following minor features, enhancements, or bug fixes.\n*   Improved performance for loading data to warehouses.\n*   Improvements to the Data Governance API.\n*   Improved sign-up and login flows, including Google authentication.\n*   Updated UX for many user journeys.\n*   Other bug fixes.\n\n## New integrations\n\n### Event Stream Sources\n*   [Appcues](https://rudderstack.com/integration/appcues/)\n*   [Userlist](https://rudderstack.com/integration/userlist/)\n*   [Microsoft App Center](https://rudderstack.com/integration/app-center/)\n*   [ActiveCampaign](https://rudderstack.com/integration/activecampaign/)\n*   [Klaviyo](https://rudderstack.com/integration/klaviyo/) \n*   [Salesforce Marketing Cloud](https://rudderstack.com/integration/salesforce-marketing-cloud/)\n*   [CleverTap](https://rudderstack.com/integration/clevertap/)\n*   Kustomer\n\n### Cloud Extract Sources\n*   [Zendesk](https://rudderstack.com/integration/zendesk-source/) → Support Data\n*   Pardot → Marketing Data\n*   Stripe → Financial Data\n*   Improved usability for sources, e.g., now it's possible to define prefixes for tables.\n*   Bug fixes for various sources.\n\n## Try RudderStack Today\n\nIf you haven’t already, sign up for [RudderStack Free](https://app.rudderlabs.com/signup?type=freetrial) today. Start using a smarter customer data pipeline that builds your customer data lake on your data warehouse. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack.\n\nJoin us on [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, explore our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on our socials: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw). Don’t miss out on any updates. [Subscribe](https://rudderstack.com/blog/) to our blogs today!"
    },
    {
        "instruction": "Write a detailed article on the future of data pipeline tools, emphasizing the need for better transformations than ETL ever had. Include sections on current limitations of ETL, emerging transformation technologies, and how they can improve data pipelines. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific tools or strategies discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in data pipeline advancements.",
        "input": "",
        "output": "Everybody hates data transformations in their pipeline tools. Developers hated the transformations in ETL tools so much that they came up with a whole new tool classification – ELT – that removed transformations completely, leaving that work to specialized tools such as [dbt](https://www.getdbt.com/) to do on the data warehouse.\n\nIt’s easy to understand why. Data pipeline tools have always made transformations difficult to use. Send it to the warehouse and transform it there. It’s easier. You only have to know SQL, and those scripts run way faster.\n\nWell… I’m calling BS on this commonly held belief. While it's not wrong, the problem with it is that no data pipeline tool has implemented data transformations well.\n\nThat’s not true anymore. We have.\n\nRudderStack Transformations are easy to build, debug, and manage; and they run in real-time. They’re data transformations that work like you think they should.\n\n## The Problems with Data Transformations in Every Data Pipeline Tool You’ve Ever Used\n\n### Difficult to Build and Manage\n\nWhen you’re transforming data there is always going to be some level of technical complexity. It almost always takes programming knowledge to build data transformations in pipeline tools. Yet these tools always seem to have some level of disdain for the data engineers that use them, and that is where things get complex.\n\nAt this point, most of these tools support reusing data transformations, which is good, but reuse is the bare minimum.\n\nCan I reuse this transformation? Yes. Great, can I reuse it in this other transformation to perform a more complex transformation???\n\nData pipeline tools don’t support common software engineering concepts such as modularity. This makes a data engineer’s job more complex. You end up having to copy and paste code from one transformation to another to replicate existing functionality as part of a new but different transformation. Then you have to manually maintain it forever.\n\nIt’s difficult to integrate building transformations into your engineering processes too. The code for your transformations lives in the data pipeline tool, and, while you can likely version your transformations at this point, only a few tools have git integration. Even more rare is a data pipeline tool having an API that lets you build and manage your transformations with all your preferred processes and tools.\n\nYuck. Give me dbt instead.\n\n### Difficult to Debug\n\nDebugging data transformations is a nightmare too. Having a debugger on the code is great and most do at this point, but that’s the bare minimum once again.\n\nCan my code run? Yes. Great, does it produce the expected results???\n\nThe solution that data pipeline tools have come up with for this is... nothing. Run data through it, and see what comes out the other side. Some have a data debugger, but you still have to run data through it.\n\nNobody in the history of time has ever thought doing this testing on Production was a good idea (or, if they did, they regretted it). So what do you do? Build a test environment... for your ETL tool?\n\nHard pass.\n\n### Transformed Data is More Valuable in Real-Time\n\nData transformations in pipeline tools always run in batches. This is an artifact of the tools they are a part of – ETL tools – which run jobs in batch and apply the transformations when the job runs.\n\nThe stereotype of traditional ETL tools is that you’d have to wait until tomorrow morning or next week to get updated data. These waiting periods are the frequency each batch runs, and they aren’t anywhere near as bad as they used to be. Most of the major ETL tools are configurable and have trimmed it down to 5 minutes or so at the shortest.\n\nThat’s much improved, but the standard isn’t 5 minutes – that’s the best and most expensive option – and 5 minutes isn’t close to real-time. A lot of the data you want to integrate and transform in-transit is more valuable when delivered in real-time.\n\n## There are Good Reasons why You’d Want to Transform Data Before Sending it to a Destination\n\nGiven the deficiencies, the mantra of ELT seems like a plain one to align with. Your data pipeline is for extracting and loading data. Use specific data transformation tools to transform your data in your warehouse. It works better.\n\nELT assumes something that isn’t true in data pipelines anymore. ELT assumes the destination is the data warehouse. So, if you don’t apply a transformation before loading, no biggie. Apply it after. It makes no real difference.\n\nModern data stacks are more complex than that though. You want to send your data to your warehouse, but you also have 20 other tools you want to send it to as well. Each of them has its own transformation needs, and applying transformations after the fact often isn’t realistic or possible.\n\nWhile you should do most modeling and heavy transformation at-rest in your warehouse with tools like dbt, in-transit transformations are still critical to solving the problems modern companies face.\n\nOne classic use case that benefits from in-transit transformations is filtering and enforcing data quality before sending to your warehouse. Performing this transformation in-transit means the data in your warehouse is always analysis-ready. If done at-rest, the fidelity of your analysis-ready data will only be as often as your transformation runs.\n\nAnother, more modern use case that benefits from in-transit transformations is masking or removing PII and sensitive data. It’s better if this kind of data is never loaded anywhere it isn’t needed, including your warehouse. While you can mask/remove PII after loading data into your warehouse, you probably can’t after loading into your 20 other destination tools. You need in-transit transformations to support this very common use case.\n\n## Transforming Data Before Sending it to a Destination Tool in Real-Time is Incredibly Valuable\n\nWhen you step outside of the batch world of ETL and ELT pipelines and into the real-time world of event streaming pipelines, the ability to transform data before loading in a destination tool becomes much more critical.\n\nSending real-time events is incredibly valuable, because you can start to implement behavior-based tactics by sending this data to tools that use it to automate execution – think sending an email through [Customer.io](https://customer.io/) or personalizing a user’s experience through [Braze](https://www.braze.com/), both based on triggering actions in your application. These behavior-based tactics can have a very high ROI when done well.\n\nYou can’t send a raw event stream to every destination tool that you use though. Your data quality would suffer. Also, you probably want to enrich your events – such as adding info from Clearbit or location data.\n\nMore importantly, you can’t send the PII and sensitive data in your event stream to all your destination tools. That is a data privacy nightmare. You should be masking or removing as much PII and sensitive data as possible before it ever reaches a destination tool. This is the safest approach for your customers and your business. For businesses in highly regulated industries, the feasibility of event streaming will hinge on this.\n\n## RudderStack Transformations. Data Transformations that Work Like you Think They Should.\n\nRudderStack Transformations let you transform any data that you send through RudderStack and you can use them across Event Stream, Warehouse Actions, and Cloud Extract pipelines. They are easy to build, debug, and manage; and they run in real-time, after data collection and before delivery to a destination. They’re data transformations that work like you think they should.\n\n**RudderStack Transformations are easy to write and maintain.** You write them in Javascript, which is a common and easy-to-use language. Every transformation is reusable. So if you write it once, you can use it everywhere. Transformations are modular too. You can save them to your organization’s Transformation Library and reuse them as part of other transformations, just like libraries in software programming (we’re even including some out-of-the-box transformation libraries in the RudderStack Library).\n\nYou can even build transformations using all the processes and tools you prefer with the Transformations API. With it, you can programmatically add and remove transformations to the platform, create an organization-wide sandbox where your team can store transformations before publishing them to the live environment, define libraries, and version control your transformations.\n\n**RudderStack Transformations makes it easy to debug your code and your data.**\n\nNot only do we give you a code debugger so you can confirm your transformation will run, but we have a transformation test as well. It lets you send a sample payload through the transformation and see the results inside of RudderStack before ever applying it to your data pipelines. We provide sample event stream payloads you can use for testing your transformations, but you can change the payload to replicate the data you will be transforming.\n\n**RudderStack Transformations run in real-time.** They aren’t batch. They are applied in-transit after data collection and before delivery to a destination. So they work with our event streaming and reverse-ETL pipelines in addition to our ELT pipelines.\n\n_Learn more about all of the new functionality we added to Transformations in our post, [Introducing New Functionality to RudderStack Transformations: API, Libraries, and Debugger](https://rudderstack.com/blog/introducing-new-functionality-to-rudderstack-transformations-api-library-and-debugger)._\n\n_Learn more about the Transformations API in our post, [RudderStack’s Transformations API](https://rudderstack.com/blog/rudderstacks-transformations-api)._\n\n## Try RudderStack Today\n\nStart using a smarter customer data pipeline that builds your customer data lake on your data warehouse. Use all your customer data. Answer more difficult questions. Send insights to your whole customer data stack. Sign up for RudderStack Cloud Free today.\n\nJoin our [Slack](https://resources.rudderstack.com/join-rudderstack-slack) to chat with our team, check out our open source repos on [GitHub](https://github.com/rudderlabs), subscribe to [our blog](https://rudderstack.com/blog/), and follow us on social: [Twitter](https://twitter.com/RudderStack), [LinkedIn](https://www.linkedin.com/company/rudderlabs/), [dev.to](https://dev.to/rudderstack), [Medium](https://rudderstack.medium.com/), [YouTube](https://www.youtube.com/channel/UCgV-B77bV_-LOmKYHw8jvBw). Don’t miss out on any updates. [Subscribe](https://rudderstack.com/blog/) to our blogs today!"
    },
    {
        "instruction": "Explain how data engineering can improve a company's data management by reducing the use of spreadsheets and manual processes. Discuss the role of automation in data collection, sanitization, cleansing, warehousing, system integrations, and reverse-ETL. Highlight the benefits of standardizing tools used by the data team for efficiency and consistency. Mention RudderStack as a comprehensive solution for customer data pipelines including event streaming, ELT/ETL, and reverse-ETL.",
        "input": "",
        "output": "Every task that your company performs that involves someone on your data team sending a spreadsheet full of data is an opportunity for improvement. Not only does having multiple people handling your data increase the likelihood of incorrect data being used, but it also makes your data less secure. Also, everyone applies their own layers of transformations, making the data less consistent.\n\nData engineering can solve these problems for you.\n\nYour data engineers build pipelines that collect, share, and store your data. They help automate the data-driven processes that your company executes, applying programmatic execution and removing human delays and human error from the equation as much as possible. Data engineers remove emails, spreadsheets, and manual processing from your data pipelines. This frees teams across your company to innovate — giving them a stable, data-backed foundation to execute and experiment from.\n\n## Data Engineering Lends Itself to Automation\n\nThe primary responsibility of data engineers is to provide accurate, high-quality, and complete data to all of your teams, in a way that is easy to access. In this post, we’ll cover the benefits of automating tasks in six key areas:\n\n* Data collection\n* Data sanitization\n* Data cleansing\n* Data warehousing\n* System integrations\n* Reverse-ETL — data movement out of the warehouse and into applications\n\nHere’s a block diagram showing the correlation between the six key areas of data engineering:\n\n### Data Collection\n\nThe indicator of successful data collection is the accuracy and consistency of your data. Suppose that your data is inconsistent, incorrect, or you’re missing expected data in your data sets. In those cases, data scientists and analysts have difficulty working with it. Other applications may have trouble activating it, and it becomes less useful for AI/ML use cases.\n\nThe more standardized and automated your data collection process, the more consistent and accurate your data will be. Automation ensures that event data is delivered consistently and that ETL pipelines are successful and run on a reliable schedule. The dream scenario for any data engineer is building a data pipeline, validating it, and never having to touch it again because it’s standardized, automated, and fault-tolerant.\n\n### Data Sanitization\n\nData sanitization is the process of removing private or sensitive information from your data. You can sanitize data during the collection process, pre-delivery or post-delivery. In either case, the indicator of successful data sanitization is accuracy and consistency — e.g., is it being applied to the correct fields, and is it being applied across all records?\n\nThe more standardized and automated your data sanitization process, the better your data privacy and the lower your potential for exposing sensitive data. Automation ensures that you aren’t storing sensitive data unless necessary and, when necessary, only the appropriate people have access to it.\n\n### Data Cleansing\n\nAlong with data sanitization, data cleansing is the most time-consuming process in data engineering. Data sanitization and cleansing require inspecting every record and making conditional updates; and that requires a lot of compute time, regardless if performed by a human or computer.\n\nData cleansing is removing or correcting bad data or unnecessary data. To make your data easier to work with and more accurate, you will frequently delete or update inaccurate data or incomplete or inconsistent data with your data set. Data cleansing processes need to be accurate and consistent — applied evenly across your data and only touching bad data.\n\nThe more standardized and automated your data cleansing process, the higher quality and more usable your data will be. Automation ensures that you are only storing good data.\n\n### Data Warehousing\n\nData warehousing is storing your data in your data warehouse, to be accessible for analysis and BI/ML use cases. Your data warehouse is often your most used data destination. The need for consistency and accuracy applies to the data warehousing process.\n\nThe more standardized and automated your data warehousing process, the more complete and useful your data set will be. Automation ensures that your data is delivered and stored consistently — in a logical format and schema, and on the schedule that you expect.\n\n### Data Integration\n\nData integration is connecting data between systems. From a technical standpoint, data integrations work similarly to data warehousing: you connect data from a source to a destination, just like you do with a data warehouse. But data integrations are typically used less for storage and more for activation, which requires them to deliver data much closer to real-time than with data warehousing. Also, there are a ton of different tools you may want to integrate with, but there is usually only one data warehouse you integrate with. So the difficulty and importance of standardization and automation are both greater for data integrations. Consistent, accurate data across your systems makes teams more cohesive and enables them to act on the most up-to-date data.\n\nThe more standardized and automated your data integration process, the less work your data engineers will have to do to maintain your pipelines, the more quickly you can experiment with new tools, and the more flexible and creative your teams can be. Automation ensures that all of your tools and teams are working with the same, high-quality data.\n\n### Reverse-ETL\n\nReverse-ETL is the new kid on the block for data engineers, but it’s just an extension of data integration that relies on data warehousing. Reverse-ETL is sending data stored in your data warehouse out to the tools you use. Data warehouses have traditionally been destination-only, so flipping the data flow and turning them into a source for data integrations is novel and can help solve many difficult use cases. Like every other process that a data engineer does, consistency and accuracy are extremely important for reverse-ETL.\n\nStandardizing and automating the reverse-ETL process has the same benefits as data integration. However, the data coming from the reverse-ETL process is more valuable to your business than normal event stream or ELT/ETL data. Data sent via reverse-ETL is typically post-analysis or post-modeling and contains insights you want to activate. So any faults in your reverse-ETL process have amplified negative impacts.\n\n## Your Data Engineers Should Standardize the Tools They Use Too\n\nData engineers have long used a variety of tools to help with these tasks. They have ETL tools, ELT tools, event streaming tools, and now reverse-ETL tools at their disposal. These tools make the responsibilities of a data engineer possible and also make standardizing and automating these tasks easy. Just as your data engineers want to standardize and automate the tasks they execute, your data team should standardize the tools they use too.\n\nAllowing your data team to use whatever tools they want might seem liberating, but it’s not efficient. Licensing will be more complex and expensive than necessary, your data will be in inconsistent schemas, and you may end up rebuilding full pieces of functionality for each tool (for things like data sanitization and data cleansing).\n\nConsolidating your data teams on a best-of-breed tool for each type of data flow makes a lot of sense. One tool for event streaming, one tool for ELT/ETL, and one tool for reverse-ELT. This approach is more standardized, but it’s still not optimal. You’ll pay three license fees, your data schemas will still differ across tools, and you may still end up re-building full pieces of functionality for each tool — but at least you’re limited to three tools.\n\nThe best approach is to unify your teams on a single, complete solution. You only pay one licensing fee, your data schemas will be similar across stores, and you can better reuse the functionality you’ve built. Moreover, your data team only has to learn one pipeline tool; and they know exactly where to go if they are having problems with any of their data pipelines.\n\nRudderStack is the single, complete solution for your data engineers to build all of the customer data pipelines your company needs — event streaming, ELT/ETL, and reverse-ETL. It allows you to easily build pipelines connecting your whole customer data stack. Sign up for RudderStack Cloud Free today."
    },
    {
        "instruction": "Write a detailed narrative about Gavin Johnson's journey from being unexpectedly jobless to joining Yugabyte as a product marketer. Include key milestones in his career, such as roles at RudderStack and New Relic, and highlight the reasons why he chose Yugabyte over other opportunities like Twilio-Segment. Discuss his background, technical skills, and passion for open source software. Also, emphasize the qualities of Yugabyte's team, market opportunity, and product that attracted him to the role. Conclude with a personal reflection on what ultimately led him to make this career choice and include a call-to-action for readers to learn more about Yugabyte Cloud.",
        "input": "",
        "output": "Hi. I’m Gavin. I joined Yugabyte on May 24, 2021, as the product marketer for Yugabyte Cloud.\n\nOn April 20, 2021, I found myself in a new situation for the first time in my working life:  I unexpectedly had no job. I’ve been constantly working since 2 weeks after my 16th birthday. I’ve spent my career trying my best to do good work and follow [Wheaton’s Law](https://knowyourmeme.com/memes/wheatons-law). I’ve been fortunate enough to build a network of people that I know, trust, and like across consulting, media & entertainment, and tech. When [I shared that I was unexpectedly looking for work](https://www.linkedin.com/posts/gavin-johnson_opentowork-activity-6790341474975383553-0NlC), the response was big. I spoke with a ton of companies, founders, and friends. I had a lot of options. I ended up choosing to come to Yugabyte. Here’s why.\n\n## MY BACKGROUND\n\nPreviously, I led product marketing for RudderStack, the open source alternative to Segment. I launched new products and features, wrote thought leadership and produced content – blog posts, technical walkthroughs, videos, whatever we needed – and sponsored several podcasts and a few newsletters.\n\nI came to RudderStack from New Relic, where I was the solutions marketer for their Digital Customer Experience solution and then, more notably, the product marketer for everything open source at New Relic – launching their [open source website](https://newrelic.com/blog/nerd-life/announcing-new-relic-open-source) and their [open source agents](https://newrelic.com/blog/best-practices/introducing-open-source-agents-and-projects).\n\nBefore that, I was a product marketer at AT&T for their video product portfolio – primarily DirecTV Now and AT&T TV. I was a senior consultant and then manager at Deloitte Consulting – leading the functional and architectural design, build, and delivery of custom IT systems. Going back to before business school, I was an IT system administrator and developer for a couple of medium-sized businesses.\n\nI have a Bachelor’s in Computer Science from Oregon State University and an MBA from the University of Southern California.\n\nAt this point in my career, I’m an experienced marketer who is very technical. I don’t code every day, but I still code. Instead, I spend my time writing and working on developer-focused marketing. I like building technical content, and I like sponsoring people that make content that the developer-side of me loves.\n\n## HOW I JOINED YUGABYTE\n\nWhen I posted on LinkedIn that I was looking for work, the response was big. There must be a lot of demand for tech product marketers right now. I received over 50 messages for different product marketing roles within a week, a minority of those from recruiters. I had a ton of direct emails too. It was overwhelming. I didn’t end up applying for a single job. By the end of my first day of unemployment, a Tuesday, I had 8 interviews booked through the next 3 days.\n\nI ended up having 41 Zoom meetings or phone calls about potential jobs between April 21 and May 14. I spoke with 11 different companies and had advanced conversations – e.g. did a project for and/or met with the extended team – with 4 of them. I received 2 job offers at very comparable role levels and compensation, one from Yugabyte and one from Twilio-Segment. I chose Yugabyte.\n\nI’d never heard of Yugabyte before I started looking for work. I didn’t even speak with Yugabyte until a full 2 weeks into my job search. Why would I choose to go to Yugabyte over a name-brand titan like Twilio?\n\n## THE REASONS I JOINED YUGABYTE\n\n### YUGABYTE’S PRODUCT IS REALLY GOOD\n\nI am very familiar with databases. I was the database administrator for the systems I managed when I worked in IT. I’ve built resilient systems (well, at least old-school resilient) with Microsoft SQL Server. I’ve personally built applications with MSSQL, PostgreSQL, and MySQL as the application databases.\n\nI’m also very familiar with cloud native architectures and microservices, and IMO the most difficult part of building a resilient cloud native application is the datastore. YugabyteDB solves that problem better than any database I’ve come across. It isn’t just me thinking this either. Several large companies, like Kroger, have chosen YugabyteDB for major applications.\n\n### YUGABYTE’S OPPORTUNITY IS HUGE\n\nThe cloud database and DBaaS market is already big, and it’s expected to get bigger. Depending on which analyst projection you believe, it is expected to be between $28B and $39B by 2026, with a CAGR between 15% and 31%. The market size and growth look very similar to the cloud data warehouse market but trailing it a few years. If we can execute well, Yugabyte – having the best offering on the market IMO – could conceivably follow a similar path to Snowflake. That is a massive opportunity.\n\n### YUGABYTE’S EXECUTIVE TEAM IS STELLAR\n\nThe co-founders built a lot of the data architecture at Facebook, building Cassandra and HBase. They have experience building massively scalable, open source databases at one of the most innovative companies on the planet when it comes to data. They also have years of experience building databases at Oracle. You couldn’t have come up with better co-founders.\n\nOur CEO, Bill Cook, is the former President of Pivotal and CEO of Greenplum. I respect the work Bill has done, especially his work at Pivotal. He is an impressive CEO for a startup. And he’s a nice person, easily the most normal tech CEO I’ve ever met.\n\nI trust that these people will do a good job leading a company with an opportunity as big as Yugabyte’s.\n\n### THE FRIENDLY, COLLEGIAL, CHALLENGING INTERVIEWS\n\nEveryone I talked to during the interview process with Yugabyte was intelligent. Not a single time during the whole interview process did I ever feel like the smartest person in the room. That’s awesome. I want to work with smart people. Everyone was super cool too, though. The interviews weren’t easy but they were enjoyable. Also, my manager, Suda, is great. We clicked pretty much immediately.\n\nI work better in organizations with this type of personality – friendly, collegial, and challenging.\n\n### YUGABYTE WANTS ME TO DO THE TYPE OF MARKETING I LIKE TO DO\n\nI’m helping launch Yugabyte Cloud later this year. We want every developer on the planet to use it. How do we get there? Expand awareness and build content that makes it easy to use Yugabyte. That means we need to sponsor podcasts, newsletters, websites, etc., and that we need to build technical content. That’s exactly the type of work I like doing.\n\n### YUGABYTEDB IS OPEN SOURCE\n\nThis doesn’t matter to a lot of people, even less so in marketing, but I care about open source software. It’s been a part of my life since I was in my early teens. If I have the choice, I prefer working for companies that build open source software. At a time when other open source companies are choosing to move away from true open source licensing, [YugabyteDB has actually gotten more open source](https://www.yugabyte.com/why-we-changed-yugabyte-db-licensing-to-100-open-source/). It is 100% open source under the Apache 2.0 license. This commitment to open source makes YugabyteDB more accessible and transparent.\n\n## THE REAL REASON I JOINED YUGABYTE\n\nI’m very much a person who tries to look at things holistically and build logical arguments to support or oppose my decisions. I did that, and Yugabyte looked like a good career choice. So did Twilio-Segment.\n\nHave you ever had that feeling in your gut that if you chose not to do something that you’d be making a mistake? That you’d be losing a rare opportunity? I had that. That’s the real reason I joined Yugabyte.\n\nSeeing as I am the product marketer for Yugabyte Cloud, the only appropriate way to close this post out is to have a call to action.\n\nWhether you’re building an MVP or operating at massive scale, [Yugabyte Cloud](https://www.yugabyte.com/cloud/) can make sure your application databases are always available and fast. Yugabyte Cloud scales with your product. You can add and remove nodes at will. So you’re never over-provisioned. Best of all, Yugabyte Cloud is easy to build with. It’s a drop-in replacement for Postgres.\n\nSign up for the [Yugabyte Cloud Beta](https://cloud.yugabyte.com/register). It’s PostgreSQL reimagined for a cloud-native world."
    },
    {
        "instruction": "Write a blog post that highlights the challenges of building innovative applications that meet user expectations for availability, speed, and experience. Emphasize Yugabyte's partnership with Hasura to integrate Yugabyte Cloud and Hasura Cloud, making it easier for developers to deploy highly available, scalable, geo-distributed databases with a single click. Explain how Hasura provides instant GraphQL APIs for all data, reducing the complexity of application development by addressing REST API flaws such as multiple endpoints and larger data transfers. Highlight the ease of use and scalability benefits of Yugabyte Cloud, encouraging readers to sign up for the beta and learn more through a webinar on implementing GraphQL applications with Yugabyte Cloud and Hasura Cloud. Use an informative and engaging tone to appeal to developers and operations teams.",
        "input": "",
        "output": "Building innovative applications that improve user experience and exceed expectations is hard. As users, we demand a lot. We demand that your applications always be available – who knows, we may want to order a new set of tires at 3:30 am on a Wednesday. We demand that your applications be fast wherever we are – doesn’t matter if it’s Phnom Penh or Philadelphia. And we definitely expect you to give us a good experience – your application should know what we want to do next and help us get there.\n\nAt Yugabyte, we recognize the challenges that developers and ops teams deal with in building innovative apps and services that deliver on user expectations. That’s why we have partnered with [Hasura](https://hasura.io/) to build an integration between [Yugabyte Cloud](https://cloud.yugabyte.com/) and [Hasura Cloud](https://cloud.hasura.io/). Now you can deploy a highly available, massively scalable, geo-distributed database to Yugabyte Cloud from Hasura Cloud with a single click.\n\nWe want to make building complex, global applications easier. This integration will do that for developers.\n\n## HASURA GIVES YOU INSTANT GRAPHQL APIS\n\nHasura gives you an instant GraphQL API for all your data. It is [open source](https://github.com/hasura) and available as a fully hosted service, Hasura Cloud. All you have to do is connect your database to Hasura, and it will automatically generate a GraphQL API from it for you. You can also create new tables and fields inside the Hasura console. So you don’t have to bounce back and forth between your database client and Hasura if you need to add new tables or fields.\n\nSo whether you have an existing application database or are starting from scratch with no database Hasura makes it fast and easy to deploy and run GraphQL applications.\n\nDevelopers love Hasura too. Both the [standard GraphQL engine](https://github.com/graphql/graphql-js) and [Hasura’s GraphQL engine](https://github.com/hasura/graphql-engine) are open source (Hey! [YugabyteDB](https://github.com/yugabyte/yugabyte-db) is open source too!). Both are popular projects. Yet, in 3 years less time, Hasura’s repo has been starred over 5,000 more times – 17.7 thousand for the standard engine to 22.9 for Hasura’s.\n\nGraph created with Star History. See it [here](https://star-history.t9t.io/#graphql/graphql-js&hasura/graphql-engine).\n\n## GRAPHQL IS INCREDIBLY POPULAR BECAUSE IT MAKES APPLICATION DEVELOPMENT EASIER\n\nWe all built and used REST APIs for years. They make data retrieval simple. They have flaws though, and GraphQL addresses a lot of these flaws. It makes application development easier.\n\n### YOU ONLY HAVE ONE ENDPOINT\n\nIn REST, for every handler function – for example, Create, Read, Update, and Delete functions – there is an endpoint. So for every table you are exposing through your API, you could have 4 endpoints (or more if you want to expose other handlers).\n\nIn GraphQL, you have one endpoint. You pass it a GraphQL query – think SQL but kinda written like JSON – and it routes it to the appropriate handlers. You create, read, update, and delete from this one endpoint. That means no more errors from guessing the wrong endpoints and no more errors in production due to mismatched endpoint names between environments. Those concerns are off your plate with GraphQL. You have an endpoint, and you send all your queries to it.\n\n### QUERIES ARE EASIER TO WRITE IN GRAPHQL\n\nLet’s say you have two tables, a Customer table and an Address table, with the Address table having the Customer Id as a foreign key.\n\nIf you wanted to query a customer and all their addresses, it would require two calls in REST, one to the Customer read endpoint and one to the Address read endpoint. Then you’d join the data yourself in-code.\n\nGraphQL supports this in a single query. You nest your query – put your Address query inside of your Customer query – and it will return a nested result – customer addresses will be returned as part of each customer record. This makes it easier to build join queries, even more so the more tables you’re joining across.\n\n### GRAPHQL TRANSFERS LESS DATA\n\nWith REST, when you call a read endpoint, you get all the fields back in the payload. You can’t limit it to just the fields you need. With GraphQL, you can specify exactly which fields you want back in a query. So payloads in GraphQL can be more specific and smaller than REST. Queries that join data across tables are another example of where GraphQL uses less data than REST.\n\nThis may not be an everyday concern for a lot of developers, but when you are building massively scalable applications with users spanning the planet, bandwidth isn’t always a guarantee. Optimizing your application’s data transfer is a necessity for global applications.\n\n## YUGABYTEDB + HASURA: MAKING IT EASIER THAN EVER TO BUILD HIGHLY AVAILABLE, MASSIVELY SCALABLE, GEO-DISTRIBUTED APPLICATIONS\n\nPairing YugabyteDB’s high-performance distributed SQL database with Hasura’s instant GraphQL gives you a fast and easy platform to build complex, global applications. You get all the benefits of GraphQL without any of the toil of writing a bunch of handler methods, connected to a highly available, massively scalable, geo-distributed database.\n\nThis used to be really difficult, but with the new integration between Yugabyte Cloud and Hasura Cloud, it’s single-click.\n\n## SIGN UP FOR YUGABYTE CLOUD\n\nWhether you’re building an MVP or operating at massive scale, Yugabyte Cloud can make sure your application databases are always available and fast. Yugabyte Cloud scales with your product. You can add and remove nodes at will. So you’re never over-provisioned. Best of all, Yugabyte Cloud is easy to build with. It’s a drop-in replacement for Postgres.\n\nSign up for the [Yugabyte Cloud Beta](https://cloud.yugabyte.com/register). It’s PostgreSQL reimagined for a cloud-native world.\n\n## OTHER YUGABYTE + HASURA RESOURCES\n\nYou can see an example of deploying a Hasura app on YugabyteDB in our blog post, [Deploy a Real-Time Polling App with Hasura Cloud and Yugabyte Cloud Distributed SQL](https://blog.yugabyte.com/deploy-a-real-time-polling-app-with-hasura-cloud-and-yugabyte-cloud/).\n\nAlso, I’ll be hosting a webinar on July 13th along with Nikhil Chandrappa, a software engineer here at Yugabyte, where Nikhil will walk you through implementing a GraphQL application using Yugabyte Cloud and Hasura Cloud. Register here: [Building Always-On, Scalable Applications with Yugabyte Cloud and Hasura Cloud](https://yugabyte.zoom.us/webinar/register/WN_vbw5ozELRqmLCy1X5X7_LQ)."
    },
    {
        "instruction": "Explain the concept of reverse ETL, its purpose, how it works, key vendors such as Hightouch, Census, and RudderStack, and compare them based on integrations and pricing models. Additionally, discuss the specific use cases where reverse ETL is most effective and mention alternatives for real-time data needs like Rockset. Provide a brief overview of what challenges reverse ETL solves in the context of modern business data requirements.",
        "input": "",
        "output": "Moving data between applications and warehousing data for analysis are recurring issues for app builders, data engineers, and IT teams. But we all know our businesses can benefit in significant ways if we are [smart with our data](https://venturebeat.com/2021/06/23/linux-foundation-unveils-new-permissive-license-for-open-data-collaboration/).\n\nThere are plenty of options for moving data now. Some have been around for years and have evolved, such as ETL (extract, transform, load) and custom-built integrations. Others have been spawned out of necessity, like ELT (extract, load, transform) and event streaming. Requirements and use cases for these data pipeline tools have grown more advanced and more demanding. From these ever-increasing demands, a novel (but sensible) use case has surfaced in the last few years: moving data from your data warehouse to the cloud applications your company uses. And a new category of [data](https://venturebeat.com/2021/06/25/precisely-82-of-data-executives-cite-data-quality-as-a-barrier/) pipeline has emerged to satisfy it: reverse ETL.\n\n## What is reverse ETL?\n\nReverse ETL is straightforward in function — move data from your data warehouse to your cloud applications. Reverse ETL tools synchronize data on a recurring schedule (configurable between a few minutes to 24 hours or longer) or when triggered by calling an API (application programming interface) endpoint the reverse ETL tool exposes or through integrations with tools like [Airflow](https://airflow.apache.org/) and [dbt](https://www.getdbt.com/).\n\n## What can I do with reverse ETL?\n\nReverse ETL tools let you realize a lot of the promise of data science. The complex, valuable modeling and analysis your data teams produce lives in your data warehouse. Being able to use this enriched, post-analysis data and automate keeping your business applications up to date with it makes the work your data scientists do more valuable. It also ensures their work delivers value in closer to real time compared with the manual processes that exist in most businesses today.\n\nReverse ETL tools focus on customer data and are best used for solving problems that require combining data across your websites, digital products, and any cloud applications you use. The specific use cases reverse ETL tools address best are:\n\n* Building better, more comprehensive customer profiles (sometimes called “customer 360”)\n* Creating more specific, granular audience segments\n* Scoring leads based on your unique, business-specific criteria\n* Identifying “at-risk” customers, or customers likely to churn\n* Delivering data to cloud applications for better reporting\n\n## Who are the leading reverse ETL vendors?\n\nThere are several reverse ETL tools available, and they all operate similarly. You configure a source connection to your data warehouse, configure a destination connection to a cloud application, and then write an SQL (structured query language) statement (or choose a table) to select the data you want to sync, choose your mappings, and set a sync schedule.\n\nDespite the similar functionality of reverse ETL tools, three vendors stand out:\n\n### [Hightouch](https://www.hightouch.io/)\n\nHightouch believes your data warehouse is your source of truth for customer data. The company makes it easy to sync that data to the cloud tools your business uses. Hightouch stands out because its tool is mature and has more source and destination integrations than any other [pure-play reverse ETL](https://www.hightouch.io/blog/reverse-etl) tool. The company has also grown its integration library faster than Census (see below) over the last six to 12 months. This is important because integrations dictate the level of flexibility your company can have with its tool selection. More integrations are better for reverse ETL.\n\nHightouch customers include Grafana, Plaid, Zeplin, and Mattermost.\n\n### [Census](https://www.getcensus.com/)\n\nIf there was an industry standard for [reverse ETL](https://blog.getcensus.com/what-is-reverse-etl/), it would probably be Census. Census hasn’t been around much longer than Hightouch, but it gained traction first and has an impressive customer lineup. It is a mature tool and has a lot of integrations, but fewer than Hightouch.\n\nCensus customers include Fivetran, dbt, Netlify, and Notion.\n\nIf you’re choosing between reverse ETL tools, you’re most likely choosing between Hightouch and Census. Your decision criteria will come down to available integrations and pricing, as Hightouch and Census have different pricing models. Hightouch prices based on the monthly volume of records synced, while Census prices based on the number of data synchronization workflows you run.\n\n### [RudderStack](https://rudderstack.com/)\n\n[RudderStack](https://rudderstack.com/blog/reverse-etl-is-just-another-data-pipeline) isn’t a pure-play reverse ETL tool — it’s an event streaming platform. The company made its name and grew its customer base by being the open source alternative to [Segment](https://segment.com/). Earlier this year, RudderStack released ETL and reverse ETL features that have made it a competitor in the reverse ETL space.\n\nThe reason this combination of features makes sense is that reverse ETL relies on event streaming or event collecting tools (frequently Segment, [Snowplow](https://snowplowanalytics.com/), or RudderStack) and ETL tools to bring data into the warehouse. RudderStack is the only reverse ETL tool that can also bring the necessary customer data into your warehouse. And the company offers significantly more destination integrations than either Hightouch or Census. This is because it is an event streaming tool, and such tools need extensive integration libraries to compete.\n\nRudderStack customers include Crate & Barrel, Priceline, Acorns, and Hinge.\n\nSegment has reverse ETL functionality too, but the company doesn’t market itself as such. Personas SQL Traits lets you sync data from your warehouse to your cloud applications, but it has to go through Segment’s Personas audience builder.\n\n​​Segment introduced new functionality late last year with [Segment Data Lakes](https://segment.com/blog/introducing-segment-data-lakes/), which builds a customer data lake for you. This reduces the importance of the company’s reverse ETL functionality.\n\n## ​​**Alternatives to reverse ETL**\n\nReverse ETL is great for things like creating customer profiles, segmenting audiences, and other customer-centric processes. The real-time requirement for these processes is not strict, which makes a lot of sense because loading and analyzing data in your data warehouse in real time is not a good architectural pattern. Data warehouses and OLAP (online analytical processing) databases can run complex analytical queries and models quickly, but they aren’t built for real-time application response.\n\nThe emerging solution to these real-time requirements is to use tools like [Rockset](https://rockset.com/) to provide real-time analytics to your applications. In function, Rockset isn’t dissimilar from Elasticsearch, but Rockset is built cloud-native and emphasizes SQL compatibility. This means you’ll be able to scale beyond what Elasticsearch supports and do core SQL functions — like joins — that Elasticsearch doesn’t support.\n\nAn example use case for Rockset is feeding data to a continuously updated leaderboard in a large multiplayer online game. If you have millions of simultaneous players, it is incredibly difficult to ingest events, calculate millions of independent scores, and sort that list in real time, but this is a bread-and-butter use case for tools like Rockset."
    },
    {
        "instruction": "Explain the concept of horizontal scaling in SQL databases compared to vertical scaling. Discuss the limitations of traditional monolithic SQL databases and how horizontal scaling overcomes these challenges by distributing resources across multiple nodes. Highlight the benefits of horizontal scaling such as increased availability, reduced latency, and scalability. The explanation should be clear and informative, suitable for an audience learning about database scaling strategies.",
        "input": "",
        "output": "Scaling your SQL database can be difficult. Most traditional, legacy SQL databases are monolithic – they are constrained to a single server, VM, or container. If your database exceeds the CPU, memory, or storage resources, you either have to move to a new, more powerful instance or upgrade the existing one. Vertical scaling works fine until your database or transaction volumes get large. Since you can only increase the resources for an instance so far, your database can only scale so far. Then you will inevitably start running into latency and stability issues with your database.\n\nHorizontal scaling addresses the problems with vertical scaling. Instead of having a single, monolithic instance, you have multiple distributed instances – called nodes. So if you need more CPU or memory, you add another node. This helps ensure availability, reduce latency, and scale far beyond what is possible with vertical scaling.\n\nIn this talk, we will discuss horizontal scaling: what it is, how it compares with vertical scaling, its strengths and weaknesses, and why it should be a requirement for your next application database."
    },
    {
        "instruction": "Write a detailed community spotlight on YugabyteDB. Include sections on recent developments in the YugabyteDB open-source community, notable contributions, and how the community has grown. Use a conversational tone to make the content accessible and engaging for readers interested in the topic. Mention any specific projects or contributors highlighted in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in YugabyteDB.",
        "input": "",
        "output": "_The Yugabyte community is always active and its members are constantly having interesting conversations and making valuable contributions. We spotlight members of the community to recognize their contributions to making the Yugabyte community a great place._\n\nRadek Gruchalski\n\nManaging Director & Software Engineer @ Klarrio GmbH\n\n**GitHub:** [https://github.com/radekg/](https://github.com/radekg/)\n\n**LinkedIn:** [https://linkedin.com/in/radgruchalski](https://linkedin.com/in/radgruchalski)\n\n**Blog:** [https://gruchalski.com/](https://gruchalski.com/)\n\nIf you’re on the [Yugabyte community Slack](https://www.yugabyte.com/slack) often, you’ve seen Radek Gruchalski. He’s an active member of the community and has even written blog posts about YugabyteDB. Radek is a veteran software developer with decades of diverse database experience. He’s built applications with Microsoft Access, Microsoft SQL Server, MySQL, PostgreSQL, and several NoSQL databases like Cassandra, MongoDB, CouchDB, and HBase. Currently, Radek is a Managing Director and Software Engineer at [Klarrio GmbH](https://klarrio.com/) where he focuses on delivering distributed applications in the cloud, on-premise, and in hybrid environments.\n\n## TELL US ABOUT YOURSELF\n\n#### HOW LONG HAVE YOU BEEN CODING? WHAT GOT YOU INTERESTED? WHAT LANGUAGES/DBS HAVE BEEN YOUR FOCUS?\n\nI have been building software for over 20 years. My first job was for a local photo equipment shop. They also had an online store. My task was creating printed magazines and dabbling with HTML bits for their online offers. One day, I tried changing some things in the shop’s Perl backend. That’s how it all started. Many, many sleepless nights later, Perl was the first programming language I learned properly, but I’ve never got to the Perl Golf mastery level. Afterward came ASP 3.0, PHP 3 with MySQL, followed by a lot of ColdFusion, Java, JavaScript, and Flash with ActionScript 2. Flash Media Server was a great product. At the beginning of my career, I did a little bit of everything: front end, back end, websites, mobile apps, and integrations.\n\nToday, I am a Managing Director at Klarrio GmbH, where I also write and deliver software to spec daily. My focus is distributed architectures and, primarily, backend applications — mainly Scala, Go, and Kafka—deployed in the cloud, on-premise, and in hybrid environments. My work cuts through all aspects of software delivery:  R&D, architecture, implementation, documentation, operations, and some bits of SRE. I create open source tools, my [Terraform Ansible provisioner](https://github.com/radekg/terraform-provisioner-ansible) being the most popular.\n\nThe most enjoyable work for me is on systems directly contributing to users’ quality of work. One of my favorite contracts, some years back, was for a UK-based aircraft parts supplier. The task was to develop a modern alternative to a COBOL back office system that powered all the operations of their business. Together with company employees and top management, I began working on improving internal CRM, ERP, warehouse storage, warehouse operations, and logistics processes with the newly built software deployed in multiple geographical locations. The most gratifying part was seeing how the software directly contributes to the performance of the group of 70+. Working so closely with them shaped me as a developer and created immense respect for the end-user.\n\n## DESCRIBE YOUR DATABASE EXPERIENCE\n\n#### WHICH DATABASES HAVE YOU WORKED WITH BEFORE? WHICH DO YOU CURRENTLY WORK WITH? WHAT PAIN POINTS HAVE YOU ENCOUNTERED?\n\nDoes Microsoft Access count? I’ve done plenty of that! The real databases I’ve worked with include MySQL, PostgreSQL, and a lot of Microsoft SQL Server 2008. I have very fond memories of Transact-SQL. The craziest database thing I’ve done was with MSSQL back in 2010. I was running an active-active replication between the UK and Florida, USA, with log shipping via Dropbox. I have worked with a fair share of NoSQL databases: Cassandra, MongoDB, CouchDB, HBase to name a few.\n\nThe biggest pain points? Database operations can be mentally taxing. Traditional RDBMS is very reliable but notoriously difficult to automate for horizontal scalability without a dedicated DBA or an SRE team.\n\n## HOW DID YOU FIRST HEAR ABOUT YUGABYTEDB?\n\n#### WHAT CAUGHT YOUR ATTENTION? WHY WAS IT INTERESTING? WERE YOU TRYING TO SOLVE A PROBLEM?\n\nAt Klarrio, we have been following YugabyteDB for over two years. Recently, we were on a hunt for a PostgreSQL-compatible database (optionally distributed), and we have decided to evaluate YugabyteDB. What caught our attention was the fact that YugabyteDB reuses the actual PostgreSQL engine but augments it with a purpose-built distributed consensus layer, which does all-things-sharding almost no-op for the operator. Our use case is a multi-tenant database with workload isolation. The choice of YugabyteDB was cemented by the ability to pin the selected tablespaces to selected cluster nodes using geo-replication features.\n\n## WHAT HAS YOUR EXPERIENCE BEEN LIKE WITH YUGABYTEDB?\n\n#### WHAT WERE YOUR EXPECTATIONS? WHAT HAVE YOU BEEN FOCUSING ON? WHAT ARE YOUR THOUGHTS ABOUT IT?\n\nConsidering how difficult distributed systems can be and that RDBMS are not easy to automate at scale, I expected that trying out YugabyteDB would be frustrating. There are so many technologies out there never delivering on the promises they make.\n\nThe task of evaluating different distributed database options landed on my plate. I had a specific end-to-end scenario to trial. It was surprising how painless it was to get YugabyteDB going. The core of the use case was implemented in a couple of days. YugabyteDB hasn’t failed me even once in the process. What was my thought afterward? “Well, that was easier than I ever expected”.\n\n## DESCRIBE WHAT SURPRISED YOU THE MOST\n\n#### TECHNICALLY, OPERATIONALLY, ETC.\n\nI had a 30+ node cluster up and running on a home lab server using Docker Compose in less than a day. It was very easy to hit the ground running. Everything else is like Postgres: databases, schemas, tablespaces on steroids, triggers, stored procedures, plugins. I really enjoy the geo-replication features.\n\nIt’s a very solid product with a permissive license. YugabyteDB is a complex distributed system but the complexity is well hidden behind an intuitive set of APIs and command line tools.\n\nBefore jumping into the code, I spent a couple of days reading through the documentation, which was very easy to follow.\n\nFinally, I’m not very familiar with C++ but I was able to follow the source code without major issues.\n\n## HAVE YOU BUILT OR MIGRATED ANY APPLICATIONS TO YUGABYTEDB?\n\n#### WHY? WHAT WERE YOUR EXPECTATIONS? HAS YUGABYTEDB DELIVERED ON THEM?\n\nI am in the process of delivering a multi-tenant database as a service for a client. So far, YugabyteDB delivers on the promise. My personal interest lies in IAM / IdP systems and I am evaluating the Ory stack on YugabyteDB.\n\n## WHAT FUTURE PLANS DO YOU HAVE FOR YUGABYTEDB?\n\n#### CONTRIBUTIONS TO THE CODE BASE, BUILDING OTHER PROJECTS WITH YUGABYTEDB AS A BACKEND, ETC.\n\nSomething like Keycloak but with row-level geo partitioning would be great to have in the IAM / IdP world. As time goes, I hope to contribute to YugabyteDB. Especially in the documentation and operational procedures area. I share my knowledge on [my personal blog](https://gruchalski.com/); I will post more YugabyteDB material there.\n\n## WHAT PRODUCT FEATURE/ENHANCEMENT ARE YOU MOST LOOKING FORWARD TO?\n\n#### YOU CAN ALWAYS FIND [OUR UP-TO-DATE ROADMAP ON GITHUB](https://github.com/yugabyte/yugabyte-db#current-roadmap).\n\nHaving a more recent database engine is always worthwhile, so I’m definitely looking forward to the PostgreSQL 12 compatibility. The other ones are online schema migrations, full ALTER TABLE support, and more day two operations documentation and guides.\n\n## HAVE YOU LOOKED AT OTHER DISTRIBUTED SQL OR DISTRIBUTED DATABASES IN ADDITION TO YUGABYTEDB?\n\n#### IN YOUR OPINION, WHAT SETS YUGAYBTEDB APART FROM THE REST?\n\nBefore starting with YugabyteDB, I evaluated Cloud Spanner with the Postgres compatible proxy, Amazon Aurora, CockroachDB, CitusDB, and creating custom tooling for managing distributed PostgreSQL. The offerings from the major hyperscalers fell short fast: they do not provide a complete Postgres API as they tend to be an add-on feature on top of a different paradigm, and they do not offer a possibility to run on-premise. The other distributed databases were disqualified based on the licensing model.\n\nYugabyteDB is the only database on the market with a complete, distributed PostgreSQL API which can be deployed on-premise and operated as a service.\n\n## HOW WOULD YOU DESCRIBE YUGABYTEDB TO SOMEBODY JUST STARTING THE PROCESS OF FINDING A NEW DATABASE?\n\n#### WHAT ADVICE WOULD YOU GIVE THEM?\n\nIf you need PostgreSQL and plan for scale, definitely consider YugabyteDB. It will take you from a single node to global scale with little-to-no effort. My advice would be to read through the documentation first. The documentation covers a lot of interesting aspects of the system and that knowledge will be beneficial long-term.\n\n## ANYTHING ELSE YOU WOULD LIKE PEOPLE TO KNOW?\n\nErlang is awesome for learning functional programming. Tests and documentation aren’t boring. Integration tests are better than unit tests. Don’t assume, talk to your users."
    },
    {
        "instruction": "YWrite a detailed guide on connecting a Spring Boot application to Yugabyte Cloud and deploying it on Amazon Elastic Kubernetes Service (EKS). Include sections on setting up the Spring Boot application, configuring Yugabyte Cloud, and deploying the application on EKS. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific tools or configurations discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in Spring Boot and Kubernetes deployments.",
        "input": "",
        "output": "[Spring Boot](https://spring.io/projects/spring-boot) is one of the most popular frameworks for building cloud native applications. It makes configuring an application easy and offers tons of starters to get you off the ground quickly. Each Spring Boot application is stand-alone and self-contained, which makes them easy to deploy in a distributed fashion – to containers or, even better, on Kubernetes.\n\n[Yugabyte Cloud](https://www.yugabyte.com/cloud/) is a perfect match for Spring Boot applications, especially ones made highly available with Kubernetes. Yugabyte Cloud gives you a PostgreSQL-compatible database that is resilient and scalable, just like the applications you run on k8s. You can scale your YugabyteDB cluster out (or in) with just a few clicks. So you can spend less time worrying about running your database and more time focused on building better software. It’s cloud native SQL for cloud native applications. [Sign up for Yugabyte Cloud today](https://cloud.yugabyte.com/register).\n\n## **Walkthrough**\n\nIn this post, we’ll walk you through connecting a Spring Boot application to Yugabyte Cloud and deploying it on Kubernetes via Amazon Elastic Kubernetes Service (EKS). You can view our walkthrough on connecting a Spring Boot application to Yugabyte Cloud and deploying it on Kubernetes via Google Kubernetes Engine (GKE) [here](https://blog.yugabyte.com/connecting-a-spring-boot-application-to-yugabyte-cloud-and-deploying-it-on-google-kubernetes-engine-gke/) and via minikube [here](https://blog.yugabyte.com/connecting-a-spring-boot-application-to-yugabyte-cloud-and-deploying-it-to-kubernetes-on-minikube/). We’ll be using a slightly updated version of the popular Spring Boot PetClinic sample application that has a profile making it compatible with YugabyteDB. The repository for this is at [https://github.com/yugabyte/spring-petclinic](https://github.com/yugabyte/spring-petclinic).\n\nIn this walkthrough, you will:\n* Create a free Yugabyte Cloud account and database cluster\n* Download the Spring Boot PetClinic sample application and connect it to Yugabyte Cloud\n* Containerize the Spring Boot PetClinic sample application\n* Deploy the Spring Boot PetClinic sample application image to EKS\n\n### **Prerequisites**\n\n* Java 8 or newer (full JDK)\n* Git\n* Docker\n* [kubectl](https://kubernetes.io/docs/tasks/tools/)\n* [AWS CLI](https://aws.amazon.com/cli/)\n\nNote: Anything in brackets [ ] needs to be replaced with information from your deployment.\n\n### **Create a free Yugabyte Cloud account and database cluster**\n\n1. Go to [https://cloud.yugabyte.com/signup](https://cloud.yugabyte.com/signup) and sign up for a Yugabyte Cloud account. After you’ve signed up and verified your email address, go to [https://cloud.yugabyte.com/login](https://cloud.yugabyte.com/login) and sign in.\n\n2. Click the Create a Free Cluster button.\n    \n\n3. Select Yugabyte Cloud Free and click the Next button.\n    \n\n4. Select a cloud provider for your cluster, name your cluster, select a region, and click the Next button.\n    \n\n5. Copy the default admin credentials by clicking the Copy Credentials link. Then check the “I have copied the admin credentials” checkbox and click the Create Cluster button.\n\n    a. You can also set your own admin credentials by selecting the “Add your own credentials” radio button and entering your own credentials.\n    \n\n6. Wait for your cluster to be provisioned. Click on “Add IP Allow List” in the top section of the Clusters page.\n    \n\n7. Click the Create New List and Add to Cluster button.\n    \n\n8. In the Name text box, enter “all-ips”. In the IP Address(es) or Range text box, enter “0.0.0.0/0”. Click the Save button. This will allow traffic from all IP addresses to your cluster.\n\n    a. Allowing all IPs access to your cluster is for development purposes only. When you deploy an application to Production, you will want to specify only the specific IP addresses or ranges that need access to your cluster.\n    \n\nYour cluster is now created in Yugabyte Cloud and is accessible to all IP addresses. Leave this page open, as you’ll be accessing the Cloud Shell later to create an application database and user.\n\n### **Download the Spring Boot PetClinic sample application and connect it to Yugabyte Cloud**\n\nNote: Instructions for how to connect this application to YugabyteDB are in `spring-petclinic/src/main/resources/db/yugabytedb/petclinic_db_setup_yugabytedb.md` from the repo you clone below.\n\n1. On your computer from terminal, clone the Spring Boot PetClinic sample application: `git clone https://github.com/yugabyte/spring-petclinic.git`.\n    ```\n    $ git clone https://github.com/yugabyte/spring-petclinic.git\n    Cloning into 'spring-petclinic'...\n    remote: Enumerating objects: 8616, done.\n    remote: Counting objects: 100% (18/18), done.\n    remote: Compressing objects: 100% (18/18), done.\n    remote: Total 8616 (delta 1), reused 13 (delta 0), pack-reused 8598\n    Receiving objects: 100% (8616/8616), 7.29 MiB | 19.03 MiB/s, done.\n    Resolving deltas: 100% (3268/3268), done.\n    ```\n\n2. `cd spring-petclinic`.\n\n3. Copy the contents of `spring-petclinic/src/main/resources/db/yugabytedb/user.sql`.\n\n4. In Yugabyte Cloud, click on “Connect”. Click on Connect to your Application.\n    \n\n5. Save your host address and port (highlighted in the screenshot below) for later. Click the Back link in the Connect to Cluster navigation breadcrumb.\n    \n\n6. Click the Launch Cloud Shell button.\n\n7. Leave the default Database Name, Database User Name, and API Type. Click the Confirm button. This will launch the Cloud Shell.\n    \n\n8. Enter the admin password you copied previously. After you enter the password, you will have a standard YSQL shell (exactly like a PSQL shell in PostgreSQL) that you can interact with from your browser.\n    ```\n    Password for user admin:\n    ysqlsh (11.2-YB-2.4.2.0-b0)\n    SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)\n    Type \"help\" for help.\n\n    yugabyte=#\n    ```\n\n9. Go to [http://localhost:8080](http://localhost:8080/). The PetClinic sample application should be available.\n    \n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running locally.\n\n### **Containerize the Spring Boot PetClinic sample application**\n\n1. Start Docker on your computer and containerize your Spring Boot PetClinic sample application: `./mvnw spring-boot:build-image`.\n\n2. Tag your image: `docker tag [image_id] spring-petclinic` – you can find your image id by running `docker image ls`.\n\n3. Run your image as a container in Docker to make sure it’s working correctly: `docker run -d --name=spring-petclinic -p 8080:8080 -e JAVA_OPTS=\"-Dspring.profiles.active=yugabytedb -Dspring.datasource.url=jdbc:postgresql://[host]:[port]/petclinic?load-balance=true -Dspring.datasource.initialization-mode=never\" spring-petclinic`.\n\n    a. Go to [http://localhost:8080](http://localhost:8080/). The PetClinic sample application should be available.\n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running locally on Docker.\n\n### **Deploy the Spring Boot PetClinic sample application image to Amazon Elastic Kubernetes Service (EKS)**\n\n1. Go to [https://aws.amazon.com/console/](https://aws.amazon.com/console/) and sign up for/sign in to Amazon Web Services (AWS).\n\n2. Type “ecr” in the search bar and click on “Elastic Container Registry”.\n    \n\n3. Create a repository by clicking the Get Started button.\n    \n\n4. Enter a repository name and click the Create Repository button.\n    \n\n5. On your computer from terminal, login to ECR with Docker: `aws ecr get-login-password --region [aws_region] | docker login --username AWS --password-stdin [aws_acct_id].dkr.ecr.[aws_region].amazonaws.com`.\n    ```\n    $ aws ecr get-login-password --region us-west-1 | docker login --username AWS --password-stdin 454529406029.dkr.ecr.us-west-1.amazonaws.com\n    WARNING! Your password will be stored unencrypted in /Users/gavinjohnson/.docker/config.json.\n    Configure a credential helper to remove this warning. See\n    https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n    Login Succeeded\n    ```\n\n6. Tag your PetClinic image with your ECR repo: `docker tag spring-petclinic:latest [aws_acct_id].dkr.ecr.[aws_region].amazonaws.com/spring-petclinic:latest`.\n\n7. Push your PetClinic image to your repo in ECR: `docker push [aws_acct_id].dkr.ecr.[aws_region].amazonaws.com/spring-petclinic:latest`.\n    ```\n    $ docker push 454529406029.dkr.ecr.us-west-1.amazonaws.com/spring-petclinic:latest\n    The push refers to repository [454529406029.dkr.ecr.us-west-1.amazonaws.com/spring-petclinic]\n    1dc94a70dbaa: Pushed\n    0d29ec96785e: Pushed\n    888ed16fa8d4: Pushed\n    ...\n    ```\n    If you go to your repo in ECR, you should see the image you just pushed.\n    \n\n8. Type “cloudformation” in the search bar and click on “CloudFormation”.\n    \n\n9. Click on Create Stack -> Create stack, With new resources (standard).\n    \n\n10. For Prepare Template, select Template is Ready. For Template Source, select Amazon S3 URL. For Amazon S3 URL, enter “https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml”. Click the Next button.\n    \n\n11. Enter a Stack Name, and click the Next button.\n    \n\n12. On the Configure Stack Options page, click the Next button.\n    \n\n13. Review your VPC stack and click the Create Stack button.\n    \n\n14. After your stack is created, type “eks” in the search bar and click on “Elastic Kubernetes Service”.\n    \n\n15. Under “Create EKS cluster” enter a cluster name and click the Next Step button.\n    \n\n16. Select a Cluster Service Role and click the Next button.\n    \n\n17. Under Networking, choose the VPC you just created (that should automatically choose the appropriate subnets) and the two security groups that are available. Under Cluster Endpoint Access, choose Public and Private. Click the Next button.\n    \n\n18. Select nothing for Configure Logging and click the Next button.\n    \n\n19. Review your EKS cluster and click the Create button.\n    \n\n20. After your cluster has been created, go to the Configuration tab and the Compute tab under it. Click on the Add Node Group button.\n    \n\n21. Give your node group a Name, select an IAM Role, and click the Next button.\n    \n\n22. Set your compute and scaling options and click the Next button.\n    \n\n23. Your subnets should automatically be selected. Click the Next button.\n    \n\n24. Review the details of your node group and click the Create button.\n    \n\n25. On your computer from terminal, configure `kubectl` to connect to your EKS cluster: `aws eks --region [aws_region] update-kubeconfig --name [cluster_name]`.\n    ```\n    $ aws eks --region us-west-1 update-kubeconfig --name spring-petclinic\n\n    Added new context arn:aws:eks:us-west-1:454529406029:cluster/spring-petclinic to /Users/gavinjohnson/.kube/config\n    ```\n\n26. Create a new file named `manifest-eks.yml`, enter the following contents, and save:\n    ```\n    apiVersion: v1\n    kind: Service\n    metadata:\n    name: spring-petclinic\n    labels:\n        run: spring-petclinic\n    spec:\n    selector:\n        app: spring-petclinic\n    ports:\n        - protocol: TCP\n        port: 80\n        targetPort: 8080\n    type: LoadBalancer\n    ---\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n    name: spring-petclinic\n    labels:\n        app: spring-petclinic\n    spec:\n    replicas: 2\n    selector:\n        matchLabels:\n        app: spring-petclinic\n    template:\n        metadata:\n        labels:\n            app: spring-petclinic\n        spec:\n        containers:\n        - name: spring-petclinic\n            image: [aws_acct_id].dkr.ecr.[aws_region].amazonaws.com/spring-petclinic:latest\n            ports:\n            - containerPort: 8080\n            env:\n            - name: JAVA_OPTS\n                value: \"-Dspring.profiles.active=yugabytedb -Dspring.datasource.url=jdbc:postgresql://[host]:[port]/petclinic?load-balance=true -Dspring.datasource.initialization-mode=never\"\n    ```\n\n27. Create the Service and Deployment on your EKS cluster: `kubectl create -f manifest-eks.yml`.\n    ```\n    $ kubectl create -f manifest-eks.yml\n    service/spring-petclinic \n    created\n    deployment.apps/spring-petclinic created\n    ```\n\n28. Get the URL of the load balancer for the PetClinic app: `kubectl get svc`.\n    ```\n    $ kubectl get svc\n    NAME               TYPE           CLUSTER-IP     EXTERNAL-IP                                                               PORT(S)        AGE\n    kubernetes         ClusterIP      10.100.0.1                                                                         443/TCP        20h\n    spring-petclinic   LoadBalancer   10.100.30.39   ad7029ef94fed4c06a25897baf9e3c31-1572590571.us-west-1.elb.amazonaws.com   80:30736/TCP   37s\n    ```\n=\n29. If you go to the External IP address listed ([http://ad7029ef94fed4c06a25897baf9e3c31-1572590571.us-west-1.elb.amazonaws.com/](http://ad7029ef94fed4c06a25897baf9e3c31-1572590571.us-west-1.elb.amazonaws.com/)), the PetClinic sample application should be available.\n    \n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running on Kubernetes on EKS.\n\n## **What’s Next?**\n\nGive Yugabyte Cloud a try by signing up for a [free tier account](https://cloud.yugabyte.com/register) in a couple of minutes. Got questions? Feel free to ask them in our YugabyteDB [community Slack](https://yugabyte-db.slack.com/join/shared_invite/zt-nvtsd9px-mV24Ue04YsJmJrSE5FJVPQ#/shared-invite/email) channel."
    },
    {
        "instruction": "Write a detailed guide on connecting a Spring Boot application to Yugabyte Cloud and deploying it on Google Kubernetes Engine (GKE). Include sections on setting up the Spring Boot application, configuring Yugabyte Cloud, and deploying the application on GKE. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific tools or configurations discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in Spring Boot and Kubernetes deployments.",
        "input": "",
        "output": "[Spring Boot](https://spring.io/projects/spring-boot) is one of the most popular frameworks for building cloud native applications. It makes configuring an application easy and offers tons of starters to get you off the ground quickly. Each Spring Boot application is stand-alone and self-contained, which makes them easy to deploy in a distributed fashion – to containers or, even better, on Kubernetes.\n\n[Yugabyte Cloud](https://www.yugabyte.com/cloud/) is a perfect match for Spring Boot applications, especially ones made highly available with Kubernetes. Yugabyte Cloud gives you a PostgreSQL-compatible database that is resilient and scalable, just like the applications you run on k8s. You can scale your YugabyteDB cluster out (or in) with just a few clicks. So you can spend less time worrying about running your database and more time focused on building better software. It’s cloud native SQL for cloud native applications. [Sign up for Yugabyte Cloud today](https://cloud.yugabyte.com/register).\n\n## **Walkthrough**\n\nIn this post, we’ll walk you through connecting a Spring Boot application to Yugabyte Cloud and deploying it on Kubernetes via Google Kubernetes Engine (GKE). You can view our walkthrough on connecting a Spring Boot application to Yugabyte Cloud and deploying it on Kubernetes via Amazon Elastic Kubernetes Service (EKS) [here](https://blog.yugabyte.com/connecting-a-spring-boot-application-to-yugabyte-cloud-and-deploying-it-on-amazon-elastic-kubernetes-service-eks/) and via minikube [here](https://blog.yugabyte.com/connecting-a-spring-boot-application-to-yugabyte-cloud-and-deploying-it-to-kubernetes-on-minikube/). We’ll be using a slightly updated version of the popular Spring Boot PetClinic sample application that has a profile making it compatible with YugabyteDB. The repository for this is at [https://github.com/yugabyte/spring-petclinic](https://github.com/yugabyte/spring-petclinic).\n\nIn this walkthrough, you will:\n* Create a free Yugabyte Cloud account and database cluster\n* Download the Spring Boot PetClinic sample application and connect it to Yugabyte Cloud\n* Containerize the Spring Boot PetClinic sample application\n* Deploy the Spring Boot PetClinic sample application image to GKE\n\n### **Prerequisites**\n\n* Java 8 or newer (full JDK)\n* Git\n* Docker\n* [GCP Cloud SDK](https://cloud.google.com/sdk)\n\nNote: Anything in brackets [ ] needs to be replaced with information from your deployment.\n\n### **Create a free Yugabyte Cloud account and database cluster**\n\n1. Go to [https://cloud.yugabyte.com/signup](https://cloud.yugabyte.com/signup) and sign up for a Yugabyte Cloud account. After you’ve signed up and verified your email address, go to [https://cloud.yugabyte.com/login](https://cloud.yugabyte.com/login) and sign in.\n\n2. Click the Create a Free Cluster button.\n    \n\n3. Select Yugabyte Cloud Free and click the Next button.\n    \n\n4. Select a cloud provider for your cluster, name your cluster, select a region, and click the Next button.\n    \n\n5. Copy the default admin credentials by clicking the Copy Credentials link. Then check the “I have copied the admin credentials” checkbox and click the Create Cluster button.\n\n    a. You can also set your own admin credentials by selecting the “Add your own credentials” radio button and entering your own credentials.\n    \n\n6. Wait for your cluster to be provisioned. Click on “Add IP Allow List” in the top section of the Clusters page.\n    \n\n7. Click the Create New List and Add to Cluster button.\n    \n\n8. In the Name text box, enter “all-ips”. In the IP Address(es) or Range text box, enter “0.0.0.0/0”. Click the Save button. This will allow traffic from all IP addresses to your cluster.\n\n    a. Allowing all IPs access to your cluster is for development purposes only. When you deploy an application to Production, you will want to specify only the specific IP addresses or ranges that need access to your cluster.\n    \n\nYour cluster is now created in Yugabyte Cloud and is accessible to all IP addresses. Leave this page open, as you’ll be accessing the Cloud Shell later to create an application database and user.\n\n### **Download the Spring Boot PetClinic sample application and connect it to Yugabyte Cloud**\n\nNote: Instructions for how to connect this application to YugabyteDB are in `spring-petclinic/src/main/resources/db/yugabytedb/petclinic_db_setup_yugabytedb.md` from the repo you clone below.\n\n1. On your computer from terminal, clone the Spring Boot PetClinic sample application: `git clone https://github.com/yugabyte/spring-petclinic.git`.\n    ```\n    $ git clone https://github.com/yugabyte/spring-petclinic.git\n    Cloning into 'spring-petclinic'...\n    remote: Enumerating objects: 8616, done.\n    remote: Counting objects: 100% (18/18), done.\n    remote: Compressing objects: 100% (18/18), done.\n    remote: Total 8616 (delta 1), reused 13 (delta 0), pack-reused 8598\n    Receiving objects: 100% (8616/8616), 7.29 MiB | 19.03 MiB/s, done.\n    Resolving deltas: 100% (3268/3268), done.\n    ```\n\n2. `cd spring-petclinic`.\n\n3. Copy the contents of `spring-petclinic/src/main/resources/db/yugabytedb/user.sql`.\n\n4. In Yugabyte Cloud, click on “Connect”. Click on Connect to your Application.\n    \n\n5. Save your host address and port (highlighted in the screenshot below) for later. Click the Back link in the Connect to Cluster navigation breadcrumb.\n    \n\n6. Click the Launch Cloud Shell button.\n\n7. Leave the default Database Name, Database User Name, and API Type. Click the Confirm button. This will launch the Cloud Shell.\n    \n\n8. Enter the admin password you copied previously. After you enter the password, you will have a standard YSQL shell (exactly like a PSQL shell in PostgreSQL) that you can interact with from your browser.\n    ```\n    Password for user admin:\n    ysqlsh (11.2-YB-2.4.2.0-b0)\n    SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)\n    Type \"help\" for help.\n\n    yugabyte=#\n    ```\n\n9. Go to [http://localhost:8080](http://localhost:8080/). The PetClinic sample application should be available.\n    \n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running locally.\n\n### **Containerize the Spring Boot PetClinic sample application**\n\n1. Start Docker on your computer and containerize your Spring Boot PetClinic sample application: `./mvnw spring-boot:build-image`.\n\n2. Tag your image: `docker tag [image_id] spring-petclinic` – you can find your image id by running `docker image ls`.\n\n3. Run your image as a container in Docker to make sure it’s working correctly: `docker run -d --name=spring-petclinic -p 8080:8080 -e JAVA_OPTS=\"-Dspring.profiles.active=yugabytedb -Dspring.datasource.url=jdbc:postgresql://[host]:[port]/petclinic?load-balance=true -Dspring.datasource.initialization-mode=never\" spring-petclinic`.\n\n    a. Go to [http://localhost:8080](http://localhost:8080/). The PetClinic sample application should be available.\n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running locally on Docker.\n\n### **Deploy the Spring Boot PetClinic sample application image to Google Kubernetes Engine (GKE)**\n\n1. Go to [https://console.cloud.google.com/](https://console.cloud.google.com/) and sign up for/sign in to Google Cloud Platform (GCP).\n\n2. Type “artifact registry” in the search bar and click on “Artifact Registry”.\n    \n\n3. Click on the Enable button for the Artifact Registry API.\n    \n\n4. Click Create Repository.\n    \n\n5. Give your repository a Name, make sure Docker is selected for Format, and choose a Region. Click the Create button.\n    \n\n6. Click into your new repository and copy its URL.\n    \n\n7. On your computer from terminal, configure Docker to use gcloud for authentication: `gcloud auth configure-docker [region]-docker.pkg.dev`.\n    ```\n    $ gcloud auth configure-docker us-west1-docker.pkg.dev\n    Adding credentials for: us-west1-docker.pkg.dev\n    After update, the following will be written to your Docker config file\n    located at [/Users/gavinjohnson/.docker/config.json]:\n    {\n    \"credHelpers\": {\n    \"us-west1-docker.pkg.dev\": \"gcloud\"\n    }\n    }\n    Do you want to continue (Y/n)?  y\n    Docker configuration file updated.\n    ```\n\n8. Tag your PetClinic image with your Artifact Registry repo: `docker tag spring-petclinic:latest [repo_url]/spring-petclinic:latest`.\n\n9. Push your PetClinic image to your repo in Artifact Registry: `docker push [repo_url]/spring-petclinic:latest`.\n    ```\n    $ docker push us-west1-docker.pkg.dev/gavins-sandbox-320519/spring-petclinic/spring-petclinic:latest\n    The push refers to repository [us-west1-docker.pkg.dev/gavins-sandbox-320519/spring-petclinic/spring-petclinic]\n    1dc94a70dbaa: Pushed\n    0d29ec96785e: Pushed\n    888ed16fa8d4: Pushed\n    ...\n    ```\n    If you go to your repo in Artifact Registry, you should see the image you just pushed.\n    \n\n10. Type “gke” in the search bar and click on “Kubernetes Engine”.\n    \n\n11. Click on the Enable button for the Kubernetes Engine API.\n    \n\n12. Click Create.\n    \n\n13. Click the Configure button under Autopilot.\n    \n\n14. Enter a Name, select a Region, and click the Create button.\n    \n\n15. After your cluster has been created, click on Workloads in the left-navigation panel.\n    \n\n16. Click on Deploy.\n    \n\n17. Select Existing Container Image and click Select in the Image Path.\n    \n\n18. Click on Artifact Registry, expand the folders out, select the image under `spring-petclinic`, and click the Select button.\n    \n\n19. Click the Add Environment Variable button and enter:\n    a. Key: `JAVA_OPTS`\n    b. Value: `-Dspring.profiles.active=yugabytedb -Dspring.datasource.url=jdbc:postgresql://[host]:[port]/petclinic?load-balance=true -Dspring.datasource.initialization-mode=never`\n    \n\n20. Click the Continue button.\n\n21. Enter a Name for your application and click the Deploy button (everything else should be auto-filled).\n    \n\n22. After your workload is deployed, click Expose on the Deployment Details.\n    \n\n23. Set Port to 80, Target Port to 8080 and Service Type to Load Balancer. Click the Expose button.\n    \n\n24. After the load balancer is created, the Service Details page displays.\n    \n\n25. If you click on the External Endpoints link ([http://34.105.123.45/](http://34.105.123.45/)), the PetClinic sample application should be available.\n    \n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running on Kubernetes on GKE.\n\n## **What’s Next?**\n\nGive Yugabyte Cloud a try by signing up for a [free tier account](https://cloud.yugabyte.com/register) in a couple of minutes. Got questions? Feel free to ask them in our YugabyteDB [community Slack](https://yugabyte-db.slack.com/join/shared_invite/zt-nvtsd9px-mV24Ue04YsJmJrSE5FJVPQ#/shared-invite/email) channel."
    },
    {
        "instruction": "Write a detailed community spotlight on YugabyteDB. Include sections on recent developments in the YugabyteDB open-source community, notable contributions, and how the community has grown. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific projects or contributors highlighted in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in YugabyteDB.",
        "input": "",
        "output": "_The Yugabyte community is always active and its members are constantly having interesting conversations and making valuable contributions. We spotlight members of the community to recognize their contributions to making the Yugabyte community a great place._\n\nMartin Roos\n\nHead of Technology @ Fortumo\n\n**LinkedIn:** [https://www.linkedin.com/in/martin-roos-16ab7532/](https://www.linkedin.com/in/martin-roos-16ab7532/)\n\nIf you’re on the [Yugabyte community Slack](https://www.yugabyte.com/slack) often, you’ve seen Martin Roos. He’s an active member of the community, often asking some of the most difficult and nuanced questions about YugabyteDB. Martin is a veteran software developer with over 20 years of database experience, working with Oracle, MySQL, PostgreSQL, and now YugabyteDB. Currently, Martin is the Head of Technology at [Fortumo](https://fortumo.com/) where he focuses on designing and building payment integration applications deployed on AWS.\n\n### **Tell us about yourself**\n\n#### **_How long have you been coding? What got you interested? What languages/DBs have been your focus?_**\n\nThe first time I sat behind a computer was in 1990 at age of 9 on the last days of the Soviet Union. The computer was a Russian “Electronika BK.” It had a spoonful of memory and the computing capacity of a toaster, but it ran Basic out of the box and I quickly got hooked on making my own text-based games and graphical animations. The frugal hardware was a blessing in disguise and gave me a good base to continue working efficiently while learning Pascal, Perl, and C on x86 hardware that roamed into Estonia with other Western influences. In 1999, during my last year of high school, I got my first job as a Perl programmer, working with Oracle databases and also with Sybase’s databases and PowerLanguage.\n\nThis was the time PHP took off rapidly as a popular new language and, with a MySQL storage backend, it captured worldwide web development. I studied it at university and was quickly hired by an online gaming company. I had to add Java to my mix of skills for a solid server-client implementation. The following years took me to various projects (even a 1.5 year stint into Malta) and landed me back in Estonia with Skype working on their web backend, building payment integrations.\n\nSkype at the time was a huge PostgreSQL shop making extensive use of its features and even implementing their own message queues within PostgreSQL. Over the next 7.5 years, I learned a bunch of better coding practices and database management skills for databases that contain hundreds of millions of records. Clusters spanned across tens of machines, things like sequence scans or locks were in no-can-do territory, and no service interruptions for end users were allowed.\n\nAfter Skype, I moved to Fortumo to build another set of payment integration applications on AWS/Java/PostgreSQL stacks. I am currently working on setting up YugabyteDB and evaluating its cloud deployment capabilities. My job title might sound grand but I’m still neck-deep in technical work day-to-day, writing code, reading logs, sketching solutions, and consulting with my colleagues.\n\n### **Describe your database experience**\n\n#### **_Which databases have you worked with before? Which do you currently work with? What pain points have you encountered?_**\n\nIn chronological order:\n\nOracle 2000-2002\n\nMySQL 2000-2008\n\nPostgreSQL 2006-2021\n\nYugabyteDB 2021\n\nFrom this set, Oracle is the old legend, but also legendary for its tall bills and mysterious error messages (deciphering ORA-44604 is not something I want to fill my days with). MySQL had a shiny prime time, but it was bringing a blunt knife to a gunfight with PostgreSQL when it came to features. It just wasn’t on par and its stellar performance degraded if you forced it into using InnoDB to have a real transactional storage backend. YugabyteDB is the new kid on the block and, if you look under the hood, it has very solid underpinnings.\n\n### **How did you first hear about YugabyteDB?**\n\n#### **_What caught your attention? Why was it interesting? Were you trying to solve a problem?_**\n\nWhilst working with big clients and big volumes of data in Fortumo, clients around the world expect our services to be scalable and up 24/7. There is no way to be successful in our business if you have to take your database down for scaling, upgrades, maintenance, etc. It is hard to attract partners like Google or Amazon if you don’t have services that run like clockwork. Things just have to be up and work – period. Big partners can also hit you with surprising big workloads on short (or no) notice, so you have to be ready to react.\n\nWhile I have done a bunch of PostgreSQL logical replication and quick switch upgrades, the processes are needlessly complicated and scary. Also, the clusters tend to be very sensitive and the tooling, even for big companies, is still too hard. Failures of master nodes are nightmare material, at least if you handle financial records. Man-hours invested into the maintenance just seemed to grow and grow.\n\nI started looking around for a designed-for-cloud database that had seamless upgrades, scaling, and automatic data sharding “designed into it” and was as compatible with PostgreSQL as possible. The first I spotted was Cockroachdb, but its shortcomings quickly became apparent and revealed that it is not really an easy drop-in replacement for PostgreSQL. YugabyteDB seems to be the offering with the closest feature set to PostgreSQL out there that can be run anywhere.\n\nAs the times are right now, I see PostgreSQL falling behind, and they don’t seem to be catching up. It is not trivial to change the storage backend of their design without sacrificing or rewriting a big set of features. But, as database users in a constantly online world, we need storage that is designed for the expectations of 2021, and sadly PostgreSQL just isn’t. This is not meant as bashing on PostgreSQL. I understand they have a bunch of history and reasoning for their design, but times have changed and businesses need to keep up.\n\nThat’s where YugabyteDB comes in. I know I keep repeating it but YugabyteDB is designed to be a scalable, clustered database. Having been designed this way is essential to make it meet the goals and expectations of today. You can’t just hotfix these features into existing monolith SQL databases. Once you have seamlessly migrated tablets between servers or done a version upgrade without downtime, you start to “see the light” even if other sacrifices have to be made.\n\n### **What has your experience been like with YugabyteDB?**\n\n#### **_What were your expectations? What have you been focusing on? What are your thoughts about it?_**\n\nAt first, I was slightly overwhelmed by the configuration options and all the little pieces and possibilities, but, eventually, I learned to navigate the documentation and got a hang of how the system operates. I have been deliberately stress testing it and occasionally breaking it, but logs have made sense in most cases and have directed me in the correct direction to solve the issues. I have focused on reliability and maintenance-upgrade procedures, cluster management possibilities, and so forth.\n\n### **Describe what surprised you the most**\n\n#### **_Technically, operationally, etc._**\n\nThe biggest surprise for me was how smoothly the clusters managed data with tablets as the servers popped up and disappeared. Everything worked without my intervention. This was fantastic. I even missed out on the fact that I had lost a node in a three-node cluster for hours since the other nodes took over the traffic and load balancers correctly took the sick node out of the cluster.\n\nI was surprised by how quickly I was able to make my applications run against YugabyteDB. If your application uses relatively simple tables and simple SQL operations, I expect you to run into little or no issues at all while converting.\n\n### **Have you built or migrated any applications to YugabyteDB?**\n\n#### **_Why? What were your expectations? Has YugabyteDB delivered on them?_**\n\nWe have finished test runs of a major application against YugabyteDB and are evaluating different kinds of setup scenarios to run YugabyteDB in production. The application was originally running on PostgreSQL and, during the test, we only had issues around the pessimistic locking being used for concurrency control in some scenarios which YugabyteDB does not yet cater for. Testing through disaster recovery and upgrade scenarios worked perfectly.\n\n### **What future plans do you have for YugabyteDB?**\n\n#### **Contributions to the code base, building other projects with YugabyteDB as a backend, etc.**\n\nAssuming that the application will prove itself stable in production (for now we have stable results from our staging runs), we will follow suit with other mission-critical applications that have big databases.\n\n### **What product feature/enhancement are you most looking forward to?**\n\n#### **_You can always find [our up-to-date roadmap on Github](https://github.com/yugabyte/yugabyte-db#current-roadmap)._**\n\nI think the number one for us is probably pessimistic locking support and YSQL point-in-time recovery support for structure changes. I think the PostgreSQL world migrators would be super happy if there were built-in tooling in YugabyteDB to do logical data replication from existing PostgreSQL databases into YugabyteDB, sort of how the AWS DMS does it. It would take a big pain point of migrations away.\n\n### **Have you looked at other distributed SQL or distributed databases in addition to YugabyteDB?**\n\n#### **_In your opinion, what sets YugaybteDB apart from the rest?_**\n\nYugabyteDB shines with high PostgreSQL compatibility thanks to its reuse of PostgreSQL code. I think this is one of the biggest arguments for it. You can convert your PostgreSQL-backed application quite easily with minimal modifications and don’t have to redesign everything from the ground up like you would have to if you were to go with a database like Cassandra or MongoDB.\n\nFrom direct competition, CockroachDB, Google Spanner, and Amazon’s Aurora Serverless come to mind. The first of these, CockroachDB, has many compatibility issues, and Spanner/Aurora both have issues with transparency in running costs. For some, vendor lock-in might be an argument as well.\n\nAlso on the table was a multimaster MySQL setup, but that is sort of duct tape over a monolithic SQL database, forcing it into acting as a distributed database. There are unexpected defects and every node has to carry the full dataset. The design here is an afterthought, and you can tell.\n\nConsidering my evaluation experiments of the different engines, YugabyteDB came out on top. No choice is perfect, but this seems to come pretty close. If Yugabyte stock were public, I would buy it because of the potential that I see in the long term.\n\n### **How would you describe YugabyteDB to somebody just starting the process of finding a new database?**\n\n#### **_What advice would you give them?_**\n\nYugabyteDB is a database designed for the cloud, built up from the start to be run in clusters on partitioned storage across nodes. If one ventures into using it, reading the documentation is definitely a must. With expanded capabilities also comes the need for expanded knowledge on how to run it. The best way to learn the basics is to spin up a Docker cluster and make your first moves there.\n\nIf you are already using an Elasticsearch cluster in your stack then this feels very similar in many ways. Make sure you have a reliable network and fast disks.\n\nIf your application is not written yet, give YCQL a chance. Everything does not have to be SQL.\n\n### **Anything else you would like people to know?**\n\nWhen choosing your tooling, it is important to make sure the tooling supports the features that you need to have. If you are having performance issues, throwing a bit more money on hardware can take care of that, but you can’t solve missing features the same way.\n\n## **What’s Next?**\n\nGive Yugabyte Cloud a try by signing up for a [free tier account](https://cloud.yugabyte.com/register) in a couple of minutes. Got questions? Feel free to ask them in our YugabyteDB [community Slack](https://yugabyte-db.slack.com/join/shared_invite/zt-nvtsd9px-mV24Ue04YsJmJrSE5FJVPQ#/shared-invite/email) channel."
    },
    {
        "instruction": "Write a detailed guide on connecting a Spring Boot application to Yugabyte Cloud and deploying it on Kubernetes using Minikube. Include sections on setting up the Spring Boot application, configuring Yugabyte Cloud, and deploying the application on Minikube. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific tools or configurations discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in Spring Boot and Kubernetes deployments.",
        "input": "",
        "output": "[Spring Boot](https://spring.io/projects/spring-boot) is one of the most popular frameworks for building cloud native applications. It makes configuring an application easy and offers tons of starters to get you off the ground quickly. Each Spring Boot application is stand-alone and self-contained, which makes them easy to deploy in a distributed fashion – to containers or, even better, on Kubernetes.\n\n[Yugabyte Cloud](https://www.yugabyte.com/cloud/) is a perfect match for Spring Boot applications, especially ones made highly available with Kubernetes. Yugabyte Cloud gives you a PostgreSQL-compatible database that is resilient and scalable, just like the applications you run on k8s. You can scale your YugabyteDB cluster out (or in) with just a few clicks. So you can spend less time worrying about running your database and more time focused on building better software. It’s cloud native SQL for cloud native applications. [Sign up for Yugabyte Cloud today](https://cloud.yugabyte.com/register).\n\n## **Walkthrough**\n\nIn this post, we’ll walk you through connecting a Spring Boot application to Yugabyte Cloud and deploying it on minikube. You can view our walkthrough on connecting a Spring Boot application to Yugabyte Cloud and deploying it on Kubernetes via Amazon Elastic Kubernetes Service (EKS) [here](https://blog.yugabyte.com/connecting-a-spring-boot-application-to-yugabyte-cloud-and-deploying-it-on-amazon-elastic-kubernetes-service-eks/) and via Google Kubernetes Engine (GKE) [here](https://blog.yugabyte.com/connecting-a-spring-boot-application-to-yugabyte-cloud-and-deploying-it-on-google-kubernetes-engine-gke/). We’ll be using a slightly updated version of the popular Spring Boot PetClinic sample application that has a profile making it compatible with YugabyteDB. The repository for this is at [https://github.com/yugabyte/spring-petclinic](https://github.com/yugabyte/spring-petclinic).\n\nIn this walkthrough, you will:\n* Create a free Yugabyte Cloud account and database cluster\n* Download the Spring Boot PetClinic sample application and connect it to Yugabyte Cloud\n* Containerize the Spring Boot PetClinic sample application\n* Start minikube\n* Deploy the Spring Boot PetClinic sample application image to minikube\n\n### **Prerequisites**\n\n* Java 8 or newer (full JDK)\n* Git\n* Docker\n* [kubectl](https://kubernetes.io/docs/tasks/tools/)\n* [minikube](https://minikube.sigs.k8s.io/docs/start/)\n\nNote: Anything in brackets [ ] needs to be replaced with information from your deployment.\n\n### **Create a free Yugabyte Cloud account and database cluster**\n\n1. Go to [https://cloud.yugabyte.com/signup](https://cloud.yugabyte.com/signup) and sign up for a Yugabyte Cloud account. After you’ve signed up and verified your email address, go to [https://cloud.yugabyte.com/login](https://cloud.yugabyte.com/login) and sign in.\n\n2. Click the Create a Free Cluster button.\n    \n\n3. Select Yugabyte Cloud Free and click the Next button.\n    \n\n4. Select a cloud provider for your cluster, name your cluster, select a region, and click the Next button.\n    \n\n5. Copy the default admin credentials by clicking the Copy Credentials link. Then check the “I have copied the admin credentials” checkbox and click the Create Cluster button.\n\n    a. You can also set your own admin credentials by selecting the “Add your own credentials” radio button and entering your own credentials.\n    \n\n6. Wait for your cluster to be provisioned. Click on “Add IP Allow List” in the top section of the Clusters page.\n    \n\n7. Click the Create New List and Add to Cluster button.\n    \n\n8. In the Name text box, enter “all-ips”. In the IP Address(es) or Range text box, enter “0.0.0.0/0”. Click the Save button. This will allow traffic from all IP addresses to your cluster.\n\n    a. Allowing all IPs access to your cluster is for development purposes only. When you deploy an application to Production, you will want to specify only the specific IP addresses or ranges that need access to your cluster.\n    \n\nYour cluster is now created in Yugabyte Cloud and is accessible to all IP addresses. Leave this page open, as you’ll be accessing the Cloud Shell later to create an application database and user.\n\n### **Download the Spring Boot PetClinic sample application and connect it to Yugabyte Cloud**\n\nNote: Instructions for how to connect this application to YugabyteDB are in `spring-petclinic/src/main/resources/db/yugabytedb/petclinic_db_setup_yugabytedb.md` from the repo you clone below.\n\n1. On your computer from terminal, clone the Spring Boot PetClinic sample application: `git clone https://github.com/yugabyte/spring-petclinic.git`.\n    ```\n    $ git clone https://github.com/yugabyte/spring-petclinic.git\n    Cloning into 'spring-petclinic'...\n    remote: Enumerating objects: 8616, done.\n    remote: Counting objects: 100% (18/18), done.\n    remote: Compressing objects: 100% (18/18), done.\n    remote: Total 8616 (delta 1), reused 13 (delta 0), pack-reused 8598\n    Receiving objects: 100% (8616/8616), 7.29 MiB | 19.03 MiB/s, done.\n    Resolving deltas: 100% (3268/3268), done.\n    ```\n\n2. `cd spring-petclinic`.\n\n3. Copy the contents of `spring-petclinic/src/main/resources/db/yugabytedb/user.sql`.\n\n4. In Yugabyte Cloud, click on “Connect”. Click on Connect to your Application.\n    \n\n5. Save your host address and port (highlighted in the screenshot below) for later. Click the Back link in the Connect to Cluster navigation breadcrumb.\n    \n\n6. Click the Launch Cloud Shell button.\n\n7. Leave the default Database Name, Database User Name, and API Type. Click the Confirm button. This will launch the Cloud Shell.\n    \n\n8. Enter the admin password you copied previously. After you enter the password, you will have a standard YSQL shell (exactly like a PSQL shell in PostgreSQL) that you can interact with from your browser.\n    ```\n    Password for user admin:\n    ysqlsh (11.2-YB-2.4.2.0-b0)\n    SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)\n    Type \"help\" for help.\n\n    yugabyte=#\n    ```\n\n9. Go to [http://localhost:8080](http://localhost:8080/). The PetClinic sample application should be available.\n    \n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running locally.\n\n### **Containerize the Spring Boot PetClinic sample application**\n\n1. Start Docker on your computer and containerize your Spring Boot PetClinic sample application: `./mvnw spring-boot:build-image`.\n\n2. Tag your image: `docker tag [image_id] spring-petclinic` – you can find your image id by running `docker image ls`.\n\n3. Run your image as a container in Docker to make sure it’s working correctly: `docker run -d --name=spring-petclinic -p 8080:8080 -e JAVA_OPTS=\"-Dspring.profiles.active=yugabytedb -Dspring.datasource.url=jdbc:postgresql://[host]:[port]/petclinic?load-balance=true -Dspring.datasource.initialization-mode=never\" spring-petclinic`.\n\n    a. Go to [http://localhost:8080](http://localhost:8080/). The PetClinic sample application should be available.\n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running locally on Docker.\n\n### **Start minikube**\n\n1. Start Docker (or another minkube-compatible container/VM manager) on your computer.\n\n2. Open terminal and start minikube: `minikube start`\n    ```\n    😄  minikube v1.22.0 on Darwin 11.5.1 (arm64)\n    ✨  Automatically selected the docker driver\n    👍  Starting control plane node minikube in cluster minikube\n    🚜  Pulling base image ...\n    💾  Downloading Kubernetes v1.21.2 preload ...\n    > gcr.io/k8s-minikube/kicbase...: 326.19 MiB / 326.19 MiB  100.00% 10.82 Mi\n    > preloaded-images-k8s-v11-v1...: 522.45 MiB / 522.45 MiB  100.00% 15.90 Mi\n    🔥  Creating docker container (CPUs=2, Memory=4000MB) ...\n    🐳  Preparing Kubernetes v1.21.2 on Docker 20.10.7 ...\n    ▪ Generating certificates and keys ...\n    ▪ Booting up control plane ...\n    ▪ Configuring RBAC rules ...\n    🔎  Verifying Kubernetes components...\n    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n    🌟  Enabled addons: storage-provisioner, default-storageclass\n    🏄  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n    ```\n\n3. Open a 2nd terminal and run: `minikube dashboard`\n    ```\n    🔌  Enabling dashboard ...\n    ▪ Using image kubernetesui/dashboard:v2.1.0\n    ▪ Using image kubernetesui/metrics-scraper:v1.0.4\n    🤔  Verifying dashboard health ...\n    🚀  Launching proxy ...\n    🤔  Verifying proxy health ...\n    🎉  Opening http://127.0.0.1:51726/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...\n    ```\n\nThat will open [http://127.0.0.1:51726/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy](http://127.0.0.1:51726/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy). Keep this tab open. You’ll use it later to check on your application deployment.\n\n### **Deploy the Spring Boot PetClinic sample application image to minikube**\n\n1. In your 1st terminal, point your shell to minikube’s Docker daemon — This will make it so that Docker and kubectl will use your minikube container registry instead of Docker’s container registry  \n    Run: `minikube docker-env`.\n    ```\n    export DOCKER_TLS_VERIFY=\"1\"\n    export DOCKER_HOST=\"tcp://127.0.0.1:58018\"\n    export DOCKER_CERT_PATH=\"/Users/gavinjohnson/.minikube/certs\"\n    export MINIKUBE_ACTIVE_DOCKERD=\"minikube\"\n    # To point your shell to minikube's docker-daemon, run:\n    # eval $(minikube -p minikube docker-env)\n    ```\n    Run: `eval $(minikube -p minikube docker-env)` —  last line in output above.\n\n2. Build your image again: `./mvnw spring-boot:build-image`.\n\n3. Tag your image again: `docker tag [image_id] spring-petclinic` – you can find your image id by running `docker image ls`.\n\n4. Create a new file named `manifest-minikube.yml`, enter the following contents, and save:\n    ```\n    apiVersion: v1\n    kind: Service\n    metadata:\n    name: spring-petclinic\n    labels:\n        run: spring-petclinic\n    spec:\n    selector:\n        app: spring-petclinic\n    ports:\n        - protocol: TCP\n        port: 8080\n        targetPort: 8080\n    type: LoadBalancer\n    ---\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n    name: spring-petclinic\n    labels:\n        app: spring-petclinic\n    spec:\n    replicas: 2\n    selector:\n        matchLabels:\n        app: spring-petclinic\n    template:\n        metadata:\n        labels:\n            app: spring-petclinic\n        spec:\n        containers:\n        - name: spring-petclinic\n            image: spring-petclinic\n            imagePullPolicy: Never\n            ports:\n            - containerPort: 8080\n            env:\n            - name: JAVA_OPTS\n            value: \"-Dspring.profiles.active=yugabytedb -Dspring.datasource.url=jdbc:postgresql://[host]:[port]/petclinic?load-balance=true -Dspring.datasource.initialization-mode=never\"\n    ```\n\n5. Create the Service and Deployment in minikube: `kubectl create -f manifest-minikube.yml`.\n    ```\n    $ kubectl create -f manifest-minikube.yml\n    service/spring-petclinic created\n    deployment.apps/spring-petclinic created\n    ```\n\n6. Open a 3rd terminal and externally expose your load balancer: `minikube tunnel`\n    ```\n    $ minikube tunnel\n    🏃  Starting tunnel for service spring-petclinic.\n    ```\n\n7. Wait for your application to come online (3 minutes or more for me).\n\n8. If you go to the local IP and Port you configured in your yaml ([http://127.0.0.1:8080](http://127.0.0.1:8080/)), the PetClinic sample application should be available.\n    \n\nThe PetClinic sample application is now connected to your Yugabyte Cloud cluster and running on Kubernetes locally on minikube.\n\nNote:  \nIf all else fails:  \n* `kubectl delete -f manifest-minikube.yml`\n* `minikube stop`\n* `minikube delete --all`\n\n## **What’s Next?**\n\nGive Yugabyte Cloud a try by signing up for a [free tier account](https://cloud.yugabyte.com/register) in a couple of minutes. Got questions? Feel free to ask them in our YugabyteDB [community Slack](https://yugabyte-db.slack.com/join/shared_invite/zt-nvtsd9px-mV24Ue04YsJmJrSE5FJVPQ#/shared-invite/email) channel."
    },
    {
        "instruction": "Write an announcement blog post for Twilio Segment regarding the launch of their Regional Twilio Segment service in the EU, highlighting how it addresses concerns raised by the Schrems II ruling. Include details on when this roadmap was initially announced, what services (Connections, Protocols, and Personas) will be available in the new offering, and emphasize the benefits for data residency within the EU. Discuss how this solution helps businesses comply with GDPR and other data regulations while also maintaining good \"data etiquette\". Provide links to more information on GDPR solutions, booking a demo, and documentation for current customers interested in migration.",
        "input": "",
        "output": "It's been a year-and-a-half since the Schrems II ruling raised questions about the lawfulness of data \ntransfers from the EU to third countries, such as the US. To address these concerns, last March [we \nannounced our roadmap for Regional Twilio Segment in the EU](https://segment.com/blog/get-data-residency-ready/), \nwhich detailed our plans and timelines to introduce data ingestion, processing, and storage in the EU.  \n  \nWe committed to delivering an EU-localized product for Connections, Protocols, and Personas by January \n2022. And that's what we are delivering today. **We're excited to reach another milestone in our Regional \nTwilio Segment roadmap and announce the availability of [Regional Twilio Segment in the EU](https://segment.com/product/gdpr/) \nto all new Business Tier customers.**  \n  \nRegional Twilio Segment is [the worlds #1 Customer Data Platform (CDP)](https://learn.segment.com/number1cdp/) \n– Twilio Segment – deployed in the EU. It gives you Connections, Protocols, and Personas with data \ningestion, processing, storage, and audience creation done on infrastructure hosted in the EU.  \n  \nRegional Twilio Segment makes it easy to collect, unify, and activate your customer data in any system \nwhere you need it while giving you the tools and controls to help enable compliance with EU data \nregulations like GDPR and Schrems II.  \n\n  \nHere's a quick rundown of Regional Segment in the EU's key benefits:\n*   **Ingest customer data in the EU:** Regional data ingestion lets you collect customer data in the EU \n    through our endpoint in Dublin, Ireland. With regional data ingestion, you have more control to \n    keep data in the EU.\n*   **Process customer data in the EU:** Regional data processing lets you process your customer data in \n    the EU. With regional data processing, customer data is processed, filtered, validated, and \n    deduplicated on infrastructure hosted in the EU.\n*   **Store customer data in the EU.** Regional data storage helps you reduce long-term data exposure. It \n    lets you store your event data, including the personal data of your customers, in Segment-hosted \n    archives in the EU. \n*   **Personalize every interaction without sacrificing your data etiquette.** Regional Personas helps \n    you create real-time audiences, computed traits, SQL traits, and Journeys in the EU. With regional \n    Personas, data storage and processing happen on infrastructure hosted in the EU.\n\n## Good data etiquette is crucial now. Twilio Segment is here to help.\nGood data doesn't just mean high-quality, complete, and actionable data. It also means practicing good \ndata etiquette with the personal data of your customers in the EU and around the world. Protecting your \ncustomers' data privacy and practicing good data etiquette are not just items to check off a list, they \nalso help you earn and maintain your customers' trust.\n\nWherever you're at in your efforts to practice good data etiquette and keep up with evolving data \nregulations, Twilio Segment is here to help. We strive to be proactive, transparent, and reliable about \nour solutions that help you practice good data etiquette and enable compliance with data regulations. We \ndo everything we can to be trustworthy partners for businesses in the EU and around the world. We are \ncommitted to providing the tools and controls to help you practice good data etiquette and navigate new \nregulations as they arise.\n\nIf you are interested in Regional Twilio Segment in the EU, new Business Tier customers can get started \nnow. Learn more about [our GDPR solutions](https://segment.com/product/gdpr/) and [book a demo](https://segment.com/demo/) \ntoday.\n\nIf you are a current customer, go to our [docs page](https://segment.com/docs/connections/regional-segment/) \nand sign up for our migration list. We look forward to talking to you about your use case and the best \ntimeframe if you have any data that you want to migrate."
    },
    {
        "instruction": "Explain the concept of a Modern Data Stack (MDS), its key features, benefits for businesses, and how it compares to traditional data architectures. Include examples of problems it solves and mention that this is part of a series with links to the next post for more details on tools used in MDS.",
        "input": "",
        "output": "*This is the first of a 2-part series giving a primer on the Modern Data Stack. You can read the second post, \"[What tools are used in a Modern Data Stack, and what do they do?](https://segment.com/blog/tools-used-in-modern-data-stack-and-what-do-they-do/)\"*\n\nDriven by demand from businesses seeking greater value and more control over their customer data, a new \napproach to data analysis and activation has emerged and become the “go-to” approach for building a data \narchitecture. \n\nThe Modern Data Stack (MDS) is centered around an ecosystem of tools businesses use to collect, move, \nstore, transform, analyze, and operationalize their data. \n\nThe MDS market has seen explosive growth in the last 5+ years. In fact, between 2015 and 2020 alone, the \n[top 30 data infrastructure startups raised over $8 billion of venture capital](https://future.a16z.com/emerging-architectures-modern-data-infrastructure/). \nCompared to traditional data architectures and approaches, MDS tools are easier and faster to implement, \neasier to deploy and maintain, easier to build on top of, usually less expensive, and offer speed and \nscale as features. \n\nSo, with the category gaining so much traction, we thought it would be helpful to put together a primer on \nthe Modern Data Stack.\n\n## What is a Modern Data Stack?\nAn MDS is a combination of easy-to-integrate tools that can be tailored to solve very specific business \nuse cases. It’s difficult to pinpoint when exactly the MDS took off, but it’s likely the first tools that \nspawned the creation of them were cloud-based CDPs like [Twilio Segment](https://segment.com/). \n\nCDPs are often a foundational piece of an MDS. CDPs make integrations, one of the most challenging parts \nof building a traditional data stack, easy. This easy integration has allowed tools that run on customer \ndata to flourish and an ecosystem of data tools to form.\n\nMDS tools share some common features that make them easy to use and economical compared to old-school data \narchitectures. \n\nCommon characteristics are:\n* **Cloud-based.** MDS tools are cloud-based and managed. They commonly have a free-tier to their managed \n  offering and are often open-source projects that can be hosted on cloud infrastructure.\n* **Warehouse-centric.** MDS tools operate directly on the data in your own warehouse or lakehouse. Our \n  research showed that the warehouse is the main trusted data source because it maximizes access, control, \n  governance, and flexibility. **Note:** We use “warehouse” to mean both data warehouse and data lakehouse \n  architectures in our writing. Their use cases are converging and the functionality they offer is \n  overlapping and intermingling (for example, [Snowflake’s external tables](https://www.snowflake.com/blog/external-tables-are-now-generally-available-on-snowflake/) \n  support data lakes by acting as the analytics layer on top of them).\n* **Modular and Best-of-Breed.** MDS tools can be swapped out for other tools that provide the same or \n  similar functionality. This lets customers of MDS tools choose the best tool for their specific needs and \n  circumstances. This also helps customers of MDS tools avoid vendor lock-in.\n* **Easy to configure.** You don’t have to know how to code or manage VMs or k8s on your cloud platform to \n  use MDS tools. They are built to operate with existing cloud platforms and other cloud services. It’s \n  generally as simple to integrate two systems as configuring access via their SaaS APIs or UIs.\n\n## What business problems does a Modern Data Stack help solve?\nAll-in-one marketing platforms – such as Salesforce Marketing Cloud and HubSpot Marketing Hub – have \nbecome popular over the last 10-15 years, and with good reason. \n\nThey give businesses a single platform for all their digital marketing activities and help create unified \ncustomer profiles. They are far from perfect though.\n\nThey are rigid in their functionality, the customer profiles they create are often limited and rigid in \nuse, and they make it difficult to work with your data outside of their tooling. Businesses that build an \nMDS avoid this rigidity and expand the possibilities of what they can do with their data.\n\nThis helps them solve problems such as:\n* **MarTech and Marketing Analytics.** The pandemic made it clear that businesses lacked the agility they \n  needed to operate in a digital-first market. The expectation is you deliver a personalized experience to \n  your customers. To do that, you need the ability to target the right audience with the right message \n  across your digital properties, respond to customer behavior quickly across channels, and measure the \n  impact of everything you execute so you know what works and what doesn’t. All-in-one marketing platforms \n  make it difficult to be as agile as your business needs. Integrating a set of best-of-breed tools is \n  often the best way to help your Marketing team be more agile and deliver better, personalized \n  experiences. The MDS was born from this use case and is often the best way to solve it. This is one of \n  the primary use cases for Twilio Segment as well.\n* **ML and Data Science.** Durable data storage is inexpensive with an MDS. This encourages collecting \n  every bit of customer data you can and storing it in a warehouse. Smart businesses then unleash data \n  scientists and analysts to extract the most critical insights from their data and then apply those \n  insights to their products, processes, and businesses.\n* **Operational Analytics.** Businesses want fast, reliable activation of their data analytics to help \n  solve an array of use cases from Sales to Marketing to Product and beyond. The manual, error-prone \n  processes that exist today to get this data into the tools they use are too fragile and slow. MDS \n  tools – specifically reverse ETL – allow you to automate the integration of your data team’s work into \n  any business application. These data pipelines are consistent, resilient, and automated.\n\n## What benefits do businesses get from a Modern Data Stack?\nBeyond avoiding the pitfalls of all-in-one marketing platforms, a well-built MDS offers businesses several \nother benefits. These benefits stem largely from the fact that MDS tools are almost exclusively managed \nSaaS, are frequently built to be cloud native, and are intentionally designed to integrate with other \ntools in the MDS ecosystem.\n\nThe primary benefits a well-built MDS offers businesses are:\n* **Solve bleeding edge use cases.** MDS tools give your digital business a competitive advantage by \n  giving you faster and greater value out of your customer data. These tools along with new, long-term \n  data storage strategies enabled by the low cost of storage in the warehouse, let you layer MDS tools on \n  top of large, historic data sets to enable specific, tailored solutions that you couldn’t build before \n  such as conversion attribution, advanced lead scoring, and ML-driven personalization.\n* **Speed and scale of data.** Since MDS tools are almost always managed SaaS, they can scale almost \n  infinitely. Expensive computations that took hours or longer with on-prem systems can be scaled in the \n  cloud – often transparently to the user – driving down the time they take to execute.\n* **Easier and faster implementation.** It doesn’t take weeks or months to add a new data system anymore. \n  You don’t have to provision infrastructure or deploy software. It’s easy to swap out or add MDS tools to \n  suit your specific use cases. Now you can maximize flexibility and control while still keeping your \n  costs low. You sign up for a new service and configure the integrations you need via the UI. It can take \n  as little as a few hours to set up and integrate a new tool now.\n* **Reduced cost and maintenance.** MDS tools generally have consumption-based pricing. You only pay for \n  what you use. Since MDS tools are almost always managed SaaS, you don’t have to invest in maintaining \n  the systems. Maintaining five nines and scaling for peak are no longer your concern. That’s what you pay \n  your vendor for.\n* **Easier to build on top of.** MDS tools are built to plugin with other tools. They frequently offer \n  out-of-the-box integrations with popular cloud services and platforms, so stitching together \n  low-or-no-code systems is easier. Product-oriented MDS tools also regularly offer APIs or SDKs, so \n  plugging them into custom-built applications or websites is easy.\n\n## Looking ahead\nNow that we’ve given you the tl;dr of the MDS, join us for [part two](https://segment.com/blog/tools-used-in-modern-data-stack-and-what-do-they-do/) \nwhere we'll cover the tools used in an MDS and what they do."
    },
    {
        "instruction": "Draft a blog post announcing Twilio Segment's new \"Data Purges\" feature, emphasizing how it supports data minimization principles as outlined by GDPR. Include an explanation of why data minimization is crucial in today’s privacy-focused landscape. Detail the steps to initiate a data purge within Twilio Segment and highlight other tools and features that support proactive data privacy and security measures, such as User Deletion & Suppression, Privacy Portal, and Twilio Segment in the EU. Ensure the tone is informative and reassuring for businesses concerned about customer data privacy and compliance with regulations.",
        "input": "",
        "output": "Today we’re introducing [Data Purges](http://segment.com/docs/privacy/account-deletion/) — a new way to \nprotect the privacy and security of your customers’ data.\n\nBusinesses are increasingly aware that the privacy and security of their customer data is important. They \nput safeguards in place to comply with regulations and to protect personally identifiable information \n(PII), protected health information (PHI), or generally sensitive information about their customers. But \none key and often underemphasized component of data privacy and security is **_data minimization_**.\n\nThe General Data Protection Regulation (GDPR) established seven principles that many companies use as a \nframework for their own data privacy programs. Data minimization is one of them. \n\nData minimization is the principle that any organization collecting personal data should be deliberate \nabout the data they collect and only collect what is needed to provide their product or service. Not \ncollecting extra customer data “just in case”. Not stockpiling historic data in hopes it might be useful \nfor something else in the future. Data minimization also means deleting data that’s no longer needed.\n\nIn the age of big data, many organizations now have innumerable opportunities to easily collect data about \ntheir users, and, in some cases, it was standard to store this data indefinitely. This standard has \nchanged. As privacy legislation continues to evolve, having a sound approach to data minimization is key.\n\n## Announcing Data Purges\nGood data doesn’t just mean high-quality, complete, and actionable data. It also means ensuring the \nprivacy and security of the personal data of your customers and making sure every vendor that touches your \ncustomer data does too. We understand that our stance and policies toward data privacy and security can \nhave real-world impacts on your business. We work hard to develop and adhere to responsible and respectful \ndata privacy and security standards at Twilio Segment.\n\nTo help you better ensure the privacy and security of your customers’ data and practice data minimization, \nwe are happy to announce that Twilio Segment now offers **data purge** functionality. Data purges let \nTwilio Segment customers, current and former, completely purge all their customer data from our systems.\n\nYou can initiate a data purge by navigating to your workspace settings in the Twilio Segment app ...  \n\n  \n... Clicking the “Delete Workspace” button ...  \n\n  \n... Confirming that you want to delete your workspace and all customer data in it ...\n\n  \n... and deleting your workspace.\n\nWhen a data purge is initiated, all customer data associated with your workspace will be purged from all \nof Twilio Segment’s internal data stores within 30 days. Please be aware that data purges are for Twilio \nSegment data stores only and these deletions are not forwarded to your connected destination tools. \n\nEven if you delete your workspace and purge all of your data from Twilio Segment systems, you may still \nneed to make additional deletion requests with your destination vendors.\n\nIf you deleted your workspace before March 31st, 2022, and would like to ensure that your old workspace’s \ndata has been purged, you can send us a request at [friends@segment.com](mailto: friends@segment.com) \nwith your workspace slug, and a member of our team will respond to help.\n\n## Twilio Segment helps you be proactive about data privacy and security\nIn addition to data purges, Twilio Segment offers several features to help you take a proactive approach \nto data privacy and security. Some features such as [Protocols](https://segment.com/product/protocols/), \n[Destination Functions](https://segment.com/docs/connections/functions/destination-functions/), \n[Destination Filters](https://segment.com/docs/connections/destinations/destination-filters/), and \n[Warehouse Selective Sync](https://segment.com/docs/connections/storage/warehouses/warehouse-syncs/#warehouse-selective-sync), \nhelp you minimize the data you collect through transformations and filtering. Other features are more \ncomprehensive and address specific data privacy and security needs.\n\n### User Deletion & Suppression\nWe provide you mechanisms to comply with the end user’s Right to be Forgotten and Right to Object under \nGDPR and CCPA by offering [User Deletion and Suppression](https://segment.com/docs/privacy/user-deletion-and-suppression/). \nIf a customer chooses to revoke or alter their consent to data collection, you can use our deletion and \nsuppression APIs to block ongoing data collection about a user. In addition, you can delete all historical \ndata about them across Twilio Segment systems, send delete messages to all connected raw data sources, \nand forward deletion requests to a growing number of supported tools downstream.\n\n### Privacy Portal\nWe introduced [Privacy Portal](https://segment.com/product/privacy-portal/) in 2019. It assists you in \nyour compliance with GDPR and CCPA by offering tools for creating and maintaining a comprehensive data \ninventory. The data inventory allows you to create a centralized repository of all the properties you \ncollect and classifies them by risk from low to high. Customers can also monitor changes to the inventory \nwith real-time alerts and enforce standard privacy controls.\n\n### Twilio Segment in the EU\nIn January 2022, we launched [Twilio Segment in the EU](https://segment.com/docs/guides/regional-segment/). \nIt gives you Connections, Protocols, and Personas with data ingestion, processing, storage, and audience \ncreation done on infrastructure hosted in the EU. It makes it easy to collect, unify, and activate your \ncustomer data in any system where you need it while helping protect the data privacy of end users in the \nEU by giving you the control to keep your customers’ data in the EU.\n\n## Ensuring the privacy and security of your customer data is crucial. Twilio Segment is here to help.\nEnsuring the privacy and security of your customers’ data is not just an item to check off a list, it also \nhelps you earn and maintain their trust. Wherever you’re at in your data privacy and security efforts, \nTwilio Segment is here to help. We strive to be proactive, transparent, and reliable about our data \nprivacy and security solutions. We do everything we can to be trustworthy partners for your business. We \nare committed to providing the tools and controls to help keep your customer data private and secure."
    },
    {
        "instruction": "Write a detailed article on the tools used in a modern data stack and their functions. Include sections on various data tools such as data warehouses, ETL/ELT platforms, data lakes, BI tools, and real-time analytics solutions. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific examples or use cases discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in modern data stacks.",
        "input": "",
        "output": "*This is the second of a 2-part series giving a primer on the Modern Data Stack. You can read the first post, \"[What is Modern Data Stack, and why is everyone making such a big deal about it?](https://segment.com/blog/what-is-MDI-and-why-its-a-big-deal/)\"*\n\nOver the past couple of years, we’ve noticed changes in the tools customers use along with Segment. At \nfirst, this expressed itself as product requests for [Cloud Sources](https://segment.com/docs/connections/sources/about-cloud-sources/) – \nour ETL functionality – or reverse ETL instead of [SQL Traits](https://segment.com/docs/personas/sql-traits/) \n(SQL Traits is reverse ETL that pulls data from your data warehouse into [Personas](https://segment.com/docs/personas/)). \nOccasionally a customer would mention using Fivetran for ETL or Hightouch for reverse ETL to compliment \nSegment.\n\nSo when we went back to our data-savvy customers and asked them to “show us their data stack”, we weren’t \nshocked to see ETL and reverse ETL tools being used. But we were surprised to see just how \nwarehouse-centric their data architectures had become.\n\nWe noticed a pattern. We noticed similar architectures with similar tools and \n[similar use cases being solved](https://segment.com/blog/what-is-MDI-and-why-its-a-big-deal/). When we \naggregated our findings, a common 3-step process came into view:\n\n**One:** All of their customer data from every system goes into a fully-managed cloud data warehouse – \nnamely [Snowflake](https://www.snowflake.com/) and [Google BigQuery](https://cloud.google.com/bigquery). \nAnd not just from the usual tools – CDPs like Segment and ETL tools like Fivetran – but also loading raw \ndata from cloud storage and, sometimes, skipping the blob storage and loading directly from \n[Apache Kafka](https://kafka.apache.org/).\n\n**Two:** Then Analytics Engineers and Data Scientists build models that make all of this data useful. One \nof the most common models is building identity-resolved customer profiles. It is far from the only \nmodeling Data teams do, but it seems to be the foundation for many of the use cases they help with. Data \nteams use a lot of different tools to model and transform data, but, for modeling data on the warehouse, \n[dbt](https://www.getdbt.com/) is the tool of choice.\n\n**Three:** After Analytics Engineers and Data Scientists have made the data useful, Data Engineers setup \ndata pipelines with reverse ETL to send it from the warehouse to downstream destinations. A frequent use \ncase is sending an identity-resolved audience from the data warehouse to MarTech tools. An example is \nusing identity-resolved customer profiles to combine event data from your website with object data from \nSalesforce in your data warehouse, using that data to model a “likely to convert” audience, and sending \nthat audience to an email marketing tool or Google Ads for remarketing. The use cases aren’t just for \nMarketing though. They extend to Sales, Support, Product, and beyond.  \n\n  \n## What makes up a Modern Data Stack?\n### Functionality\nWhen we started analyzing all of these new warehouse-centric data stacks, we started to notice some common \nfunctionality across them. Correspondingly, tool categories have sprung up around many of these \nfunctionalities – such as data collection & pipelining, data storage, and analytics.\n\nBelow are descriptions of the different core functionalities that make up a Modern Data Stack (MDS).\n\n#### Data collection & pipelining\nAn MDS runs on data. Data is the bare necessity. Without data – especially customer data – an MDS has no \npoint and serves no function. The data that powers an MDS comes from a lot of sources, but there are \nprimarily two types of data that are collected and piped to the data warehouse, event data and object \ndata. The tools for collecting these two types of data make it simple and fast to move customer data from \nany source to the data warehouse.\n\n##### CDPs & event pipes (for event data)\nEvent data comes from your websites and applications. It is behavioral data about your customers collected \nthrough their use of your digital properties. Customer Data Platforms (CDPs) are the primary tools used to \ncollect and pipe event data. Event data can also come from IoT devices or sensors, but, in those \ninstances, it isn’t usually customer data and is infrequently used in an MDS.\n\n**Example tools:** [Twilio Segment](https://segment.com/) ([Connections](https://segment.com/docs/connections/)), \n[mParticle](https://www.mparticle.com/), [RudderStack](https://www.rudderstack.com/), \n[Snowplow](https://snowplowanalytics.com/)\n\n##### ETL/ELT (for Object Data)\nObject data often comes from cloud applications that your business uses – Salesforce, Zendesk, SendGrid, \nStripe, Facebook Ads, Google Ads, etc. – but can also come from application databases and data warehouses. \nGenerally, it is tabular and describes the current state of something – like a customer or a process. You \ncollect and pipe object data with ETL tools.\n\n**Example tools:** Twilio Segment ([Cloud Sources](https://segment.com/docs/connections/sources/about-cloud-sources/)), \n[Airbyte](https://airbyte.com/), [Fivetran](https://www.fivetran.com/), \n[Stitch](https://www.stitchdata.com/)\n\n##### Reverse ETL\nAn MDS not only has to provide the functionality to bring data into the data warehouse, but it also has to \nprovide pipelines out of the warehouse as well. A new type of tool, Reverse ETL, satisfies that requirement.\n\nReverse ETL tools sync data from the data warehouse into the tools your business teams use. They formalize \nintegrations from your data warehouse out to destination applications in the same way Segment first \nformalized event streaming integrations. The most popular use cases focus on building identity-resolved \ncustomer profiles – modeled and stored in the data warehouse – and using Reverse ETL to send derivatives \nlike audiences or propensity models to downstream tools.\n\n**Example tools:** [Census](https://www.getcensus.com/), [Hightouch](https://hightouch.io/), \n[Lytics](https://www.lytics.com/), \nRudderStack ([Reverse ETL](https://www.rudderstack.com/product/reverse-etl))\n\n#### Data storage\nThere are a lot of ways to store huge amounts of data now, and it’s cheaper and easier than ever. \nUnmanaged cloud data warehouses such as [AWS RedShift](https://aws.amazon.com/redshift/) are and have long \nbeen [popular destinations in Segment](https://segment.com/the-cdp-report/), but that popularity is \nshifting. The storage products that are gaining the most adoption are managed cloud data warehouses, and \nthey are a major contributor to why building an MDS has gotten popular.\n\nData cloud warehouses are really easy to use. They are “managed” by the vendor. No matter how much data \nyou throw at it, you never have to manage infrastructure, deploy a VM, or have to scale an instance or \ncluster.\n\nManaged cloud data warehouses also separate compute costs from storage costs. You pay for the storage you \nuse and the compute you use separately, and you only pay for what you use. With most unmanaged cloud data \nwarehouses, your data warehouse instance size determines your storage capacity and your compute capacity. \nIf you run out of storage but are barely utilizing the compute on your instance, you still have to upgrade \nto the next instance size up and vice versa. This generally makes managed cloud data warehouses more \ncost-effective than unmanaged ones.\n\n**Example tools:** [BigQuery](https://cloud.google.com/bigquery), [Snowflake](https://www.snowflake.com/)\n\n##### Data lakes\nData lakehouses along with new table formats that power them – notably [Delta Lake](https://delta.io/), \n[Apache Iceberg](https://iceberg.apache.org/), and [Apache Hudi](https://hudi.apache.org/) – are gaining \ntraction as well. A data lakehouse is a data lake implemented in an architecture that includes a metadata \nand governance layer on top of the data store. This lets a data lakehouse function the same way as a data \nwarehouse. So it can be used for BI and analytics in addition to the data science and machine learning \nworkloads that data lakes usually serve.\n\nWe are seeing more and more of our largest enterprise customers building data lakehouses and showing \ninterest in these new table formats. Some businesses use them in a [medallion architecture](https://databricks.com/glossary/medallion-architecture) \nas an inexpensive bronze layer to process and clean data before sending it to their managed cloud data \nwarehouse – their gold layer – for analytics. Others have completely replaced their data warehouse with a \ndata lakehouse.\n\n**Example tools:** [Databricks](https://databricks.com/)/[Apache Spark](https://spark.apache.org/), \n[Dremio](https://www.dremio.com/)\n\n#### Data transformation and modeling\nOnce you have all this customer data in your data warehouse, you have to do something worthwhile with it. \nData Scientists still use Python to process data on the warehouse. That hasn’t changed. But now Analysts, \nData Scientists, and the new role of Analytics Engineer have rallied pretty heavily behind an open source \ntool called dbt.\n\ndbt lets you model data with SQL. If you know SQL, you can use dbt. It also lets you build models that \nhave other models as dependencies. dbt builds a directed acyclical graph (DAG) based on the dependencies \nyou define in your models. So when dbt runs a model, it executes all of the prerequisite models in the \ncorrect order as per the DAG. After a model runs, the output is materialized – the output data set is \nturned into a table or view – in the data warehouse.\n\nWith its ability to build hierarchical data models, dbt ends up being a collaborative tool for Data teams. \nIts project structure also lends itself well to version control, and dbt promotes using Git to version \ncontrol your projects. This helps satisfy the inherent need for version control in a collaborative coding \nenvironment.\n\n**Example tools:** dbt, [Looker](https://www.looker.com/) ([LookML](https://www.looker.com/platform/data-modeling/)), \n[Malloy](https://github.com/looker-open-source/malloy)\n\n#### Identity resolution & customer profiles\nIdentity resolution is the process of using 1st party data to build customer profiles based on identified \nand anonymous interactions across all of your websites and apps. Those customer profiles are used to build \naudiences, propensity models, etc., that businesses rely on to optimize their customer experience, reduce \nchurn, increase growth, etc.\n\n[By definition](https://www.cdpinstitute.org/learning-center/what-is-a-cdp/#section1), CDPs provide some \nform of identity resolution and customer profiles. Segment has one of the most respected and trusted \nidentity resolution and customer profile offerings on the market in [Personas](https://segment.com/product/personas/). \nYet, no offering that we’ve come across lets you send those identity-resolved customer profiles to your \ndata warehouse. So in an MDS, identity resolution and customer profiles are usually modeled by Data teams \nwith event data stored in the data warehouse and saved to the data warehouse. After building \nidentity-resolved customer profiles from first-party data, you can then join and enrich the profiles with \nfirst-party data from your cloud applications and business systems for a more complete and accurate view \nof your customers.\n\nBuilding identity-resolved customer profiles is an extremely common use case for companies with an MDS. It \nis also a time-consuming, labor-intensive process that, to do well, requires a lot of collaboration over \ntime between Data, Marketing, and Product. It took the Segment Data team and Growth Marketing team \napproximately 2 years of continuous collaboration before we had customer profiles we trusted.\n\n**Example tools:** Homegrown identity-resolution via dbt (as described in [this blog post from dbt](https://docs.getdbt.com/blog/customer-360-view-identity-resolution))\n\n#### Analytics & BI\nAnalytics and business intelligence (BI) tools have been around in more or less the same form for a long \ntime. You use SQL or a visual builder to build charts and visualizations based on data in your data \nwarehouse. Then you can add those charts to dashboards. Each offers different exploration, discoverability, \nand collaboration features.\n\nYou can use orchestration tools to automate some common as well as some complex tasks.\n\nHere’s a simple example of a workflow that you could build and schedule with a data orchestration tool. \nLet’s say you need to build a customer list that is refreshed daily. Building it requires firing multiple \nETL runs to the data warehouse to refresh object data about your customers. After ETL completes, you have \nto execute a dbt model on your data warehouse to build the customer list. After dbt completes, reverse ETL \nhas to run to send the customer list to whatever downstream tool your business needs the list in.\n\n**Example tools:** [Apache Airflow](https://airflow.apache.org/), [Dagster](https://dagster.io/), \n[Prefect](https://www.prefect.io/)\n\n#### Data catalog\nData catalogs provide an organized inventory of data in an organization. They automatically collect \nmetadata about your company's data and support adding descriptive documentation about tables or columns. \nThis makes it much easier for stakeholders across your company to search for and discover the data you’re \nalready collecting and storing.\n\nData catalogs also provide data lineage functionality to help track where the data that you use comes \nfrom, where it's copied or replicated, and what data models and dashboards it is used in over time. This \nmakes data debugging easier and can aid in compliance tracking. \n\n**Example tools:** [Alation](https://www.alation.com/), [Atlan](https://atlan.com/), [Castor](https://www.castordoc.com/), \n[Collibra](https://www.collibra.com//), [Stemma](https://www.stemma.ai/)/[Amundsen](https://www.amundsen.io/)\n\n### Non-Functional considerations\nDuring our discussions with our customers, we noticed some non-functional aspects of an MDS that are \nimportant and frequently subjective – such as privacy & security, data quality, and data observability. \nThese aren’t tasks to be done, but qualities an MDS exhibits. As such, tool categories have not sprung up \naround all of these areas.\n\nBelow are descriptions of the important, non-functional aspects of an MDS that matter to Data teams.\n\n#### Privacy & security\nPrivacy & security covers the proper handling of data – consent, notice, and regulatory obligations. Data \nis a strategic asset, and businesses need to comply with rapidly-changing, industry-standard regulations \nand adhere to promises made to consumers to protect their data. Common mechanisms include use case or \ncustomer-specific data usage controls, PII auto-classification, role- or account-based access controls, \nand anonymization strategies. Instead of developers, the target customers are data security and data \nprivacy managers, whose jobs are to minimize the risk of data breaches or leakage.\n\n#### Data quality\nData quality describes how much you trust your data. Data quality can be impacted by the amount and type \nof data collected, the accuracy of your data, and the recency of your data. Most companies try to assure \ntheir data quality with tools and processes that ensure complete, accurate, and fresh data. Investing in \ndata quality will improve trust in your data, reduce time spent navigating and validating data, and \nultimately allow your business to grow faster.\n\n**Example tools:** Twilio Segment ([Protocols](https://segment.com/product/protocols/)), [Trackingplan](https://www.trackingplan.com/)\n\n#### Data observability\nData observability is the ability to understand the health and state of data in your system in near \nreal-time. It’s the equivalent of Dev Ops for your data and includes tools to monitor, trace, and triage \ndata incidents. The net effect is improved data quality, reduced downtime, and, when done well, \npreventative maintenance of your data. \n\n**Example tools:** [Acceldata](https://www.acceldata.io/), [Databand](https://databand.ai/), \n[Great Expectations](https://greatexpectations.io/), [Metaplane](https://metaplane.dev/), [Monte Carlo](https://www.montecarlodata.com/)\n\n#### Data freshness\nWe define “data freshness” as “the time it takes for the effect of an event ingested through Segment to \nbecome observable and usable by a consuming application.” Our customer research shows that most use cases \ncan be met with a 5-minute SLA – with the exceptions of transactional messages and [on-site personalization](https://segment.com/recipes/personalize-landing-pages-to-visitor-interests/) \nwhich requires data freshness in seconds.\n\nCloud data warehouses have some limitations when it comes to data freshness. Depending on multiple \nfactors, latency can vary but typically ends up being 5 minutes or longer. So, today, use cases with a \nmore aggressive SLO for data freshness might not be satisfied by what a data warehouse-centric \narchitecture on its own can achieve. With that said, we do believe warehouses will be able to offer data \nfreshness in seconds in the future\n\n## Build your data foundation today with Twilio Segment\nSegment is a foundational piece of a Modern Data Stack. The event data we collect and pipe to your data \nwarehouse powers your MDS. Integrations are one of the most challenging parts of building a data stack, \nand we make them easy. And our identity-resolved customer profiles, offered through Personas, solve some \nof the most common MDS use cases.\n\n[Sign up for a free Twilio Segment](https://app.segment.com/signup) workspace and start building the right \nfoundation for your MDS today."
    },
    {
        "instruction": "Draft an announcement blog post for the release of Earthly v0.7. Include details about compatibility with the new CI/CD platform called Earthly CI, promotion of Podman support and the `WAIT` command from Beta to GA status, breaking changes such as mandatory use of the `VERSION` command in Earthfiles, and a table listing all features promoted to GA in this version. Highlight the significance of reliability and stability for users’ build and CI/CD processes. Mention thanks to the community for their support and contributions. End with a call-to-action encouraging readers to get started with Earthly today.",
        "input": "",
        "output": "Today, we're announcing the release of Earthly v0.7. The new version introduces changes that make Earthly work with Earthly CI, our new CI/CD platform (read more about it in *[Earthly CI: Launching a new era for CI](https://earthly.dev/blog/launching-earthly-ci/)*), in addition to promoting several features from Experimental or Beta to GA – notably Podman support and the `WAIT` command.\n\nWe do not take major or minor releases lightly at Earthly. We've never had a major release, and our last minor release was over a year ago, in December 2021 (read about it in [Announcing Earthly v0.6](https://earthly.dev/blog/earthly-v0-6/)). That's because we know that the reliability and stability of your build and CI/CD processes are of the utmost importance. In every minor release of Earthly (and also eventually when we have a major release), all features promoted to GA have finalized APIs and have been through thorough testing. That's why we are comfortable enabling all features promoted to GA by default.\n\n## Compatibility With Earthly CI\n\nEarthly 0.7 is the first version compatible with Earthly CI. All Earthly CI pipelines are defined in Earthfiles using Earthly's simple, familiar syntax and stored in the project's repository.\n\nEarthly 0.7 introduces the new keywords `PIPELINE` and `TRIGGER` to define Earthly CI pipelines. You can execute pipelines on your computer in the exact same way you execute build targets. So it's super simple to iterate on and test CI pipelines on your computer before you push them to prod.\n\nThe following example defines a CI pipeline named `build-pipeline` that executes the `+my-service` target and is triggered whenever there is a pull request raised against the `main` branch of the repository:\n\n```{.Dockerfile caption=\"Earthfile\"}\nbuild-pipeline:\n    PIPELINE\n    TRIGGER pr main\n    BUILD +my-service\n\nmy-service:\n    BUILD +image\n    BUILD +unit-test\n    BUILD +integration-test\n\nimage:\n    FROM alpine:3.17\n    COPY +src/my-service /bin\n    ENTRYPOINT [\"/bin/my-service\"]\n    ARG VERSION=latest\n    SAVE IMAGE --push acmecorp/my-service:$VERSION\n\nunit-test:\n    FROM +src\n    COPY main_test.go ./\n    RUN go test ./...\n\nintegration-test:\n    FROM +src\n    COPY main_integration_test.go .\n    COPY docker-compose.yml ./\n    WITH DOCKER --compose docker-compose.yml\n        RUN go test --tags=integration ./...\n    END\n\nsrc:\n    COPY go.mod go.sum ./\n    RUN go mod download\n    COPY main.go ./\n    RUN go build -o my-service main.go\n    SAVE ARTIFACT my-service AS LOCAL my-service\n```\n\nFor more information on using the `PIPELINE` and `TRIGGER` commands, see [our docs](https://docs.earthly.dev/docs/earthfile#pipeline-beta).\n\n## Podman Support\n\n[Podman](https://podman.io/) is an alternative to Docker. It is an open source tool for developing, managing, and running containers on Linux systems. Podman also works on Mac systems using [`podman machine`](https://docs.podman.io/en/latest/markdown/podman-machine.1.html) and Windows systems using [WSLv2](https://learn.microsoft.com/en-us/windows/wsl/about#what-is-wsl-2) and `podman machine`. Podman is daemonless, which has some advantages over Docker's approach.\n\nEarthly has supported Podman for a while now, but, with Earthly 0.7, Podman support is being promoted out of beta status and is generally available. Earthly will automatically detect the container frontend, whether Docker or Podman, and use it automatically, both for running Buildkit locally and outputting built images locally.\n\nFor more information on using Podman with Earthly, see [our docs](https://docs.earthly.dev/docs/guides/podman).\n\n## `WAIT` Command\n\n***Breaking Change**: The behavior of `--push` mode has changed in a backwards incompatible manner.*\n\nPushing no longer requires everything else to succeed. Previously, `--push` commands would only execute if all other commands had succeeded. This requirement is no longer enforced. This allows for more flexible push ordering using the new `WAIT` command. To achieve the behavior of the previous `--push` mode, you need to wrap any pre-required commands in a `WAIT`/`END` block.\n\n### Example: Push an Image Only if Tests Have Passed\n\n```{.Dockerfile caption=\"Earthfile\"}\ntest-and-push:\n  WAIT\n    BUILD +test\n  END\n  BUILD +my-image\nmy-image:\n  ...\n  SAVE IMAGE --push my-org/my-image:latest\n```\n\n### Example: Push an Image and Deploy Using the Newly Pushed Image\n\nWhere `./deploy.sh` is a custom deployment script instructing a production environment to start using the image that was just pushed.\n\n```{.Dockerfile caption=\"Earthfile\"}\npush-and-deploy:\n  ...\n  WAIT\n    BUILD +my-image\n  END\n  RUN --push ./deploy.sh my-org/my-image:latest\nmy-image:\n  ...\n  SAVE IMAGE --push my-org/my-image:latest\n```\n\nFor more information on using the `WAIT` command, see [our docs](https://docs.earthly.dev/docs/earthfile#wait).\n\n## `VERSION` Is Now Mandatory\n\n***Breaking Change:** The `VERSION` command is now required for all Earthfiles, and an error will occur if it is missing.*\n\nIf you are not ready to update your Earthfiles to use 0.7 (or 0.6), you can declare VERSION 0.5 to continue to use your Earthfiles.\n\n## All Features Promoted to GA in 0.7\n\n|Feature Flag    |Description|\n|-|-|\n|--explicit-global |    Base target args must have a --global flag in order to be considered global args|\n|--check-duplicate-images    | Check for duplicate images during output|\n|--earthly-version-arg    | Enables builtin ARGs: EARTHLY_VERSION and EARTHLY_BUILD_SHA|\n|--use-cache-command    | Allow use of CACHE command in Earthfiles|\n|--use-host-command    | Allow use of HOST command in Earthfiles|\n|--use-copy-link    | Use the equivalent of COPY --link for all copy-like operations|\n|--new-platform    | Enable new platform behavior|\n|--no-tar-build-output    | Do not print output when creating a tarball to load into WITH DOCKER|\n|--use-no-manifest-list    | Enable the SAVE IMAGE --no-manifest-list option|\n|--use-chmod    | Allow setting the permissions of the copied files.|\n|--shell-out-anywhere    | Allows shelling-out in any earthly command (including in the middle of ARG)|\n|--earthly-locally-arg    | Enable the EARTHLY_LOCALLY arg|\n|--use-project-secrets    | Enable project-based secret resolution|\n|--use-pipelines |    Enable the PIPELINE and TRIGGER commands|\n|--earthly-git-author-args    | Enable the EARTHLY_GIT_AUTHOR and EARTHLY_GIT_CO_AUTHORS args|\n|--wait-block    |Enable the WAIT / END block commands|\n\nFor more information on the individual Earthfile feature flags see the [Earthfile version-specific features page.](https://docs.earthly.dev/docs/earthfile/features)\n\nFor more detailed information on Earthly 0.7, read the [Earthly 0.7 release notes](https://github.com/earthly/earthly/releases/tag/v0.7.0).\n\n## Thank You to the Earthly Community\n\nHuge thank you to the Earthly community! From the 8.7K+ developers that have starred us on [GitHub](https://github.com/earthly/earthly) to our 800+ member community Slack ([click to join](https://earthly.dev/slack)), you all have played an instrumental role in the development of Earthly 0.7. Your code contributions, feature requests, and feedback made this release possible. We are so grateful to have such a dedicated and supportive community of developers from all over the world that believe in Earthly and want to make builds better.\n\n## Get Earthly Today\n\nMake your CI/CD super simple. Get Earthly. Earthly is a versatile, approachable CI/CD framework that gives you repeatable builds that you write once and run anywhere; has a simple, instantly recognizable syntax – like Dockerfile and Makefile had a baby; and works with every language, framework, and build tool."
    },
    {
        "instruction": "Draft an announcement for achieving SOC 2 Type 1 compliance by March 29, 2023. Include:\n\n- A statement about the company's commitment to security.\n- An explanation of what SOC 2 is and why it matters for SaaS companies like Earthly.\n- Details on the difference between SOC 2 Type 1 and Type 2 compliance.\n- Information on how this impacts current users (Earthly CI and Satellites).\n- A forward-looking statement about future plans towards SOC 2 Type 2 compliance, aiming to achieve it by Fall 2023.\n- Contact information for requesting a copy of the report.\n\nEnsure the tone is confident and reassuring.",
        "input": "",
        "output": "We take security very seriously here at Earthly. As a user, you shouldn't have to worry about a SaaS vendor being sloppy with your or your business' data. We have an information security program that has been in place and communicated throughout our organization for quite some time now, but that assurance only means so much. That's why we are excited to announce that Earthly achieved SOC 2 Type 1 compliance on March 29, 2023.\n\n## What Is SOC 2?\n\n[Service Organization Control 2](https://www.aicpa-cima.com/topic/audit-assurance/audit-and-assurance-greater-than-soc-2) (SOC 2) is a set of criteria established by the [American Institute of Certified Public Accountants](https://www.aicpa-cima.com/home) (AICPA) to assess controls relevant to the security, availability, and processing integrity of the systems a service organization uses to process users' data and the confidentiality and privacy of the information processed by these systems. SOC 2 compliance is important for SaaS providers like Earthly, as it demonstrates that we have implemented controls to safeguard users' data.\n\nThere are two types of SOC 2 compliance:\n\n1. **SOC 2 Type 1**:  Validates that an organization has established appropriate controls at a specific point in time.\n2. **SOC 2 Type 2**:  Confirms that an organization has maintained and operated those controls over a period of time, typically 6 to 12 months.\n\n## What Does This Mean for Earthly CI and Satellites Users?\n\nFrom a functional standpoint, achieving SOC 2 Type 1 compliance doesn't mean anything. Earthly CI and Earthly Satellites will work exactly as they always have and as expected.\n\nFrom a non-functional standpoint, SOC 2 Type 1 compliance means that we have established a set of controls and processes to ensure the security of our users' data. This compliance demonstrates that we have the necessary measures in place to protect sensitive information from unauthorized access and disclosure.\n\n## What's Next? SOC 2 Type II\n\nOur commitment to security doesn't end with SOC 2 Type 1. We are already working towards achieving SOC 2 Type 2 compliance, which we plan to accomplish by Fall 2023. This will further validate that we maintain the highest levels of security, ensuring that our users can continue to rely on and trust Earthly.\n\n*To request a copy of our SOC 2 Type 1 report, contact security@earthly.dev.*"
    },
    {
        "instruction": "Write a detailed article on the general availability (GA) release of Earthly Satellites. Include sections on what Earthly Satellites are, their benefits, and how they can be used to enhance CI/CD pipelines. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific features or use cases discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in CI/CD tools.",
        "input": "",
        "output": "We're excited to announce that [Earthly Satellites](https://earthly.dev/earthly-satellites) have moved from Beta to General Availability (GA)! Satellites are remote build runners managed by the Earthly team. Satellites make builds fast, are super simple to use, and work with any CI as well as builds triggered from your laptop.\n\n## Why Use Earthly Satellites?\n\nEarthly Satellites come with their own cache that is local to the satellite. This means that builds are faster when executed repeatedly on a satellite. This can be especially useful when using Satellites from a sandboxed CI environment, where the cache from previous builds would not otherwise be available or it would take time to download and upload.\n\n## Typical Use Cases For Earthly Satellites\n\n* Speeding up CI builds in sandboxed CI environments such as GitHub Actions, GitLab, CircleCI, and others. Most CI build times are improved by 2-20X with Satellites.\n* Sharing compute and cache with your coworkers or CI.\n* Executing builds on x86 architecture natively when you are working from an Apple Silicon machine (Apple M1/M2) and vice versa, arm64 builds from an x86 machine.\n* Benefiting from high-bandwidth internet access from the satellite, allowing for fast downloads of dependencies and pushes for deployments. This is particularly useful if you are in a location with slow internet.\n\n## How Earthly Satellites Work on Your Laptop\n\n* You kick off the build from the command line, and Earthly uses a remote satellite for execution.\n* The source files used are the ones you have locally in the current directory.\n* The build logs from the satellite are streamed back to your terminal in real time, so you can see the progress of the build.\n* The outputs of the build - images and artifacts - are downloaded back to your local machine upon success.\n* Everything looks and feels as if it is executing on your computer in your terminal.\n* In reality, the execution takes place in the cloud with high parallelism and a lot of caching.\n\n## How Earthly Satellites Work in Your CI\n\n* The CI starts a build and invokes Earthly.\n* Earthly starts the build on a remote satellite, executing each step in isolated containers.\n* The same cache is used between runs on the same satellite, so parts that haven't changed do not repeat.\n* Logs are streamed back to the CI in real time.\n* Any images, artifacts, or deployments that need to be pushed as part of the build are pushed directly from the satellite.\n* Build pass/fail is returned as an exit code, so your CI can report the status accordingly.\n\nI could go on and on about how Earthly Satellites works and why it's great, but, instead of having to trust me, I interviewed Ses Goe, a Satellites user, so you can get his unbiased feedback on Satellites.\n\n## Interview With Earthly Satellites User, Ses Goe From NOCD\n\n### Tell me about [NOCD](https://www.treatmyocd.com/)\n\nOCD is a global health epidemic in the sense that 2% of the world's population is estimated to have OCD. Our mission at NOCD is to restore hope for people with OCD through better awareness and treatment. We provide a particular type of therapy, called exposure and response prevention therapy, which has been clinically shown to be the most effective treatment for people who have OCD.\n\n### Tell me about your role at NOCD\n\nMy technical title is Director of Internal Tools, but we're small so I end up touching a little bit of everything. I would say that my primary responsibilities involve really anything internal – internal dashboards that we use for billing and marketing, internal systems we use to make the process when doctors refer people to us smooth, etc. Anything that our greater corporate employees handle I'm probably in some way responsible for maintaining.\n\n### Before Earthly, what tools did NOCD use for builds and CI?\n\nWe primarily used GitHub Actions with plain old Docker. And builds were notoriously really, really, really, really slow. It was not uncommon to have our CI/CD pipeline take an hour to run all the way through. Especially when we were running our full integration test suite. It just took absolutely forever. A large percentage of that was the fact that our Docker images weren't cached in any meaningful way. So everything was being rebuilt from scratch over and over and over, and it just slowed everything down. I discovered Earthly at one point and just gave it a try. And that was what convinced me that this problem was worth investing more time into.\n\n### What initially attracted you to Earthly?\n\nCaching but also the fact that the cache was basically attached to your Docker image as a part of the build process. Therefore you could upload that cache to your private registry, and you could pull that cache from your registry. So it meant that it [the cache] would transition across CI runners, which is usually the biggest issue with CI.\n\nWhen you're using a public service like GitHub Actions or Semaphore or Circle CI or whatever CI platform you prefer, you typically don't have the luxury of a dedicated CI box. And that's also kind of the whole point of CI platforms these days. They're ephemeral – you get a fresh box every push, which typically forces you to create repeatable (slow) builds. But there is no way to share a cache between builds or even use a Docker cache. I experimented with Circle CI's integrated Docker cache for their build runners. That turns out to not really work like I think, or how most people would expect, it would. At least it was weird for me anyway. Earthly gave us the solution to that and it took literally just going from Dockerfile to Earthfile to bring build times from 45-ish minutes down to probably 15 or 20, just from skipping the 25 minutes of installing Python dependencies. That was enough for us to use Earthly from that point forward.\n\n### What initially attracted you to Earthly Satellites?\n\nI tuned our CI/CD performance as much as I could, but the vast majority of our build time was still spent downloading the Docker image and its cache before actually running anything, because our image ends up being 1.5GB, or something like that, after being fully built. It's pretty big. The majority of our time was spent downloading that cache. I was trying to find a way to maybe improve that or figure out if I was using bad practices. When I learned that when using Satellites you're basically transferring files to an external runner that is no longer ephemeral, you have now reinvented the idea of a dedicated build machine so to speak. That was all I needed to go, \"Okay, well obviously that's the fastest possible way to do this.\"\n\nIt was love at first sight switching over to Satellites. It was the same kind of case as switching to Earthly for us. Switching to Earthly took us from 45 minutes to 20 minutes, and using Satellites took us from like 20 minutes to 4 or 5 on average. It's been another 2x increase, literally just from using Satellites. It's been astounding.\n\n### Now that you've used Satellites for a while, what are the features or benefits that you think are the most important?\n\nNot only has Satellites been great in terms of speed and performance, that's the main reason that we love it, but the reason I love it as the maintainer of our build pipeline is it took away all of the hacky stuff that I was doing to make builds fast for our devs. The inline cache is great, but it requires a lot more ceremony to specify the default read branch (usually develop), among other branch-specific things. With Satellites, all of that stuff disappeared because the satellite is smart enough to just work out of the box. I don't even think about whether I'm running on a PR branch versus running on a development branch. I don't really think about any of that stuff anymore, because I just think of it as if it's running locally on a machine. That mental model tends to translate really, really well because that is effectively what it is. It's an ephemeral machine that you spin up. You spin up the same one every time to run your build.\n\nAnother reason that we've effectively hitched ourselves to your wagon is every time I have a conversation with an Earthly employee, it's like you guys are just headed for the moon, man. There's so much opportunity for improvement, and you guys are building around all the primitives of BuildKit itself. So for any further cache improvements you come up with, it's just gonna benefit everybody. It's super cool to see I'm really excited to hear all the stuff you all are working on.\n\n## Try Earthly Satellites\n\nWe want to extend a big thank you to Ses as well as all of the other Earthly Satellites users that have helped us refine and improve Satellites to the point that we can now bring them into GA.\n\nIf your CI builds are slow or your team is constantly having to waste time waiting for slow builds, try [Earthly Satellites](https://earthly.dev/earthly-satellites). They are remote build runners that make builds fast with an automatic and instantly available build cache. Builds can be triggered by any CI as well as from your laptop. And they're super simple to use. Get started by following [the steps in our documentation to self-serve Satellites](https://docs.earthly.dev/earthly-cloud/satellites).\n\n**Next, you might like to read about Earthly Compute, the service that underpins Earthly Satellites. Dive into the technicalities of Satellites with some of the engineers that build them in our blog post [Remote Code Execution as a Service](/blog/remote-code-execution/).**"
    },
    {
        "instruction": "Write a detailed case study on how NOCD improved their CI build times and reduced latency using Earthly and Earthly Satellites. Include the following points:\n\n- Background: Brief introduction about NOCD (NOCD is the #1 telehealth provider for OCD treatment).\n- Problem: Long build times due to lack of caching in Docker image builds.\n- Solution 1: Introduction of Earthly for automatic caching, leading to a reduction from 45 minutes to 20 minutes.\n- Challenge After Solution 1: CI latency caused by slow cache downloads and uploads with Semaphore.\n- Solution 2: Implementation of Earthly Satellites for instantly available cache, reducing build times further down to about 5 minutes (9x speed increase in total).\n- Results: Highlight the significant improvement in build time from initial 45 minutes to finally around 5 minutes.\n- Testimonial: Include a quote from Ses Goe, Director of Internal Tools at NOCD, discussing their experience with Earthly and Satellites.\n- Benefits of Using Earthly: Automatic caching, reduced CI latency, flexibility, simplicity, and stability.\n\nMaintain a professional tone and ensure the context is clear for understanding the technology's impact on NOCD’s operations.",
        "input": "",
        "output": "\\\n\n## Highlights\n\n* [NOCD](https://www.treatmyocd.com/) struggled with long build times and latency in their CI builds due to a lack of caching and, after caching was implemented with Earthly, slow cache downloads and uploads.\n* They implemented Earthly and Earthly Satellites. Earthly's automatic caching sped up builds, and Earthly Satellites' instantly available cache eliminated latency in their CI caused by cache downloads and uploads.\n* The combined implementation of Earthly and Earthly Satellites resulted in a 9x speed increase in CI builds, reducing build times from 45 minutes to just 5 minutes.\n\n## Results\n\n* **9x faster builds in CI**\n* **Build Time Reduction: 45min -> 20min** (by implementing Earthly w/ Semaphore)\n* **Build Time Reduction: 20min -> 5min** (by implementing Earthly + Earthly Satellites w/ Semaphore)\n\n\"It was love at first sight switching over to Satellites. It was the same kind of case as switching to Earthly for us. Switching to Earthly took us from 45 minutes to 20 minutes, and using Satellites took us from like 20 minutes to 4 or 5 on average. It's been another 2x increase, literally just from using Satellites. It's been astounding.\" - Ses Goe, Director of Internal Tools at NOCD\n\n## Customer Profile\n\nNOCD is the #1 telehealth provider for the treatment of obsessive-compulsive disorder (OCD). It creates access to online therapy for people with OCD through its telehealth platform. Members can access and schedule live, face-to-face video therapy sessions with licensed therapists that specialize in exposure and response prevention therapy (ERP) - considered the \"gold standard\" in OCD treatment. In between sessions, patients have access to 24/7 support through NOCD's self-help tools and peer communities.\n\nEarthly was introduced to NOCD through its internal tooling team. One of that team's most significant responsibilities involves building and deploying the core Docker images used in their customer-facing services. NOCD primarily writes its software in Python, uses Semaphor for CI (previously used GitHub Actions), and deploys to AWS.\n\n## Challenge: Long Build Times Due to Lack of Caching\n\nBefore implementing Earthly, NOCD faced issues with long build times, driven heavily by the installation of Python dependencies for their Docker image builds. Their Python dependencies rarely changed, which meant that the time taken to download these dependencies was almost always duplicative and wasted.\n\n\"We primarily used GitHub Actions with plain old Docker. And builds were notoriously really, really, really, really slow. It was not uncommon to have our CI/CD pipeline take an hour to run all the way through. Especially when we were running our full integration test suite. It just took absolutely forever. A large percentage of that was the fact that our Docker images weren't cached in any meaningful way. So everything was being rebuilt from scratch over and over and over, and it just slowed everything down.\" - Ses Goe, Director of Internal Tools at NOCD\n\n### Solution: Earthly's Automatic Caching\n\nEarthly's caching functionality addressed this challenge. It automatically identifies parts of the build that haven't changed since the last successful execution, like Python dependencies, and stores them for future use. When a new build is initiated, Earthly reuses the cached elements instead of rebuilding them, which dramatically speeds up the build process.\n\n\"It took literally just going from Dockerfile to Earthfile to bring build times from 45-ish minutes down to probably 15 or 20, just from skipping the 25 minutes of installing Python dependencies. That was enough for us to use Earthly from that point forward.\" - Ses Goe, Director of Internal Tools at NOCD\n\n## Challenge: Build Latency in CI From Cache Download and Upload\n\nNOCD moved off GitHub Actions to Semaphor for their CI, Earthly for their builds, and AWS for inline caching. This proved significantly faster than when they were using GitHub Actions alone. Unfortunately, downloading the cache from AWS to Semaphore and vice versa became the new primary contributor to latency in their CI builds.\n\n\"I tuned our CI/CD performance as much as I could, but the vast majority of our build time was still spent downloading the Docker image and its cache before actually running anything, because our image ends up being 1.5GB, or something like that, after being fully built. It's pretty big.\" - Ses Goe, Director of Internal Tools at NOCD\n\n### Solution: Earthly Satellites' Instantly Available Cache\n\nEarthly Satellites provided a solution to NOCD's CI latency issues by storing build caches close to the build execution, directly on the satellite. This reduced build times by eliminating the cache download to and upload from AWS that was required on every CI run with their previous inline caching solution.\n\n\"Satellites took us from like 20 minutes to 4 or 5 on average. It's been another 2x increase, literally just from using Satellites. It's been astounding.\" - Ses Goe, Director of Internal Tools at NOCD\n\n## Why Earthly\n\nNOCD chose Earthly and Earthly Cloud over their prior solution and other alternatives for a few key reasons:\n\n1. **Automatic Caching:** Earthly provides an efficient caching mechanism that significantly reduced build times for NOCD with no extra configuration required.\n2. **Reduced Latency in CI:** Earthly Satellites eliminated the cache download to and upload from AWS that was required on every CI run with their previous inline caching solution and was the primary source of their CI latency issues.\n3. **Flexibility and Ease of Integration:** Earthly fit seamlessly into NOCD's existing workflows. They were able to utilize Earthly with both their old CI – GitHub Actions – as well as through their transition to their current CI – Semaphore,  and they now use Earthly Satellites with Semaphore as well.\n4. **Simplicity and Stability:** Earthly provided a straightforward and reliable solution with little required maintenance. NOCD's Earthfiles often don't change for 6 months or longer.\n\n\"Every time I have a conversation with an Earthly employee, it's like you guys are just headed for the moon, man. There's so much opportunity for improvement, and you guys are building around all the primitives of BuildKit itself. So for any further cache improvements you come up with, it's just gonna benefit everybody. It's super cool to see.\" - Ses Goe, Director of Internal Tools at NOCD"
    },
    {
        "instruction": "Write a detailed article on the topic of diluting DORA (DevOps Research and Assessment) metrics. Include sections on what DORA metrics are, common issues with their implementation, and strategies for ensuring accurate and meaningful use of these metrics. Use a conversational tone to make the content accessible and engaging for readers interested in DevOps practices. Mention any specific examples or case studies discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in DevOps and performance measurement.",
        "input": "",
        "output": "Marketers and consultants are scummy. They try to make you think they have the answers to the problems you and your business are running into by telling half-truths that have a foundation in technical best practices, but they don't actually have the answers to these complex problems.\n\nI have a somewhat unique and fairly thorough understanding of how marketers and consultants do this, how they take useful technical best practices and bastardize them for their own gain. I have a Computer Science degree and started my career as a computer programmer. I moved up into a system administrator role – the spiritual predecessor to DevOps roles. Eventually, I decided to get an MBA and worked for Deloitte as a senior consultant and eventually manager on tech projects afterward. After Deloitte, I moved into product marketing, working at DirecTV and then at several tech companies including New Relic, Twilio Segment, and, now, Earthly. I have been the user, the buyer, the marketer, and the consultant at different points in my career. I've seen this pattern repeated over and over.\n\nIn this post, I'm going to focus specifically on [DevOps Research and Assessment (DORA)](https://dora.dev/), what it is, why it matters, how marketers and consultants abuse it, and the negative impacts that abuse causes.\n\n## Understanding DORA\n\n\\\n\nDORA started as a collaboration between a small group of researchers and [Puppet](https://www.puppet.com/) – one of the first DevOps tools on the market. They researched and gathered data from a wide swath of companies about their software development and delivery processes, producing the State of DevOps Report – one of the earliest and by far the most trusted reports on DevOps – from 2014 to 2017. The 2018 State of DevOps Report was noticeable sans Puppet but had a new diamond sponsor, Google. The company eventually acquired DORA in 2019, and they have continued to research and publish the State of DevOps Report every year since under the Google Cloud and DORA banners.\n\n> You can learn more about DORA's history in co-founder Jez Humble's 2019 blog post, [\"DORA's Journey: An Exploration\"](https://medium.com/@jezhumble/doras-journey-an-exploration-4c6bfc41e667).\n\nFrom the very earliest State of DevOps Reports, one of the primary takeaways is that companies with better IT performance are more productive, more profitable, and have a higher market share.\n\n\"Our data shows that IT performance and well-known DevOps practices, such as those that enable continuous delivery, are predictive of organizational performance. As IT performance increases, profitability, market share and productivity also increase.\" - [2014 State of DevOps Report](https://dora.dev/publications/pdf/state-of-devops-2014.pdf)\n\nThe report included three metrics used to measure IT performance:\n\n1. Deployment frequency\n2. Lead time for changes\n3. Mean time to recover from failure\n\nDORA has since expanded this to four metrics:\n\n1. Deployment frequency\n2. Lead time for changes\n3. Time to restore service\n4. Change failure rate\n\nEach State of DevOps report includes a lot of information and research on what it takes to meaningfully improve these metrics, including tooling that helps measure and/or improve these metrics and a heavy emphasis on the organizational culture needed to perform well and improve. It looks at large systemic changes that teams can make to help improve IT performance and encourages measuring that performance according to this set of metrics.\n\nIt's difficult to explain how impactful DORA's approach to DevOps and especially the DORA metrics have been. Every company that makes Observability (previously known as Monitoring) tooling anchors on the DORA metrics. Almost every best practice regarding DevOps anchors on DORA metrics. It's difficult to find individual success stories, because the use of DORA metrics is so widespread now that no one success story seems like a big deal. Every success story on every observability or DevOps tool website is a story of the success of DORA.\n\nThis success and the trust that DORA has gained, especially the success of the DORA metrics, has made it a ripe vehicle to disguise selling snake oil and bullshit. The DORA metrics seem like a straightforward path to success, but, if you read the reports beyond just the metrics, it clearly isn't. The path to improving your company's DORA metrics was never prescriptive in any of the reports, and that's because it usually involves deep cultural change in an IT or Engineering organization. There is no cookie-cutter approach to organizational change. It's difficult and unique to each organization. Having simple, easy-to-measure metrics as goals and an ambiguous path to get there leaves a lot of room to create imaginary solutions and sell people things that may or may not help them.\n\n## How Marketers Abuse DORA\n\n### Not All Marketing Is Bad\n\n\\\n\nI'm going to give marketers a lot of credit for popularizing useful technical best practices like those espoused by DORA. Make no mistake that Puppet's involvement in the original and first few State of DevOps Reports wasn't wholly altruistic. Those reports were incredibly strong validation that the processes Puppet promoted and the tools that they built could make your IT operations better and, in turn, improve your business performance. And they helped produce the reports, not just as a sponsor from my understanding. So of course they were going to use these reports for marketing and sales purposes. There's nothing wrong with that. I consider this an example of good marketing – spreading knowledge and best practices that are not specific to the product your company makes and then showing how your product helps enable those best practices.\n\nMarketers do a lot of good things that follow this pattern. They create and sponsor reports, like Google prior to acquiring DORA and like CircleCI, GitLab, Deloitte, and many others that sponsored the most recent State of DevOps Report. They create and sponsor organizations and events – such as the [Cloud Native Computing Foundation](https://www.cncf.io/) and [KubeCon](https://www.cncf.io/kubecon-cloudnativecon-events/). They spend time and money doing a bunch of other things that are beneficial and also work as marketing.\n\nWhen I was the product marketer for the open source program at New Relic, I put together and the company executed a marketing program that sponsored the [Oregon State University Open Source Lab](https://osuosl.org/), made a grant to [freeCodeCamp](https://www.freecodecamp.org/) for them to create a [course on OpenTelemetry](https://www.youtube.com/watch?v=r8UvWSX3KA8) (that has been viewed 190k times, nice!), sponsored multiple tech podcasts, and started the company's GitHub Sponsors program that has now [sponsored over 70 developers and organizations](https://github.com/orgs/newrelic/sponsoring) (do you have any idea how difficult it is to get a corporate procurement manager to give you approval to spend $5K on donations to individuals through one of GitHub's then-beta features?). Every item on that list is a good thing in my opinion. Every item on that list also works as marketing.\n\n### Where Marketers Go Bad\n\n\\\n\nWhere marketers go bad is when they start trying to make you believe that the product they're pitching will solve problems it can't. For example, if an observability tool vendor claims that their tool can increase your deployment frequency, reduce your lead time for changes, reduce your time to restore service, and/or reduce your change failure rate, those marketers are promising solutions to problems their tool couldn't possibly solve. I don't think any observability vendors make claims like this anymore, because they understand it's ludicrous.\n\nThe more common thing you'll see marketers do in this vein is to create \"marketing solutions\" that promise to solve difficult business problems by following a set of handy steps, but, in reality, the problems are much more complicated than a few standard steps can solve. The insidious thing about the marketing solutions is that a lot of them are legit. Determining if one is worth your time or not isn't easy though. You have to figure out if a marketing solution is selling the answer to a technical problem or an organizational one. If it solves a technical problem – e.g. how to use our tool on AWS – there's a good chance it's legit. If it solves an industry-specific, non-organizational problem – e.g. how to deploy a managed version of our tool into a private VPC for industries with high security requirements – there's a good chance it's legit.\n\nIf a marketing solution promises to solve an organizational problem, if it solves a technical problem but a non-trivial portion of it is devoted to organizational problems, or if any part of the solution relies on assisted or unassisted organizational change, don't believe what they are selling. These types of solutions are frequently more marketing hype than value. They promise to solve a big, organization-wide problem by following a specific set of steps that the vendor has totally tried out with other customers and not just made up. Sometimes they even promise (or at least hint) that their solution will create the organizational change required to solve these organization-wide problems. These solutions are rarely truly successful, but they require a lot of professional services hours and generate a lot of revenue anyway.\n\nFor DORA, this exhibits itself by observability tool vendors creating solutions for things like DevOps or Cloud Adoption – things that require organizational change. They use DORA's State of DevOps Report and the DORA metrics to shape the steps in their solutions and the pitches they make to customers.\n\nThe result is frequently a hefty amount of instrumentation that leaves the customer more locked into the vendor's tool (although this is getting less prevalent with OpenTelemetry), a bag of dashboards that show DORA metrics alongside a smattering of other metrics the vendor's solution deems important, and a fat bill.\n\n## How Consultants Abuse DORA\n\n### Consultants Are Smart, and Sometimes You Need Them\n\n\\\n\nI'm not as generous in my assessment of consultants as I am in my assessment of marketers. This is from my personal experience being both a tech consultant at a major consultancy and a marketer at companies of all sizes, from early-stage startups to huge conglomerates. Consultants do much less good than marketers, and they build their entire business around solutions, the same area that marketers run afoul of abusing technical best practices.\n\nSo here's the part where I tell you the pros of consultants. They sponsor a lot of the same technical reports, organizations, events, content creators, etc., as marketers. They also produce a lot of research-heavy reports that have useful info (unfortunately, they also produce a lot of sales pitches disguised as reports). Consultants probably do the best job of spreading technical best practices across industries, because they jump around industries so often. Sometimes you need a consultant because you need an outside perspective, you want advice from people with experience in areas your team is lacking, or you don't want to hire people full-time for temporary roles for work that is project-based or non-recurring. I can also say with certainty that my co-workers when I was a consultant were, for the most part, some of the most intelligent, well-rounded, well-composed people I've ever met in my life. If you need to put together an Academic Decathlon team or want to get the best tutors to get your child through high school and college with straight A's, consultants are the way to go.\n\n### Where Consultants Go Bad\n\n\\\n\nConsultants go bad the same way marketers go bad, with solutions. That's all consultants sell though. Sometimes it's a fully-composed solution with defined steps, sometimes it's a hand-wavey framework that is used as a guide to a project, sometimes it's architecting and building completely custom systems. Consultants have made an industry out of the same practices that stain marketers' reputations, trying to make you believe that the solution they're pitching will solve your problems, regardless of whether it can or can't.\n\nA consultant is like a marketer cross-bred with a 1980s professional wrestler who's amped through the roof on amphetamines and anabolics. They're on the road non-stop for years, say whatever they need to earn a buck, have questionable ability to execute on what they promise, and frequently end up divorced and/or in rehab before they're forty. This lifestyle, much like that of professional wrestlers, often leads to short-term decision-making, both for them and their clients. Consultants generally only last two to four years at top firms too (they are pretty terrible jobs), and they have aggressive sales and utilization quotas to hit. Sometimes, closing a $500K contract at the end of the sales year for less than necessary work will get them an extra $5K in their annual bonus this year, which very well could be worth more than an additional $20K on your bonus next year, because you can never tell if you'll be there next year as a consultant. Once again, consultants are smart. They understand their incentive structure and how to work towards their incentives. Sometimes that leads to bad behavior like selling useless work or prioritizing short-term gains over long-term relationships with their customers.\n\nFor DORA, this exhibits itself by consulting companies creating drivel like this from McKinsey, [\"Yes, you can measure software developer productivity\"](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/yes-you-can-measure-software-developer-productivity) (the inspiration for this post). McKinsey is far from the only consulting company to push solutions like this one, ostensibly for the technology, media, and communications industries. Every consulting company has some version of this solution that they sell. This one takes DORA metrics, adds in [SPACE metrics](https://queue.acm.org/detail.cfm?id=3454124) to measure developer productivity (developed by GitHub, Microsoft Research, and the University of Victoria), and then adds in some custom McKinsey-curated metrics that are supposed to \"... identify what can be done to improve how products are delivered and what those improvements are worth, without the need for heavy instrumentation.\"\n\nAre your BS meters beeping like crazy right now? Let's see, this solution promises to solve an organization problem, requires a large number of metrics that you need to collect for it to function properly, and you have no good way of measuring ~50% of them because they can't be instrumented. The beauty of DORA metrics is that they are constrained and they can be measured precisely (through instrumentation). This solution that this consulting company is selling sounds like you're getting a bag of semi-related, occasionally useful metrics with a project that is a nightmare during requirements, drowning in change requests and additional billings, and results in something pretty to look at but not the organizational change you needed.\n\n## Help Stop the Dilution of DORA\n\nDORA's annual State of DevOps Report and the metrics they've developed to improve IT performance are extremely useful and widely adopted. There isn't a ton of argument about the efficacy of improving DORA metrics. There is a lot of disagreement on how to get to the point that your team can efficiently monitor and improve DORA metrics though. That's because it requires organizational change, and organizational change isn't cookie-cutter. Marketers and consultants capitalize on this ambiguity and create solutions promised to improve your DORA metrics. No matter what they say though, their solutions will never be able to consistently cause the organizational change promised by them. The organizational change required for DevOps to thrive and for DORA metrics to be their most useful has to come from within an organization. You can't outsource it or ask an outsider to do it for you, because the hard part isn't the right tool or metric, but the people and processes.\n\nSo, what can be done to help prevent this recurrent bastardization of technical best practices? I could make an impassioned plea to all of the decent, honest marketers and consultants out there (the majority of both are decent and honest), but that wouldn't get the job done. It doesn't matter how honest and decent marketers and consultants are, their behavior will follow their incentives, and their incentives are too perverse. The sale is what matters most to both of these groups as a whole, not the efficacy of the solutions they provide. So asking these groups or even individuals in these groups to change their behavior is pointless.\n\n\\\n\nIt's on you, the technical professionals – developers, DevOps engineers, SREs, engineering managers, all the way up to CTOs – to remove the incentive marketers and consultants have to develop BS solutions. Be critical about research, reports, or other content developed or contributed to by marketers and consultants. Ask if the solutions marketers and consultants are trying to sell you require organizational change. If it does, don't buy anything. Ask if they are selling work that isn't transient, that should be done by a full-time employee. If they are, don't buy anything. Ask if the solution they are selling sounds feasible and reasonable. If it doesn't, you should not buy anything and ask a lot more questions. When it comes to marketers and consultants (and, honestly, so many other people in life), be skeptical, be critical, and don't believe anything that sounds too good to be true."
    },
    {
        "instruction": "Write a detailed article on the release of Earthly v0.8. Include sections on new features, improvements, and bug fixes introduced in this version. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific examples or use cases discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in Earthly and CI/CD tools.",
        "input": "",
        "output": "Today, we're announcing the release of Earthly v0.8. Earthly v0.8 introduces new features and improvements, such as Auto-skip (in Beta) for improved performance in monorepos, enhanced ARG handling, improved performance for outputting images from remote runners, new Earthly Satellites instance sizes, and more.\n\n\\\n\nWe do not take major or minor releases lightly at Earthly. We've never had a major release, and our last minor release was almost a year ago, in February 2023 (read about it in [Announcing Earthly v0.7](https://earthly.dev/blog/earthly-v0-7/)). That's because we know that the reliability and stability of your build and CI/CD processes are of the utmost importance. In every minor release of Earthly (and also eventually when we have a major release), all features promoted to General Availability (GA) have finalized APIs and have been through thorough testing. That's why we are comfortable enabling all features promoted to GA by default.\n\n## Auto-Skip: Faster Monorepo Builds (Beta)\n\nAuto-skip, now in Beta, enhances build efficiency, particularly in monorepos where multiple projects are built simultaneously. Auto-skip utilizes a global cache stored in a cloud database and allows Earthly to skip large parts of a build in certain situations. It requires an Earthly Cloud account and can be activated for an entire run, by using `earthly --auto-skip` in the CLI, or specific targets, by using `BUILD --auto-skip` in your Earthfile. Auto-skip operates on an all-or-nothing basis, entirely skipping a target or not, falling back on other caching methods if not skipped. For more details, visit the [Auto-skip documentation](https://docs.earthly.dev/docs/caching/caching-in-earthfiles#id-3.-auto-skip).\n\n## Enhanced ARG Handling with `LET`/`SET` and ARG Scoping\n\nEarthly v0.8 brings a new level of control and flexibility to build arguments. It introduces the `LET` and `SET` commands, allowing for the declaration of local variables (`LET`) and the ability to change their values (`SET`). This version also includes ARG scoping, meaning `LET` takes precedence over `ARG` and `ARG` takes precedence over `ARG --global`. These additions provide more precise control over variables in your builds.\n\n### Example Using `LET` and `SET`\n\n~~~{.dockerfile caption=\"Earthfile\"}\nVERSION 0.8\nFROM golang:1.21-alpine\nWORKDIR /go-workdir\n\n…\n\n# start-ui runs a frontend container to handle browser traffic.\nstart-ui:\n    # mode sets the build mode for the UI. In development mode, \n    # it will only listen on HTTP port 8000. In production mode, \n    # it will listen on TLS and HTTP, using common HTTP and TLS \n    # ports, and handle TLS certificates using certbot.\n    ARG mode = \"production\"\n    # frontend_host sets the hostname that the frontend listens on. \n    # Ignored if\n    # $mode != production.\n    ARG frontend_host = ignored\n    # proxy_host sets the hostname that the frontend connects to in order \n    # to proxy requests to the backend. Ignored if $mode != production.\n    ARG proxy_host = ignored\n    # autocert_dir sets the directory that will be mounted for caching \n    # autocert certificates. Ignored if $mode != production.\n    ARG autocert_dir = ignored\n\n    BUILD +require-ca\n\n    WAIT\n        BUILD ./ui+image --mode \"$mode\"\n    END\n\n    LET frontend = \"$frontend_host\"\n    LET proxy = \"$proxy_host\"\n    LET port_args = \"--publish ${tls_port}:4043 \\\n    --publish ${certbot_port}:8000\"\n    LET server_scheme = \"https\"\n    IF [ \"$mode\" != \"production\" ]\n        SET frontend = localhost\n        SET proxy = localhost\n        SET port_args = \"--publish ${tls_port}:8000\"\n        SET server_scheme = \"http\"\n    END\n    BUILD +start-ui-internal --mode \"$mode\" --frontend \\\n    \"$frontend\" --proxy \"$proxy\" --server_scheme \"$server_scheme\" \\\n    --autocert_dir \"$autocert_dir\" --port_args \"$port_args\"\n\n…\n~~~\n\nFor more details, visit the [`LET`](https://docs.earthly.dev/docs/earthfile#let) and [`SET`](https://docs.earthly.dev/docs/earthfile#set) documentation.\n\n## Improved Performance for Remote Runners\n\n\\\n\nOutputting images from a remote runner, such as Earthly Satellites, has improved performance as it no longer transfers layers that are already present locally. Previously a full tar.gz was transferred which contained all layers in the image. Now, Earthly performs a docker pull which only downloads required image layers.\n\n## New Earthly Satellites Instance Sizes\n\nIn response to growing scalability needs, we're introducing larger Earthly Satellites instance sizes. We previously offered x-small, small, medium, large, and x-large instances. We are now adding 2x-large, 3x-large, and 4x-large instances. These new options offer more powerful resources for your builds. You can find more details about compute sizing and pricing on our [Pricing page](https://earthly.dev/pricing).\n\n## Renaming COMMAND to FUNCTION\n\nEarthly supports creating reusable functions in Earthfiles. Previously, these were named User-defined commands (UDCs). In an effort to enhance clarity around this functionality, we are renaming UDCs to Functions. Correspondingly the `COMMAND` keyword has been renamed to `FUNCTION` and should be updated in your Earthfiles.\n\n## Farewell to `v0.5`\n\nEarthly v0.5 was released on February 1, 2021, and after nearly 3 years, support for it is being phased out. If you are still using Earthly v0.5 or `VERSION 0.5` in your Earthfiles, upgrade soon.\n\n### Upgrading To Earthly `v0.8`\n\nFollow these steps to upgrade to Earthly v0.8 safely:\n\n1. If you are still using `VERSION 0.5` in your Earthfiles, upgrade to `VERSION 0.6` or `VERSION 0.7`.\n2. Upgrade your Earthly binary to v0.8 in CI and locally, across your team. The v0.8 binary can run both `VERSION 0.6` and `VERSION 0.7` Earthfiles.\n3. Once everyone is using the Earthly v0.8 binary, upgrade your Earthfiles one-by-one to `VERSION 0.8`. It is ok to have a mix of `VERSION 0.6`, `VERSION 0.7`, and `VERSION 0.8` Earthfiles in the same project.\n\nNote that when upgrading between versions, you may encounter backwards-incompatible changes. Check out the change log of each version for more information: [v0.8](https://github.com/earthly/earthly/releases/tag/v0.8.0), [v0.7](https://github.com/earthly/earthly/releases/tag/v0.7.0), [v0.6](https://github.com/earthly/earthly/releases/tag/v0.6.0), [v0.5](https://github.com/earthly/earthly/releases/tag/v0.5.0).\n\n## All Features Promoted to GA in `v0.8`\n\n|Feature Flag    |Description|\n|-|-|\n|--no-network    | Allow the use of `RUN --network=none` commands|\n|--arg-scope-and-set    | Enable the `LET` / `SET` commands and nested `ARG` scoping|\n|--use-docker-ignore    | Enable the use of `.dockerignore` files in `FROM DOCKERFILE` targets|\n|--pass-args    | Enable the optional `--pass-args` flag for the `BUILD`, `FROM`, `COPY`, and `WITH DOCKER --load` commands|\n|--global-cache    | Enable global caches (shared across different Earthfiles), for cache mounts and `CACHE` commands having an ID|\n|--cache-persist-option    | Adds `CACHE --persist` option to persist cache content in images, Changes default `CACHE` behaviour to not persist|\n|--use-function-keyword    | Enable using `FUNCTION` instead of `COMMAND` when declaring a function|\n|--use-visited-upfront-hash-collection    | Switches to a newer target parallelization algorithm|\n\nFor more information on the individual Earthfile feature flags see the [Earthfile version-specific features page.](https://docs.earthly.dev/docs/earthfile/features)\n\nFor more detailed information on Earthly v0.8, read the [Earthly v0.8 release notes](https://github.com/earthly/earthly/releases/tag/v0.8.0).\n\n## Thank You to the Earthly Community\n\n\\\n\nHuge thank you to the Earthly community! From the 1,500+ developers that have signed up for [Earthly Cloud](https://cloud.earthly.dev/login) to the 10K+ that have starred us on [GitHub](https://github.com/earthly/earthly) to our 1,200+ member community Slack ([click to join](https://earthly.dev/slack)), you all have played an instrumental role in the development of Earthly v0.8. Your code contributions, feature requests, and feedback made this release possible. We are so grateful to have such a dedicated and supportive community of developers from all over the world that believe in Earthly and want to make it better."
    },
    {
        "instruction": "Craft an announcement for a tech company named \"Earthly\" about the launch of three new larger instance sizes (2xlarge, 3xlarge, 4xlarge) for their product called Earthly Satellites. Include details on the specifications of these instances such as vCPUs, RAM, and cache disk space available in both x86 and arm64 architectures. Emphasize how these new instances are designed to provide more compute power, flexibility, and better build experiences. The tone should be enthusiastic and customer-focused, highlighting user feedback as a driving force behind the product's evolution. Lastly, encourage users to sign up and try out the new features while providing links for pricing details and to Earthly Cloud.",
        "input": "",
        "output": "At Earthly, we're constantly trying to make builds more consistent, faster, and give our users a better build experience. Today, we're thrilled to announce the launch of three new, larger Earthly Satellites instance sizes – 2xlarge, 3xlarge, and 4xlarge – available in x86 and arm64 architectures. These new instance sizes are designed to give Satellites users more compute power, more RAM, more cache storage, and more flexibility in choosing the right satellites for their builds.\n\n### A Closer Look at the New Satellite Instance Sizes\n\n\\\n\n- **2xlarge:**  16 vCPUs, 64 GB of RAM, and a 500GB cache disk. x86 & arm64 available.\n- **3xlarge:**  32 vCPUs, 128 GB of RAM, and a 1TB cache disk. x86 & arm64 available.\n- **4xlarge:** 64 vCPUs, 256 GB of RAM, and a 2TB cache disk. x86 & arm64 available.\n\n_[Visit our Pricing page](https://earthly.dev/pricing#compute-pricing) for more details about the Satellites instance sizes we offer._\n\n## From Feedback to Features\n\n\\\n\nOur new Satellites instance sizes are an upgrade, but they're also a sign of our commitment to our users. Our previous instance sizes – x-small through x-large – were insufficient for some Satellites customers. We heard that feedback and expanded our offerings to help ensure that Earthly Satellites remains the go-to choice for every build.\n\nAs we continue to mature and expand our platform, your feedback and insights remain invaluable. [Sign up for Earthly Cloud](https://cloud.earthly.dev/login), explore the new Satellites instance sizes, put them to the test, and let us know how they work for you."
    },
    {
        "instruction": "Write a detailed article announcing Earthly's self-hosted Satellites. Include sections on what self-hosted Satellites are, their benefits, and how to set them up and use them effectively. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific configurations or best practices discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in CI/CD tools and self-hosting solutions.",
        "input": "",
        "output": "I remember the first application that I self-hosted, [BugTracker.NET](https://ifdefined.com/bugtrackernet.html). It's ancient now, but this was in 2009. I had just landed my second job out of undergrad. I went from being an incredibly underpaid computer programmer focusing on one of Microsoft's now-legacy ERP systems, Dynamics AX, and development on top of Microsoft SQL Server to being a system administrator for that same ERP system for a new company at more than double the pay.\n\nWhen I landed in this new role, they didn't have a bug/feature tracking system. They had a helpdesk ticketing system that the infrastructure team used, but it was insufficient for what I needed, and the licensing scheme didn't allow end-users to enter tickets themselves. I didn't have the budget for a bug/feature tracking system either, and I wasn't going to get it. The company was investing $2M over two years into the ERP system I was responsible for implementing. This was a huge spend for them. There was no chance I was getting another $10K or whatever the price was going to be for a bug/feature tracking system. So I went searching for solutions to my problem, and BugTracker.NET was the best option I found.\n\n### Self-Hosting Wasn't Easy, but It Provided Real Value\n\nI was extremely familiar with open source software by this point. I'd been installing Linux on every PC I pieced together from garage sale finds since I was 13 or 14 (I Frankenstein'd together a lot of PCs from salvaged parts). I had never self-hosted anything at that point though, and I'd never brought an open source tool into a company I had worked for. I needed it though. The business needed it too, they just didn't know it.\n\nSo I set it up. I spun up a VM, configured IIS, installed all the prerequisites, created a new database in one of our non-production SQL Server instances, and installed BugTracker.NET. It wasn't exactly straightforward, and it required a lot of resources we happened to have \"lying around\" (e.g. a server available with extra space for the VM, extra license keys for Windows Server, and a barely used, non-production SQL Server instance), but I got it working within a day.\n\nWe used self-hosted BugTracker.NET for the following three years I was at this company (they probably used it after I left). It became the de facto standard for bug/feature tracking outside of the infrastructure team. It solved an important problem for us and provided incredible value to the business. After that, I was sold on self-hosting. Sure it was a pain to get everything configured, but it was worth it.\n\nAfter Docker came out, the number of applications I personally self-hosted ballooned. Docker lowered the barrier to entry for self-hosting a lot. I didn't have to spin up a VM and configure a web server for each application. Now I just have one web server running with Docker installed. Self-hosting an application is as easy as creating a new database in one of my Postgres instances, launching a container with the application's image, and adding it to the server's reverse proxy. This has allowed me to experiment with a lot of self-hosted applications. Right now, I have one VPS where I self-host [Firefly III](https://www.firefly-iii.org/), [Bitwarden](https://bitwarden.com/), [Nextcloud](https://nextcloud.com/), and [Plex](https://www.plex.tv/), all with Docker.\n\n### Self-Hosting Is Problem-Solving\n\nSelf-hosting, for me, centers around one thing: problem-solving with software. I have something I want to do or an issue I keep running into. I find open source software that does what I need and self-host it. My problem is solved.\n\nSometimes the problems you need to solve go beyond the capabilities of the software though. Sometimes the problem is that I have multiple options and don't know which piece of software to use. Self-hosted software (along with SaaS free tiers) makes trying things out and making a decision easier. Or maybe the problem is that I can't have a piece of software operating in a vendor's cloud, it needs to be in mine, whether for purposes of security or for hardening production environments. In that case, self-hosted is almost the only option.\n\n## Announcing Self-Hosted Earthly Satellites\n\n\\\n\nHere at Earthly, I'm a marketer, but the way I choose to market is to focus on the problems we can help our users and customers solve. I know a lot of you out there have a VPS running Plex or a homelab, and I want you to be able to use Earthly Satellites and see how awesome it is. And – maybe I'm not supposed to say this part out loud – if some of you self-host Earthly Satellites for a side project, you'll like it, and you might bring it into work (and that's where we get paid).\n\nWe are announcing the availability of self-hosted Earthly Satellites. Earthly Satellites are remote build runners that work natively with Earthly. They are also the primary service included with Earthly Cloud. Up to now, Earthly Satellites were only available as a fully managed service hosted in Earthly's infrastructure. This lets you bring Earthly Satellites into your ecosystem, have them run on your infrastructure, and still use the same Earthly Cloud UI and native Earthly CLI integration that our managed and hosted Satellites offer. So your build execution, build artifacts, and even your code never leave your hardware or your ecosystem.\n\n### Why Choose Self-Hosted Earthly Satellites?\n\n\\\n\n* _Enhanced Security_:  We know that the more complex your application's infrastructure is, the more likely it is that you need CI/CD to run next to production. This is the primary use case for self-hosted Satellites. In addition, if having a highly available application is important to your company, you'll likely want to keep your production environment as isolated as possible. This lets you reduce the variables you don't control that can cause your app to stutter.\n* _Customizable Performance_:  With self-hosted Satellites, you can customize instances for specific hardware and network configurations. So you can make sure each instance has whatever resources your builds need in a much more specific way than with our hosted Satellite sizes. If you need more cache storage, you can add more cache storage without upgrading to a larger machine.\n* _Use Cloud Vendor Credits_:  Since self-hosted Satellites are in your environment using your cloud infrastructure, you can use the credits you already have with your cloud vendor to pay for compute. This frequently makes getting approval to use a tool easier and, depending on the discount you have with your cloud vendor, could be less expensive than using hosted Satellites.\n* _Full Control_:  You can manage and update your self-hosted Satellites according to your schedule and requirements. If your business has its own scheduled maintenance windows, you can update your self-hosted Satellites then and not be beholden to our SaaS maintenance windows.\n* _Automatic TLS Key Management_:  We automatically manage and rotate TLS keys for hosted Satellites. It's a basic requirement that Earthly Satellites has always satisfied. Self-hosted Satellites are no different. Your and everyone on your team's TLS keys will be automatically generated and rotated just like they are for hosted Satellites. So you never have to think about encryption with your builds nor the process that everyone on your team has to follow to get it right. It's done for you automatically.\n* _Seamless Integration_:  Self-hosted Satellites integrate seamlessly with Earthly and Earthly Cloud. When you launch a self-hosted satellite, it automatically registers with Earthly Cloud and is ready to run builds from the command line. If you want to run a build on a self-hosted satellite, you use the same commands that you would on an Earthly-hosted satellite.\n* _Consistent, Fast Builds that Work with Any CI_:  With self-hosted Satellites, you still get all the benefits you expect from Earthly:  write once, run anywhere builds that work the same on your laptop as they do in CI; ridiculous build speed from Earthly's automatic caching and parallel execution; and compatibility with any CI.\n\n### Getting Started with Self-Hosted Earthly Satellites\n\n\\\n\nLaunching a self-hosted satellite is as easy as running the public Docker image. A few environment variables are needed to link the satellite with your Earthly Cloud account. The satellite will use these values to automatically register it with the Earthly Cloud control plane. Note that Earthly Cloud will never connect to your satellite directly. So the host and port of your satellite can be private to your internal network or VPC.\n\nHere is a minimal command to start a self-hosted satellite:\n\n~~~{.bash caption=\">_\"}\ndocker run --privileged \\\n    -v satellite-cache:/tmp/earthly:rw \\\n    -p 8372:8372 \\\n    -e EARTHLY_TOKEN=[your_token] \\\n    -e EARTHLY_ORG=[your_earthly_cloud_org] \\\n    -e SATELLITE_NAME=[your_satellite_name] \\\n    -e SATELLITE_HOST=[your_satellite_hostname_or_ip] \\\n    earthly/satellite:v0.8.3\n~~~\n\nThe following environment variables are required:\n\n* `EARTHLY_TOKEN` - a persistent auth token obtained by running `earthly account create-token [token-name]` from the Earthly CLI.\n* `EARTHLY_ORG` - the name of your organization created in [Earthly Cloud](https://cloud.earthly.dev/).\n* `SATELLITE_NAME` - a name you choose to identify the satellite instance.\n* `SATELLITE_HOST` - Hostname or IP address of your satellite for the Earthly CLI to connect (note that users will select the satellite by name when running builds).\n\nOnce your satellite is launched and has finished registering with the Earthly Cloud control plane, it can be selected and managed using typical Earthly Satellite commands from the CLI. For example, to select the satellite for persistent use in subsequent builds:\n`earthly sat select [your_satellite_name]`\n\nOr, to invoke a build once on the satellite, without having it persistently selected:\n`earthly --sat [your_satellite_name] +your-target`\n\n_For the complete guide, visit [the Self-Hosted Satellites page in our documentation](https://docs.earthly.dev/earthly-cloud/satellites/self-hosted) ._\n\n## Start Self-Hosting Earthly Satellites Today\n\n\\\n\n[Sign up for Earthly Cloud](https://cloud.earthly.dev/login) to start self-hosting your own Earthly Satellites. They are even available on our free tier, so you can start with self-hosted Satellites now at no cost ([visit our Pricing page](https://earthly.dev/pricing#compute-pricing) for more details about pricing for self-hosted Satellites). Try out self-hosted Earthly Satellites, and let us know how they work for you."
    },
    {
        "instruction": "Write an announcement blog post for Earthly about achieving SOC 2 Type 2 compliance. Include details on the significance of this achievement, what SOC 2 is, and explain the difference between SOC 2 Type 1 and Type 2 compliance. Also, mention how this impacts Earthly Cloud users and their trust in Earthly's security measures. Lastly, provide a way for interested parties to request a copy of the report. Tone should be professional yet accessible, highlighting Earthly’s commitment to data security and user trust.",
        "input": "",
        "output": "We take security very seriously here at Earthly. As a user, you shouldn't have to worry about a SaaS vendor being sloppy with your business' data. We have an information security program that has been in place and communicated throughout our organization for quite some time now, and we achieved SOC 2 Type 1 compliance in April 2023 ([Read our SOC 2 Type 1 announcement](https://earthly.dev/blog/soc-type-one/)). This announcement is quite late, but we are still excited to announce that Earthly achieved SOC 2 Type 2 compliance on October 26, 2023.\n\n## What's the difference between SOC 2 Type 1 and Type 2?\n\n[Service Organization Control 2](https://www.aicpa-cima.com/topic/audit-assurance/audit-and-assurance-greater-than-soc-2) (SOC 2) is important for assessing and verifying the controls around the security, availability, processing integrity, confidentiality, and privacy of systems that process user data. This is particularly significant for SaaS providers like Earthly.\n\nThere are two types of SOC 2 compliance:\n\n* **SOC 2 Type 1** examines and validates the design and implementation of an organization's controls at a single point in time. This assessment is akin to a snapshot, providing assurance that at a specific moment, the organization had appropriate mechanisms in place to safeguard data in accordance with SOC 2 standards.\n* **SOC 2 Type 2**  goes a step further by evaluating how effectively these controls are maintained and operated over an extended period, typically 6 to 12 months. This type of compliance is more dynamic and ongoing, offering a deeper, more continuous insight into the organization's operational effectiveness. It requires the organization to not only establish but also consistently apply its controls over time, demonstrating a sustained commitment to data security and compliance.\n\n## What Does This Mean for Earthly Cloud Users?\n\nOur SOC 2 Type 2 compliance doesn't impact the functionality of Earthly Cloud (or Earthly Satellites). Both will continue to work exactly as they always have and as our users expect.\n\nFrom a non-functional standpoint, SOC 2 Type 2 compliance helps assure our users that they can rely on Earthly Cloud (and Earthly Satellites) for continuous business operations and that downtime and potential disruptions are minimized. It also helps assure users that risks associated with data breaches, unauthorized access, and data loss are mitigated. Also, many industries and companies have stringent regulatory requirements that require SOC 2 Type 2 compliance from SaaS vendors. For those users, it means that Earthly meets their security needs and is a valid option for them to consider and choose.\n\n_To request a copy of our SOC 2 Type 2 report, contact [security@earthly.dev](mailto:security@earthly.dev) ._"
    },
    {
        "instruction": "Explain the key benefits of Earthly and Earthly Cloud in terms of simplicity, consistency, and speed, emphasizing the latest feature called \"Auto-Skip\" that improves build efficiency in monorepo setups. Include details on what Auto-Skip is, how to use it, its current status (beta), and a call-to-action for users to sign up for Earthly Cloud to start using this new feature. Use an enthusiastic tone and provide clear instructions with examples.",
        "input": "",
        "output": "Earthly and Earthly Cloud have a lot of benefits and useful features, but there are three areas where we feel that, if we knock it out of the park, developers will love our tools, and we will succeed as a business.\n\n1. **Simplicity.** If Earthly is easy to use, then the barrier to entry is lower and developers will be more likely to use it. Our syntax being a very familiar mixture of Dockerfile and Makefile makes Earthly easy to use. The fact that we work with every language, framework, and build tool also makes it easy to use. So does the fact that it works with any CI. The less you have to think about and research how to use or implement Earthly, the better.\n2. **Consistency (or Repeatability).** Since Earthly containerizes builds, they can run anywhere Docker can run. So if it runs correctly on your computer, you know that it will run correctly on your colleagues' computers, and you know it will run correctly in CI. With Earthly, builds are self-contained, isolated, consistent, and portable. This makes build failures across teams due to differing build environments a thing of the past. It also means you can easily reproduce and debug CI failures locally instead of having to commit and push changes to your repo over and over again just to test CI.\n3. **Speed.** Builds are consistently slower than they should be. Every time a build runs, the same commands execute, even ones where the results of execution are no different than the last time the build ran. Installing dependencies is a great example. Every time a build runs it installs dependencies, but those dependencies change infrequently. So that's a waste of time. Some CI tools have caching so these superfluous steps can be skipped, but even those solutions are slower than they should be. Since CI runners are almost always ephemeral, they have to download the cache at the beginning of every build and upload it at the end of every build. Even if no steps in the build need to run, this download and upload have to happen. That's a waste of time too. Earthly gives you automatic caching so superfluous steps are skipped, and Earthly Satellites aren't ephemeral, so no cache download and upload is necessary.\n\nThis announcement is related to that third area, speed. We're excited to announce an augmentation to our already stellar caching options. A new layer of caching that makes builds run even faster. We're introducing Auto-Skip.\n\n## What Is Auto-Skip\n\n\\\n\nAuto-Skip allows Earthly to skip large parts of a build in certain situations and is especially useful in monorepo setups, where you are building multiple projects at once, and only one of them has changed. It is a global cache stored in a cloud database that is only available to users of Earthly Cloud.\n\nAuto-Skip is an all-or-nothing type of cache. Either the entire target is skipped, or none of it is. This is because Auto-Skip doesn't know which parts of the target have changed. Instead, it traverses the full Earthly target graph and creates a cache key using a hash of all the graph's values and files. If the cache key matches the last run, it skips the target. If the cache keys don't match, Earthly will fallback to the other forms of caching, [layer-based caching](https://docs.earthly.dev/docs/caching/caching-in-earthfiles#id-1.-layer-based-caching) and [cache mounts](https://docs.earthly.dev/docs/caching/caching-in-earthfiles#id-2.-cache-mounts), to run the build as efficiently as possible.\n\nUnlike layer caching, Auto-Skip can skip `RUN --no-cache` and `RUN --push` commands. This can be useful in situations when you would like to skip a deployment if nothing has changed.\n\n_Note that Auto-Skip is still in beta, so details about it may change slightly, but it is being used without issue by several happy customers as well as in our own repos._\n\n## How to Use Auto-Skip\n\nAuto-Skip is very easy to use. You have to have an Earthly Cloud account and be authenticated in Earthly to use Auto-Skip. There are two ways to invoke Auto-Skip:\n\n1. Auto-Skip can be activated for an entire run by using `earthly --auto-skip` in the CLI.\n2. Auto-Skip can be activated for individual targets by using `BUILD --auto-skip` in your Earthfile.\n\n_[Visit our docs from more information and details about using Auto-Skip](https://docs.earthly.dev/docs/caching/caching-in-earthfiles#id-3.-auto-skip)_\n\n## Sign Up for Earthly Cloud and Start Using Auto-Skip Today\n\n\\\n\n[Sign up for Earthly Cloud](https://cloud.earthly.dev/login) to start using Auto-Skip. It's available to all Earthly Cloud users. Try it out, and let us know how it works for you."
    },
    {
        "instruction": "Explain the new feature `earthly docker-build`, including its benefits, usage examples, and integration with Earthly Satellites. Include details about BuildKit's role in Earthly and provide a brief overview of Earthly's team offsite meetings where this idea was conceived. Make sure to mention how developers can start using Dockerfiles with Earthly without needing to write Earthfiles immediately. Also, include links to further reading on BuildKit and detailed documentation for `earthly docker-build`.",
        "input": "",
        "output": "Earthly has a lot of benefits and useful features, all built on a foundation provided by [BuildKit](https://github.com/moby/buildkit). If you're unfamiliar with BuildKit, it's a tool designed to enhance the process of building container images. It optimizes build performance through parallel processing and efficient caching, significantly reducing build times. BuildKit is the default builder for Docker but supports a variety of frontends, which is how Earthly plugs into it and uses it in the execution of Earthfiles.\n\n_Learn more about BuildKit in our blog post, [What is Buildkit?](https://earthly.dev/blog/what-is-buildkit-and-what-can-i-do-with-it/)._\n\nWe have team offsites every six to nine months. At our offsites, we discuss how business and product development are going, figure out what we're going to build next, and set some goals for our company. A few months ago at our last offsite, we had an idea… I bet it'd be pretty easy to add a feature to Earthly that lets it run good, old-fashioned Dockerfiles. That way, developers who were new to Earthly and wanted to try it out could get value quickly without investing the time and effort into writing Earthfiles. One of our engineers looked into it, and it was pretty easy. So they built it, and now it's available to you.\n\nAnnouncing `earthly docker-build`, a feature of Earthly that lets you run builds using your Dockerfiles, no Earthfiles required. This makes it much easier to have a repo with mixed Dockerfiles and Earthfiles as you start using Earthly, and possibly more importantly, lets you use Earthly Satellites as a persistent remote BuildKit cache for Dockerfile-based builds. So you can get faster Dockerfile-based builds that work with any CI.\n\n\\\n\n## What Is `earthly docker-build`?\n\n`earthly docker-build` is a command built-in to Earthly that lets you build container images from Dockerfiles, very similar to running `docker build`. Since both Earthly and Docker use BuildKit to build container images, you get all the benefits you get with `docker build`, such as parallelism and caching for reduced build times.\n\nThere are two primary benefits that `earthly docker-build` provides:\n\n1. **You can use both Earthfiles and Dockerfiles in CI.** This provides flexibility for your builds. It allows you to start using Earthly early while gradually migrating big projects, using your existing Dockerfiles, and over time, converting them to Earthfiles. You can migrate incrementally, tackling migration of easier parts of your build early and more complex parts later, when you're more familiar with Earthly.\n2. **You can use Earthly Satellites as a persistent remote BuildKit cache.** CI runners are almost always ephemeral. If you're using `docker build` in CI, either your BuildKit cache, one of the most beneficial features of BuildKit, will get blown away at the end of every build, or you'll have to manually configure caching in your CI, which generally requires downloading the cache at the beginning of the build and uploading it at the end of the build, which is slow. Using `earthly docker-build` With Earthly Satellites gives you a persistent BuildKit cache that will speed up every build with no cache upload and download required.\n\n## How To Use `earthly docker-build`\n\nIf you're familiar with Earthly and `docker build`, `earthly docker-build` is very easy to use. If you want to build a Dockerfile of that name in the build context of the current directory, you run `earthly docker-build .`. There are also several options to provide the functionality you're used to with `docker build`.\n\n* **`--dockerfile `** to specify an alternative Dockerfile.\n* **`--tag `** to specify the name and tag to use for an image (multiple `--tag` options are supported for multiple tags).\n* **`--target `** to specify a target in a multi-target Dockerfile.\n* **`--platform `** to specify the platforms to build the image for.\n* **`--push`** to push container images to remote docker registries. With Docker, this requires a separate push command after you build and tag your image.\n\nIf you want to use `earthly docker-build` on an [Earthly Satellite](https://docs.earthly.dev/earthly-cloud/satellites), that's easy too. You just specify the satellite name with `--sat ` directly after the `docker-build` part of the command. For example:  `earthly docker-build --sat my-satellite --tag my-image:latest .`.\n\n_[Visit our docs from more information and details about using `earthly docker-build`](https://docs.earthly.dev/docs/earthly-command#earthly-docker-build)_\n\n## Sign Up for Earthly Cloud and Start Using `earthly docker-build` Today\n\n\\\n\nYou can use `earthly docker-build` with open source Earthly, but if you [sign up for Earthly Cloud](https://cloud.earthly.dev/login) you get 6,000 build minutes per month free on Satellites and the ability to use them as a persistent remote BuildKit cache. Try `earthly docker-build` out, and let us know how it works for you."
    },
    {
        "instruction": "Explain the concept of functions in programming languages and their importance, then introduce Earthly Functions, including how to define them, use them in an Earthfile, and import them from other repositories. Provide examples and direct users to documentation for more details. Include a call-to-action for signing up for Earthly Cloud to start using these features.",
        "input": "",
        "output": "The concept of functions runs deep in software development. Pretty much every programming language has functions or something similar that delivers the same capabilities: a block of code that performs a specific task that you can call from anywhere. Functions are a fundamental part of C. Methods, functions associated with specific classes, are part of almost every object-oriented programming language. Even SQL has functions.\n\nEvery programming language has functions because they are incredibly valuable. They make it easier to make programs modular and code reusable. These same benefits of modularity and reusability are valuable in your builds too. That's why we want to introduce you to Earthly Functions. Functions are exactly what you'd expect, reusable sets of instructions that can be imported into build targets or other functions in your Earthfiles. They are designed to make it easier to make your Earthfiles more modular and your build code less redundant.\n\n## How to Use Functions\n\n\\\n\nFunctions are defined similarly to build targets except that the function name must be in ALL_UPPERCASE_SNAKE_CASE and they must start with `FUNCTION`. For example:\n\n~~~{.dockerfile caption=\"Earthfile\"}\nMY_COPY:\n    FUNCTION\n    ARG src\n    ARG dest=./\n    ARG recursive=false\n    RUN cp $(if $recursive =  \"true\"; then printf -- -r; fi) \"$src\" \"$dest\"\n~~~\n\nYou invoke a function using the `DO` command. For example:\n\n~~~{.dockerfile caption=\"Earthfile\"}\nbuild:\n    FROM alpine:3.18\n    WORKDIR /function-example\n    RUN echo \"hello\" >./foo\n    DO +MY_COPY --src=./foo --dest=./bar\n    RUN cat ./bar # prints \"hello\"\n~~~\n\nFunctions look and are used in a very similar way to build targets. There are a few differences though. Functions inherit the build context and the build environment from the caller. So any local `COPY` operations in a function will use the directory where the calling Earthfile exists; any files, directories, and dependencies created by previous steps of the caller are available to the function to operate on; and any file changes resulting from execution of the function are passed back to the caller as part of the build environment.\n\n_[Visit our docs from more information and details about using functions](https://docs.earthly.dev/docs/guides/functions)_\n\nYou can use functions that are defined in other Earthfiles in your repo or even other repositories. To do this you need to use `IMPORT` just like you would if you were importing build targets from another directory or repository. For an example, I'll be using Earthly [lib](https://github.com/earthly/lib). Earthly lib is a collection of reusable functions that we maintain to be used for common operations in Earthfiles. Here's how to easily mount and cache Gradle's cache using our [Gradle functions from Earthly lib](https://github.com/earthly/lib/tree/main/gradle).\n\n~~~{.dockerfile caption=\"Earthfile\"}\nVERSION 0.8\nIMPORT github.com/earthly/lib/gradle:3.0.2 AS gradle\nFROM gradle:8.7.0-jdk21\n\ndeps:\n    COPY settings.gradle.kts build.gradle.kts ./\n    COPY src src\n    # Sets $EARTHLY_GRADLE_USER_HOME_CACHE and $EARTHLY_GRADLE_PROJECT_CACHE\n    DO gradle+GRADLE_GET_MOUNT_CACHE\n\nbuild:\n    FROM +deps\n    RUN --mount=$EARTHLY_GRADLE_USER_HOME_CACHE --mount=$EARTHLY_GRADLE_PROJECT_CACHE gradle --no-daemon build\n    SAVE ARTIFACT ./build AS LOCAL ./build\n~~~\n\n_[Visit our Earthly lib repo for more pre-built functions and details on how to use them](https://github.com/earthly/lib/tree/main)_\n\n## Sign Up for Earthly Cloud and Start Using Functions Today\n\n\\\n\nYou can use Functions with open source Earthly. The easiest way to get Earthly is to [sign up for Earthly Cloud](https://cloud.earthly.dev/login). It walks you through the process of downloading and getting started with Earthly. Earthly Cloud also gives you 6,000 build minutes per month free on Earthly Satellites. Try Functions out, and let us know how they work for you."
    },
    {
        "instruction": "Write a detailed article on efficiently caching dependencies with Earthly. Include sections on understanding dependency caching, best practices for implementing caching, and specific examples of how to optimize Earthfiles for faster builds. Use a conversational tone to make the content accessible and engaging for readers interested in this topic. Mention any specific commands or configurations discussed in the original post. Ensure the article is comprehensive yet easy to follow, suitable for both beginners and intermediate users interested in CI/CD tools and build optimization.",
        "input": "",
        "output": "One piece of consistent feedback we get from Earthly users is that the caching benefits are real. Users love that Earthly starts caching parts of their build automatically. You don't have to do anything to get it to work, and that will always be the case.\n\nBut you don't have to stop there. Optimizing the structure of your Earthfile can often improve baseline caching and increase speed even more. To do this effectively, it's essential to understand how caching works in Earthly.\n\nMost commands in an Earthfile create a cache layer as part of execution. Each target in an Earthfile is similar to a cake with multiple layers, each new command is a new layer on the top of the cake that is cached for reuse.\n\nWhen a target is executed a subsequent time, Earthly will attempt to reuse as much of the cached \"cake\" as possible. Starting from the bottom of the \"cake\", each cached layer will be reused unless the command's input has changed – input being ARG values, files being COPY'd, or the command itself. Earthly will work its way up the cached layers of the \"cake\" until it runs into a command with input that has changed – a cache bust. At that point, it will execute that command and every command for the layers in the \"cake\" above it.\n\nThe more of the cached \"cake\" we can reuse, the faster our builds get. We want to ensure that, if there's a cache bust and a command must be executed again, it happens after as much execution as possible, generally as late in a build as possible. Because of this, how you structure your Earthfile can significantly impact your build speed.\n\n## Structuring Earthfiles to Efficiently Cache Dependencies\n\n\\\n\n### Copying Files into the Build Context as Late as Possible\n\nLet's say you have a simple Go application with three files:  `go.mod`, `go.sum`, and `main.go`. In this example, the `go.mod` and `go.sum` files are updated locally by running `go mod tidy`.\n\n#### `go.mod`\n\n~~~{.go caption=\"\"}\nmodule github.com/earthly/earthly/examples/go\n\ngo 1.22\n\nrequire github.com/sirupsen/logrus v1.9.3\n\nrequire golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8 // indirect\n~~~\n\n#### `go.sum`\n\n~~~{.go caption=\"\"}\n\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/sirupsen/logrus v1.9.3 h1:dueUQJ1C2q9oE3F7wvmSGAaVtTmUizReu6fjN8uqzbQ=\ngithub.com/sirupsen/logrus v1.9.3/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.7.0 h1:nwc3DEeHmmLAfoZucVR881uASk0Mfjw8xYJ99tb5CcY=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngolang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8 h1:0A+M6Uqn+Eje4kHMK80dtF3JCXC4ykBgQG4Fe06QRhQ=\ngolang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c h1:dUUwHk2QECo/6vqA44rthZ8ie2QXMNeKRTHCNY2nXvo=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n~~~\n\n#### `main.go`\n\n~~~{.go caption=\"main.go\"}\npackage main\n\nimport \"github.com/sirupsen/logrus\"\n\nfunc main() {\n    logrus.Info(\"hello world\")\n}\n~~~\n\nIt would make sense to add an Earthfile like the one below for your build. It has one build target, `+build`, that copies in the required source code, installs dependencies, and builds the application; and another build target, `+docker` that uses the built application from the previous build target and creates a Docker image with it.\n\n#### Earthfile\n\n~~~{.dockerfile caption=\"Earthfile\"}\nVERSION 0.8\nFROM golang:1.22-alpine3.19\nWORKDIR /app\n\nbuild:\n    COPY go.mod go.sum main.go .\n    RUN go build -o output/example main.go\n    SAVE ARTIFACT output/example AS LOCAL local-output/example\n\ndocker:\n    COPY +build/example .\n    ENTRYPOINT [\"/app/example\"]\n    SAVE IMAGE go-example:latest\n~~~\n\nThis works, but it isn't using Earthly's caching as efficiently as it could be. The first step `COPY go.mod go.sum main.go .` copies in all three files at once. If any of these files change, the whole `COPY` command and everything following it has to be re-executed.\n\nInstead, break up the files that you are copying into your build instead of copying them all in at once. Try to move each one, but especially ones that are frequently changed, as far down your Earthfile as you can.\n\n#### Earthfile\n\n~~~{.dockerfile caption=\"Earthfile\"}\nVERSION 0.8\nFROM golang:1.22-alpine3.19\nWORKDIR /app\n\nbuild:\n    # Download deps before copying code.\n    COPY go.mod go.sum .\n    RUN go mod download\n    # Copy and build code.\n    COPY main.go .\n    RUN go build -o output/example main.go\n    SAVE ARTIFACT output/example AS LOCAL local-output/example\n\ndocker:\n    COPY +build/example .\n    ENTRYPOINT [\"/app/example\"]\n    SAVE IMAGE go-example:latest\n~~~\n\nThis change makes it so the infrequently changed `go.mod` and `go.sum` files are copied, and `go mod download` is executed, downloading the dependencies for the build. Then, the more frequently changed `main.go` is copied and the build command is executed.\n\nIn the previous Earthfile, dependencies would be re-downloaded every time `main.go` (or `go.mod` or `go.sum`) changed, underutilizing Earthly's caching. With this new Earthfile, dependencies are only re-downloaded if `go.mod` and `go.sum` change. Otherwise, they are pulled from Earthly's cache.\n\n### Reusing Dependencies\n\nYou may want to do more with your Earthfiles than just build the application and create a Docker image though. You may want to run a preview of the application (which we'll cover in this example) or run tests against a build before the image is published. In these cases, reusing cached dependencies can decrease build times as well.\n\n#### Earthfile\n\n~~~{.dockerfile caption=\"Earthfile\"}\nVERSION 0.8\nFROM golang:1.22-alpine3.19\nWORKDIR /app\n\ndeps:\n    # Download deps\n    COPY go.mod go.sum .\n    RUN go mod download\n\npreview:\n    FROM +deps\n    # Copy and run code.\n    COPY main.go .\n    RUN go run main.go\n\nbuild:\n    FROM +deps\n    # Copy and build code.\n    COPY main.go .\n    RUN go build -o output/example main.go\n    SAVE ARTIFACT output/example AS LOCAL local-output/example\n\ndocker:\n    COPY +build/example .\n    ENTRYPOINT [\"/app/example\"]\n    SAVE IMAGE go-example:latest\n~~~\n\nIn this example, both the `+preview` and `+build` targets are using the `+deps` target as a base. So if the `+deps` target is cached, both `+preview` and `+build` targets will reuse the cached results whenever either is run.\n\nIn addition to more efficiently reusing the cache, this is also a good example of making your Earthfiles more modular. Downloading dependencies is a discrete unit of work, separate from running a preview or building the application. It's required for both, so it's often lumped in with each. This example breaks off the discrete unit of work that is downloading dependencies and puts it in its own build target so that its code and logic can be reused, in the `+preview` and `+build` targets in this example.\n\n## Earthly Makes Your Builds Fast\n\nStructuring your Earthfiles appropriately is just the tip of the iceberg when it comes to Earthly's making your builds fast. Aside from having efficient layer caching, you can dive into using [cache mounts](https://docs.earthly.dev/docs/caching/caching-in-earthfiles#id-2.-cache-mounts), which are particularly useful if you're using a build tool that offers incremental builds. Or you can [sign up for Earthly Cloud](https://cloud.earthly.dev/login) and start using [auto-skip](https://docs.earthly.dev/docs/caching/caching-in-earthfiles#id-3.-auto-skip) to squeeze even more speed out of your builds. Or you can start using [Earthly Satellites](https://earthly.dev/earthly-satellites) to speed up your CI builds. Earthly offers a lot of ways to make your builds faster."
    },
    {
        "instruction": "Write an announcement for a product update on Earthly Cloud's new UI features. Include details about improvements made based on user feedback, such as build timings and graphs, and highlight key sections of the updated interface including the Home Dashboard and New Build Details Screen with its Overview, Timings (with Target Timings and Command Timings sub-tabs), Graph, and Logs tabs. Mention how users can interact with these features and invite them to try out the new UI and provide feedback for future enhancements.",
        "input": "",
        "output": "Over the past several months, we've been working on revamping the UI of Earthly Cloud to better meet the needs of our users. Previously, our interface allowed you to view your satellites, their instance details, and status, and you could see build logs, but we knew there was room for improvement. We recognized the need to include build details and received valuable feedback from our users requesting features like build timings and build graphs. Today, we're thrilled to announce a series of updates and enhancements to the Earthly Cloud UI.\n\n## Home Dashboard\n\n\\\n\nThe Earthly Cloud home dashboard isn't new, but it's a great entry point to Earthly Cloud's UI. It has an Active Builds section with information about each currently running build and a Builds section with all of the builds across your org represented by visual indicators that make it easy to determine if individual builds were successful, failed, or canceled. If you hover your cursor over any of these build icons, additional details about the build are displayed. Clicking any of these build icons will take you to the new Build Details screen which will be detailed later in this post. The dashboard also has a Satellites section listing all satellites in your org, their status, and instance details.\n\n## New Build Details Screen\n\nThe new Build Details screen aims to give you more information about your builds, including what commands were executed, how long different parts of the build took to execute, the build graph, and build logs. It has 4 tabs: Overview, Timings, Graph, and Logs.\n\n### Overview\n\n\\\n\nThe first and default tab is the Overview tab. It is a quick look at the status and performance of a build. It shows details like build status, duration, the percentage of the build that was cached, and a high-level build timeline.\n\n### Timings\n\n#### Target Timings\n\n\\\n\nThe second tab is the Timings tab which has 2 sub-tabs. The first is Target Timings. Target Timings show how long each target run as part of the build took to execute. Clicking any of the targets takes you to its Target Detail screen.\n\n\\\n\nThe Target Detail screen shows you the build logs for the target next to the commands that were executed for the target.\n\n#### Command Timings\n\n\\\n\nThe second sub-tab on the Timings tab is Command Timings. This screen gives a detailed breakdown of how long each command run as part of the build took to execute. Clicking any of the commands takes you to its Target Detail screen with the command you clicked highlighted.\n\n### Build Graph\n\n\\\n\nThe third tab on the Build Details screen is the Graph tab. This tab shows a graph visualization of each target run as part of the build and which relied on which others. Clicking any of the targets takes you to its Target Detail screen.\n\n### Build Logs\n\n\\\n\nThe last tab on the Build Details screen is the Logs tab. This shows the logs for your build. If you've ever used Earthly's Cloud logs, this should look very familiar.\n\n## Sign Up for Earthly Cloud and Start Using the New UI Today\n\n[Sign up for Earthly Cloud](https://cloud.earthly.dev/login) to start using the new UI and all of the features included with it. It's available to all Earthly Cloud users. Try it out, then let us know how it works for you and if you have any suggestions for other enhancements to the Earthly Cloud UI that you'd like to see us build."
    }
]